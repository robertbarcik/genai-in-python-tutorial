{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8g1uIqtW0kq"
   },
   "source": [
    "# ü§ñ RAG: Retrieval-Augmented Generation\n",
    "\n",
    "\n",
    "**RAG** is a technique that combines the power of Large Language Models (LLMs) with the ability to retrieve relevant information from external knowledge sources.\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Real-World Applications\n",
    "\n",
    "- **Customer Support**: Answer questions using product documentation\n",
    "- **Internal Knowledge Bases**: Help employees find company information\n",
    "- **Document Q&A**: Extract insights from reports, contracts, or research papers\n",
    "- **Code Documentation**: Search through codebases and generate explanations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "‚úÖ **Understand the three components of RAG** (Retrieval ‚Üí Augmentation ‚Üí Generation)\n",
    "\n",
    "‚úÖ **Learn about embeddings** and how they represent meaning numerically\n",
    "\n",
    "‚úÖ **Implement semantic search** to find relevant documents\n",
    "\n",
    "‚úÖ **Build a complete RAG pipeline** from scratch\n",
    "\n",
    "‚úÖ **Understand production considerations** (vector databases, chunking, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M58p-O7W0kr"
   },
   "source": [
    "---\n",
    "\n",
    "# 1. Theory: Why RAG Exists\n",
    "\n",
    "## üö´ The Problem: LLM Limitations\n",
    "\n",
    "Before we dive into RAG, let's understand WHY we need it.\n",
    "\n",
    "### Knowledge Cutoff Dates\n",
    "LLMs are trained on data up to a specific date. They don't know about:\n",
    "- Recent events or news\n",
    "- New products or companies launched after training\n",
    "- Updated policies or procedures\n",
    "\n",
    "### No Access to Private Data\n",
    "LLMs can't access:\n",
    "- Your company's internal documents\n",
    "- Proprietary customer information\n",
    "- Personal or confidential data\n",
    "- Real-time database contents\n",
    "\n",
    "### Hallucination Risks\n",
    "When uncertain, LLMs may:\n",
    "- Generate plausible-sounding but incorrect information\n",
    "- Mix facts from different sources incorrectly\n",
    "- Fill gaps with \"reasonable\" guesses\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ The Solution: RAG\n",
    "\n",
    "RAG gives LLMs the ability to \"look things up\" before answering. Here's how:\n",
    "\n",
    "### The Three Steps of RAG:\n",
    "\n",
    "1. **üîç Retrieval**\n",
    "   - Search your knowledge base for relevant documents\n",
    "   - Find the information that best matches the user's question\n",
    "   - Like finding the right page in a textbook\n",
    "\n",
    "2. **üìù Augmentation**\n",
    "   - Add the retrieved information to the prompt as \"context\"\n",
    "   - Tell the LLM: \"Here's the relevant information, use it to answer\"\n",
    "   - Like giving someone notes before asking them a question\n",
    "\n",
    "3. **üí¨ Generation**\n",
    "   - LLM generates an answer based on the provided context\n",
    "   - Answer is grounded in real information, not guesses\n",
    "   - Like a student answering from their notes instead of memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxtzEQX9W0ks"
   },
   "source": [
    "---\n",
    "\n",
    "# 2. Setup\n",
    "\n",
    "Let's set up our environment and prepare to build our RAG system.\n",
    "\n",
    "## üì¶ What We'll Install\n",
    "\n",
    "- **openai**: Official OpenAI Python SDK for API access\n",
    "- **pandas**: Data manipulation (we'll store documents in a DataFrame)\n",
    "- **numpy**: Numerical operations (for vector similarity calculations)\n",
    "- **matplotlib**: Optional visualization tools\n",
    "\n",
    "## üîë API Configuration\n",
    "\n",
    "You'll need an OpenAI API key. You have two options:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpESabM9W0ks"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai pandas numpy matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ6br3RlW0kt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI models to use\n",
    "OPENAI_LLM_MODEL = \"gpt-5-nano\"  # For text generation\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"  # For embeddings\n",
    "\n",
    "print(f\"ü§ñ LLM Model: {OPENAI_LLM_MODEL}\")\n",
    "print(f\"üî¢ Embedding Model: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sMdXfeNW0kt"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized!\")\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmvO6d0CW0ku"
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Step 1: Preparing Our Documents\n",
    "\n",
    "## üìÑ What Are \"Documents\" in RAG?\n",
    "\n",
    "In RAG, a \"document\" is any piece of text that contains information:\n",
    "- Product descriptions\n",
    "- Company profiles\n",
    "- Support articles\n",
    "- Research papers\n",
    "- Code documentation\n",
    "- Meeting notes\n",
    "\n",
    "## üèóÔ∏è Document Structure Matters\n",
    "\n",
    "Good documents are:\n",
    "- **Self-contained**: Each document has complete information about one topic\n",
    "- **Well-structured**: Clear, organized, with key information highlighted\n",
    "- **Right-sized**: Not too long (loses focus) or too short (lacks context)\n",
    "- **Consistent**: Follow the same format for similar types of information\n",
    "\n",
    "## üìä Scale\n",
    "\n",
    "- **In this tutorial**: 10 sample startup companies (learning purposes)\n",
    "- **In production**: Could be thousands or millions of documents\n",
    "\n",
    "## üéØ Our Task\n",
    "\n",
    "We'll create a small knowledge base of 10 startup companies, each with:\n",
    "- Company name\n",
    "- Industry\n",
    "- Location\n",
    "- Description (what they do)\n",
    "- Investors\n",
    "- Founded year\n",
    "\n",
    "Then we'll convert this structured data into natural language \"documents\" that are easy to search.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e4TYjalW0ku"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's create our knowledge base of startup companies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYebzSVPW0ku"
   },
   "outputs": [],
   "source": [
    "# Create mock data: 10 startup companies\n",
    "# This simulates a knowledge base you might have in a real application\n",
    "\n",
    "companies_data = [\n",
    "    {\n",
    "        \"name\": \"Pentera\",\n",
    "        \"industry\": \"Cybersecurity\",\n",
    "        \"location\": \"Tel Aviv, Israel\",\n",
    "        \"description\": \"Pentera provides automated security validation platforms that help organizations continuously test their cybersecurity defenses. Their platform simulates real-world attacks to identify vulnerabilities before hackers can exploit them.\",\n",
    "        \"investors\": [\"K1 Investment Management\", \"Insight Partners\", \"Blackstone\"],\n",
    "        \"founded\": 2015\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Wiz\",\n",
    "        \"industry\": \"Cloud Security\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"Wiz is a cloud security platform that helps organizations identify and remove critical risks across their cloud infrastructure. They provide comprehensive visibility and threat detection for AWS, Azure, and Google Cloud.\",\n",
    "        \"investors\": [\"Sequoia Capital\", \"Greenoaks\", \"Salesforce Ventures\", \"Cyberstarts\"],\n",
    "        \"founded\": 2020\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ramp\",\n",
    "        \"industry\": \"FinTech\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"Ramp is a corporate card and spend management platform that helps companies save time and money. Their platform automates expense tracking, provides real-time insights, and identifies cost-saving opportunities.\",\n",
    "        \"investors\": [\"Founders Fund\", \"Stripe\", \"Goldman Sachs\", \"Thrive Capital\"],\n",
    "        \"founded\": 2019\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Notion\",\n",
    "        \"industry\": \"Productivity Software\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Notion is an all-in-one workspace that combines notes, tasks, wikis, and databases. Teams use Notion to collaborate, organize knowledge, and manage projects in one unified platform.\",\n",
    "        \"investors\": [\"Coatue\", \"Sequoia Capital\", \"Index Ventures\"],\n",
    "        \"founded\": 2016\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Anduril Industries\",\n",
    "        \"industry\": \"Defense Technology\",\n",
    "        \"location\": \"Costa Mesa, USA\",\n",
    "        \"description\": \"Anduril Industries builds advanced defense technology products including autonomous systems, sensors, and AI-powered solutions. Their technology is used for border security, base security, and military applications.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"Founders Fund\", \"8VC\", \"Valor Equity Partners\"],\n",
    "        \"founded\": 2017\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Databricks\",\n",
    "        \"industry\": \"Data Analytics\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Databricks provides a unified analytics platform built on Apache Spark. Their lakehouse platform combines data warehousing and data lakes, enabling companies to build data, analytics, and AI solutions at scale.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"NEA\", \"Coatue\", \"Tiger Global\"],\n",
    "        \"founded\": 2013\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Figma\",\n",
    "        \"industry\": \"Design Software\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Figma is a collaborative interface design tool that runs in the browser. Designers use Figma to create, prototype, and collaborate on user interfaces for web and mobile applications in real-time.\",\n",
    "        \"investors\": [\"Sequoia Capital\", \"Greylock Partners\", \"Kleiner Perkins\", \"Index Ventures\"],\n",
    "        \"founded\": 2012\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Plaid\",\n",
    "        \"industry\": \"FinTech Infrastructure\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Plaid provides financial services APIs that enable applications to connect with users' bank accounts. Their platform powers thousands of fintech apps including Venmo, Robinhood, and Chime.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"NEA\", \"Index Ventures\", \"Goldman Sachs\"],\n",
    "        \"founded\": 2013\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"UiPath\",\n",
    "        \"industry\": \"Robotic Process Automation\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"UiPath is a leading RPA platform that helps organizations automate repetitive business processes. Their software robots can handle tasks like data entry, document processing, and system integration.\",\n",
    "        \"investors\": [\"Accel\", \"CapitalG\", \"Sequoia Capital\", \"Tiger Global\"],\n",
    "        \"founded\": 2005\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snyk\",\n",
    "        \"industry\": \"Developer Security\",\n",
    "        \"location\": \"Boston, USA\",\n",
    "        \"description\": \"Snyk is a developer security platform that helps teams find and fix vulnerabilities in code, dependencies, containers, and infrastructure as code. Their tools integrate directly into developer workflows.\",\n",
    "        \"investors\": [\"Accel\", \"Coatue\", \"Tiger Global\", \"Boldstart Ventures\"],\n",
    "        \"founded\": 2015\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created data for {len(companies_data)} startup companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jVg7ALjW0kv"
   },
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier handling\n",
    "# DataFrames are great for structured data manipulation\n",
    "\n",
    "df = pd.DataFrame(companies_data)\n",
    "\n",
    "print(f\"‚úÖ Created database of {len(df)} companies\")\n",
    "print(\"\\nüìä First few companies:\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB1CByncW0kv"
   },
   "outputs": [],
   "source": [
    "def create_document_text(company: dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert a company dictionary into a readable text document.\n",
    "    This is what we'll create embeddings for and search through.\n",
    "\n",
    "    Args:\n",
    "        company: Dictionary containing company information\n",
    "\n",
    "    Returns:\n",
    "        A formatted text document describing the company\n",
    "    \"\"\"\n",
    "    # Join the list of investors into a readable string\n",
    "    investors_str = \", \".join(company[\"investors\"])\n",
    "\n",
    "    # Create a natural language document\n",
    "    # Note: This structure makes it easy for semantic search to find relevant info\n",
    "    text = f\"\"\"{company['name']} is a {company['industry']} company headquartered in {company['location']}. {company['description']} The company was founded in {company['founded']}. Key investors include: {investors_str}.\"\"\"\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Apply the function to all companies to create searchable documents\n",
    "df['document'] = df.apply(lambda row: create_document_text(row.to_dict()), axis=1)\n",
    "\n",
    "print(\"‚úÖ Created searchable documents for all companies\")\n",
    "print(\"\\nüìÑ Example document:\")\n",
    "print(\"=\"*70)\n",
    "print(df['document'].iloc[0])\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyNOFLB_W0kv"
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Step 2: Creating Embeddings\n",
    "\n",
    "\n",
    "**Embeddings** are numerical representations that capture the *meaning* of text. Think of them as coordinates in a \"meaning space.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckIgHzFlW0kw"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Let's generate embeddings for all our company documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbLhqlGlW0kw"
   },
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding vector for the given text using OpenAI's API.\n",
    "\n",
    "    Args:\n",
    "        text: The text to embed\n",
    "\n",
    "    Returns:\n",
    "        A list of floats representing the embedding vector (1536 dimensions)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call OpenAI's embeddings API\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=OPENAI_EMBEDDING_MODEL\n",
    "        )\n",
    "\n",
    "        # Extract the embedding vector from the response\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function with a sample\n",
    "sample_text = \"This is a test sentence about cybersecurity\"\n",
    "sample_embedding = get_embedding(sample_text)\n",
    "\n",
    "if sample_embedding:\n",
    "    print(f\"‚úÖ Generated embedding with {len(sample_embedding)} dimensions\")\n",
    "    print(f\"\\nüìä First 10 values: {sample_embedding[:10]}\")\n",
    "    print(f\"\\nüí° Each document will become a vector like this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N4v8tkiW0kw"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all company documents\n",
    "# This is the step that converts our text into searchable vectors\n",
    "\n",
    "print(\"üîÑ Generating embeddings for all documents...\")\n",
    "print(\"   (This may take a few seconds)\\n\")\n",
    "\n",
    "# Apply the get_embedding function to each document\n",
    "df['embedding'] = df['document'].apply(get_embedding)\n",
    "\n",
    "# Check for any failures\n",
    "failed_count = df['embedding'].isnull().sum()\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {failed_count} embeddings failed to generate\")\n",
    "else:\n",
    "    print(f\"‚úÖ Successfully generated embeddings for all {len(df)} documents!\")\n",
    "\n",
    "# Show some stats about the embeddings\n",
    "print(f\"\\nüìä Embedding Statistics:\")\n",
    "print(f\"   Dimensions: {len(df['embedding'].iloc[0])}\")\n",
    "print(f\"   Total embeddings: {len(df)}\")\n",
    "print(f\"\\nüìù First embedding (first 10 values):\")\n",
    "print(f\"   {df['embedding'].iloc[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlwMLZQwW0kx"
   },
   "source": [
    "## üí° What We Just Did\n",
    "\n",
    "We converted all 10 company documents into numerical vectors (embeddings). Each document is now:\n",
    "- A 1536-dimensional vector\n",
    "- Representing the semantic meaning of the text\n",
    "- Ready to be compared with query embeddings\n",
    "\n",
    "## üóÑÔ∏è Production Note: Vector Databases\n",
    "\n",
    "In this tutorial, we're storing embeddings in a Pandas DataFrame (in memory). This works fine for learning, but in production:\n",
    "\n",
    "- **Don't do this**: Store millions of embeddings in memory\n",
    "- **Do this instead**: Use a vector database (Pinecone, Weaviate, Chroma, Qdrant)\n",
    "- **Why?**: Vector databases are optimized for fast similarity search at scale\n",
    "\n",
    "We'll cover vector databases in later notebooks!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pB3GtJTW0kx"
   },
   "source": [
    "---\n",
    "\n",
    "# 5. Step 3: Semantic Search\n",
    "\n",
    "Now that we have embeddings for all documents, we need to measure how similar they are to a query.\n",
    "\n",
    "### üßÆ Cosine Similarity\n",
    "\n",
    "The most common way to measure vector similarity is **cosine similarity**:\n",
    "\n",
    "- Measures the angle between two vectors\n",
    "- Returns a value between -1 and 1\n",
    "- 1 = identical direction (very similar)\n",
    "- 0 = perpendicular (unrelated)\n",
    "- -1 = opposite direction (very different)\n",
    "\n",
    "### üí° Simple Explanation\n",
    "\n",
    "Imagine two arrows in space:\n",
    "- If they point in the same direction ‚Üí high similarity\n",
    "- If they point in different directions ‚Üí low similarity\n",
    "\n",
    "**Why this works**: Documents with similar meanings have embeddings that \"point\" in similar directions in the high-dimensional space.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîç The Retrieval Process\n",
    "\n",
    "Here's how we find relevant documents:\n",
    "\n",
    "### Step-by-Step:\n",
    "\n",
    "1. **User asks a question**: \"What does Pentera do?\"\n",
    "\n",
    "2. **Embed the question**: Convert it to a vector using the same embedding model\n",
    "\n",
    "3. **Calculate similarity**: Compare question vector with all document vectors\n",
    "   ```\n",
    "   Question: [0.2, 0.5, 0.1, ...]\n",
    "   \n",
    "   Doc 1 (Pentera): [0.3, 0.4, 0.2, ...] ‚Üí similarity: 0.92 ‚úÖ\n",
    "   Doc 2 (Wiz):     [0.3, 0.4, 0.1, ...] ‚Üí similarity: 0.87\n",
    "   Doc 3 (Ramp):    [0.1, 0.2, 0.8, ...] ‚Üí similarity: 0.45\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "4. **Rank by score**: Sort documents by similarity (highest first)\n",
    "\n",
    "5. **Return top-k**: Get the most relevant documents (typically top 1-5)\n",
    "\n",
    "### üí° Key Point: This Is \"Retrieval\" in RAG\n",
    "\n",
    "This semantic search process is the **\"Retrieval\"** component of RAG. We're retrieving the most relevant documents to provide as context to the LLM.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zlVO6ViW0kx"
   },
   "source": [
    "\n",
    "\n",
    "Let's implement semantic search to find relevant documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whC7l6MzW0kx"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: list, vec2: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    Returns a value between -1 and 1, where 1 means identical direction.\n",
    "\n",
    "    Args:\n",
    "        vec1: First embedding vector\n",
    "        vec2: Second embedding vector\n",
    "\n",
    "    Returns:\n",
    "        Similarity score (higher = more similar)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for mathematical operations\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "\n",
    "    # Calculate dot product (how much vectors point in same direction)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Calculate magnitudes (length of each vector)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "\n",
    "    # Cosine similarity = dot product / product of magnitudes\n",
    "    return dot_product / norm_product\n",
    "\n",
    "# Test with two sample embeddings\n",
    "test_vec1 = get_embedding(\"cybersecurity startup\")\n",
    "test_vec2 = get_embedding(\"security company\")\n",
    "test_vec3 = get_embedding(\"restaurant food delivery\")\n",
    "\n",
    "sim_similar = cosine_similarity(test_vec1, test_vec2)\n",
    "sim_different = cosine_similarity(test_vec1, test_vec3)\n",
    "\n",
    "print(\"üß™ Testing Cosine Similarity:\\n\")\n",
    "print(f\"   'cybersecurity startup' vs 'security company': {sim_similar:.4f} ‚úÖ\")\n",
    "print(f\"   'cybersecurity startup' vs 'restaurant food': {sim_different:.4f}\")\n",
    "print(f\"\\nüí° Notice: Similar concepts have higher scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGaTIn2mW0kx"
   },
   "outputs": [],
   "source": [
    "def find_most_relevant_documents(query: str, documents_df: pd.DataFrame, top_k: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the top_k most relevant documents for a given query.\n",
    "    This is the core semantic search function.\n",
    "\n",
    "    Args:\n",
    "        query: The search query (user's question)\n",
    "        documents_df: DataFrame with documents and their embeddings\n",
    "        top_k: Number of top documents to return\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with top_k most relevant documents, sorted by similarity\n",
    "    \"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "\n",
    "    # Step 1: Generate embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        print(\"‚ùå Failed to generate query embedding\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: Calculate similarity with all documents\n",
    "    # For each document embedding, calculate cosine similarity with query\n",
    "    documents_df['similarity'] = documents_df['embedding'].apply(\n",
    "        lambda doc_embedding: cosine_similarity(doc_embedding, query_embedding)\n",
    "    )\n",
    "\n",
    "    # Step 3: Get top_k results (highest similarity scores)\n",
    "    results = documents_df.nlargest(top_k, 'similarity')\n",
    "\n",
    "    print(f\"‚úÖ Found {len(results)} relevant document(s)\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Semantic search function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkY88NIgW0kx"
   },
   "outputs": [],
   "source": [
    "# Test semantic search with our original question\n",
    "question = \"What does the startup company Pentera do and who invested in it?\"\n",
    "\n",
    "print(\"üöÄ Testing Semantic Search\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find most relevant document\n",
    "relevant_docs = find_most_relevant_documents(question, df, top_k=1)\n",
    "\n",
    "# Display results\n",
    "if relevant_docs is not None and len(relevant_docs) > 0:\n",
    "    print(\"üìÑ MOST RELEVANT DOCUMENT:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Company: {relevant_docs.iloc[0]['name']}\")\n",
    "    print(f\"Industry: {relevant_docs.iloc[0]['industry']}\")\n",
    "    print(f\"Similarity Score: {relevant_docs.iloc[0]['similarity']:.4f}\")\n",
    "    print(f\"\\nDocument Text:\")\n",
    "    print(\"-\"*70)\n",
    "    print(relevant_docs.iloc[0]['document'])\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüí° SUCCESS: We found the right document!\")\n",
    "    print(\"   This is the 'Retrieval' part of RAG working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY41bseQW0ky"
   },
   "source": [
    "## üéâ Semantic Search Is Working!\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "1. We asked: \"What does Pentera do and who invested in it?\"\n",
    "2. The system converted our question to an embedding\n",
    "3. It compared our question with all 10 company documents\n",
    "4. It found that the Pentera document was most similar\n",
    "5. It returned that document with a high similarity score\n",
    "\n",
    "### üìä Understanding Similarity Scores\n",
    "\n",
    "- **0.9 - 1.0**: Extremely relevant (nearly identical meaning)\n",
    "- **0.7 - 0.9**: Very relevant (strong semantic match)\n",
    "- **0.5 - 0.7**: Somewhat relevant (partial match)\n",
    "- **< 0.5**: Not very relevant (weak or no match)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57dMF1vRW0ky"
   },
   "source": [
    "---\n",
    "\n",
    "# 6. Step 4: Augmented Generation\n",
    "\n",
    "\n",
    "\n",
    "Now we have the relevant document. How do we use it? We need to carefully structure our prompt to:\n",
    "\n",
    "### 1. Provide Context Explicitly\n",
    "```\n",
    "Context:\n",
    "Pentera is a cybersecurity company...\n",
    "```\n",
    "\n",
    "### 2. Give Clear Instructions\n",
    "```\n",
    "Answer the question using ONLY the context provided.\n",
    "```\n",
    "\n",
    "### 3. Tell Model to Stay Grounded\n",
    "```\n",
    "If you cannot answer from the context, say so.\n",
    "```\n",
    "\n",
    "### 4. Ask the Question\n",
    "```\n",
    "Question: What does Pentera do?\n",
    "```\n",
    "\n",
    "## üéØ The RAG Prompt Structure\n",
    "\n",
    "A typical RAG prompt looks like:\n",
    "\n",
    "```\n",
    "Answer the question below using ONLY the context provided.\n",
    "If you cannot answer from the context, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "[Retrieved document(s) here]\n",
    "\n",
    "Question: [User's question here]\n",
    "\n",
    "Answer:\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "This is the **\"Augmentation\"** in RAG. We're augmenting (enriching) the prompt with retrieved information. The LLM now has:\n",
    "- The user's question\n",
    "- Relevant context to answer it\n",
    "- Clear instructions on how to use the context\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. **Reduces Hallucination**: Model has facts to work with\n",
    "2. **Provides Source Attribution**: We know where the answer came from\n",
    "3. **Up-to-date Information**: Documents can be updated without retraining\n",
    "4. **Domain-Specific**: Works with private/proprietary information\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okjcZYKKW0ky"
   },
   "source": [
    "\n",
    "\n",
    "Let's implement the generation step with retrieved context!\n",
    "\n",
    "\n",
    "The prompt we'll construct has three key parts:\n",
    "\n",
    "1. **Instructions**: \"Answer using ONLY the context provided\"\n",
    "2. **Context**: The retrieved document(s) with relevant information\n",
    "3. **Question**: The user's original query\n",
    "\n",
    "This structure ensures the LLM stays grounded in the retrieved facts and doesn't hallucinate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOnkA_62W0ky"
   },
   "outputs": [],
   "source": [
    "def generate_answer_with_rag(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the retrieved context.\n",
    "    This is the 'Generation' step in RAG.\n",
    "\n",
    "    Args:\n",
    "        query: The user's question\n",
    "        context: The retrieved document(s) to use as context\n",
    "\n",
    "    Returns:\n",
    "        The generated answer based on the context\n",
    "    \"\"\"\n",
    "    # Construct the prompt with context and instructions\n",
    "    # This is the \"Augmentation\" - we're adding retrieved context to guide the LLM\n",
    "    prompt = f\"\"\"Answer the question below using ONLY the context provided.\n",
    "If you cannot answer the question from the context, say \"I don't have enough information in the provided context to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Call the LLM with the augmented prompt using the Responses API\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_LLM_MODEL,\n",
    "            input=prompt\n",
    "        )\n",
    "\n",
    "        return response.output_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error generating answer: {e}\"\n",
    "\n",
    "print(\"‚úÖ RAG answer generation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB_ZlJIwW0ky"
   },
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, documents_df: pd.DataFrame, top_k: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieval + Augmented Generation.\n",
    "\n",
    "    This function combines all steps:\n",
    "    1. Retrieve relevant documents (semantic search)\n",
    "    2. Extract context from retrieved documents\n",
    "    3. Generate answer using context\n",
    "\n",
    "    Args:\n",
    "        query: User's question\n",
    "        documents_df: DataFrame with documents and embeddings\n",
    "        top_k: Number of documents to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with query, retrieved docs, and answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    relevant_docs = find_most_relevant_documents(query, documents_df, top_k)\n",
    "\n",
    "    if relevant_docs is None or len(relevant_docs) == 0:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": None,\n",
    "            \"answer\": \"Failed to retrieve documents\"\n",
    "        }\n",
    "\n",
    "    # Step 2: Extract context from retrieved documents\n",
    "    # Join multiple documents with double newlines for clarity\n",
    "    context = \"\\n\\n\".join(relevant_docs['document'].tolist())\n",
    "\n",
    "    # Step 3: Generate answer with context\n",
    "    answer = generate_answer_with_rag(query, context)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": relevant_docs,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Complete RAG pipeline function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elv9--f9W0ky"
   },
   "outputs": [],
   "source": [
    "# Run the complete RAG pipeline\n",
    "question = \"What does the startup company Pentera do and who invested in it?\"\n",
    "\n",
    "print(\"üöÄ Running Complete RAG Pipeline...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = rag_pipeline(question, df, top_k=1)\n",
    "\n",
    "print(f\"\\n‚ùì QUESTION:\")\n",
    "print(f\"   {result['query']}\\n\")\n",
    "\n",
    "print(f\"üìÑ RETRIEVED DOCUMENT:\")\n",
    "print(f\"   Company: {result['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"   Similarity: {result['retrieved_docs'].iloc[0]['similarity']:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ RAG ANSWER:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüéâ SUCCESS! RAG pipeline is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llPH_wFSW0ky"
   },
   "source": [
    "## üéâ Complete RAG Pipeline Is Working!\n",
    "\n",
    "\n",
    "\n",
    "We successfully implemented the complete RAG pipeline:\n",
    "\n",
    "1. **üîç Retrieval**: Found the most relevant document (Pentera profile)\n",
    "2. **üìù Augmentation**: Added that document as context to our prompt\n",
    "3. **üí¨ Generation**: LLM generated an accurate answer using the context\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWIXzyZ0W0kz"
   },
   "source": [
    "---\n",
    "\n",
    "# 7. Optional Experiment: Top-K Retrieval\n",
    "\n",
    "\n",
    "So far, we've been retrieving just the top 1 most relevant document. But what if we retrieve multiple documents?\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "**More Documents (higher top_k):**\n",
    "- ‚úÖ More complete information\n",
    "- ‚úÖ Better coverage if information is split across documents\n",
    "- ‚ùå More tokens = higher cost\n",
    "- ‚ùå Irrelevant context can confuse the model\n",
    "- ‚ùå Longer processing time\n",
    "\n",
    "**Fewer Documents (lower top_k):**\n",
    "- ‚úÖ Lower cost\n",
    "- ‚úÖ Faster\n",
    "- ‚úÖ More focused context\n",
    "- ‚ùå Might miss relevant information\n",
    "\n",
    "Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQQkH5RlW0kz"
   },
   "outputs": [],
   "source": [
    "# Try a question that might benefit from multiple documents\n",
    "experiment_question = \"Which companies are in the cybersecurity industry?\"\n",
    "\n",
    "print(\"üß™ EXPERIMENT: Comparing top-1 vs top-3 retrieval\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì Question: {experiment_question}\\n\")\n",
    "\n",
    "# Test with top-1\n",
    "print(\"üìä Test 1: Retrieving top-1 document\")\n",
    "print(\"-\"*70)\n",
    "result_top1 = rag_pipeline(experiment_question, df, top_k=1)\n",
    "\n",
    "print(f\"Retrieved: {result_top1['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"\\nAnswer: {result_top1['answer']}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Test with top-3\n",
    "print(\"\\nüìä Test 2: Retrieving top-3 documents\")\n",
    "print(\"-\"*70)\n",
    "result_top3 = rag_pipeline(experiment_question, df, top_k=3)\n",
    "\n",
    "print(f\"Retrieved:\")\n",
    "for idx, row in result_top3['retrieved_docs'].iterrows():\n",
    "    print(f\"  {idx+1}. {row['name']} (similarity: {row['similarity']:.4f})\")\n",
    "\n",
    "print(f\"\\nAnswer: {result_top3['answer']}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   - Top-1: May only mention one company\")\n",
    "print(\"   - Top-3: Can mention multiple companies if they're all in the context\")\n",
    "print(\"   - Trade-off: More complete vs more costly\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy9o6RmYW0kz"
   },
   "source": [
    "\n",
    "\n",
    "### When to Use top_k = 1:\n",
    "- Questions about a specific entity (\"Tell me about Pentera\")\n",
    "- When you need focused, specific information\n",
    "- Cost/speed is a priority\n",
    "- Your documents are comprehensive (all info in one place)\n",
    "\n",
    "### When to Use top_k > 1:\n",
    "- Comparative questions (\"Which companies do X?\")\n",
    "- Information might be split across documents\n",
    "- Need comprehensive coverage\n",
    "- Accuracy is more important than cost\n",
    "\n",
    "\n",
    "\n",
    "**top_k is a hyperparameter you tune based on your use case!**\n",
    "\n",
    "In production, you might:\n",
    "- Start with top_k = 3-5\n",
    "- Test with your specific questions\n",
    "- Measure quality vs cost\n",
    "- Adjust based on results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZp_fuTcW0kz"
   },
   "source": [
    "---\n",
    "\n",
    "# 8. Production Considerations\n",
    "\n",
    "## üì¶ Vector Databases in Production\n",
    "\n",
    "In this notebook, we stored embeddings in a Pandas DataFrame. This works great for learning with 10 documents, but NOT for production:\n",
    "\n",
    "### ‚ùå Problems with Our Approach:\n",
    "\n",
    "1. **Slow**: Must compare query to ALL documents every time\n",
    "   - 10 documents: Fast\n",
    "   - 1 million documents: Extremely slow\n",
    "\n",
    "2. **Limited Scale**: Can't handle millions of documents\n",
    "   - Everything stored in memory\n",
    "   - No optimization for large-scale search\n",
    "\n",
    "3. **No Persistence**: Data lost when notebook closes\n",
    "   - Must regenerate embeddings every time\n",
    "   - No way to update documents incrementally\n",
    "\n",
    "4. **No Advanced Features**:\n",
    "   - Can't filter by metadata (\"cybersecurity companies only\")\n",
    "   - No hybrid search (semantic + keyword)\n",
    "   - No approximate nearest neighbor (ANN) algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Production Solution: Vector Databases\n",
    "\n",
    "Vector databases are specialized systems optimized for similarity search:\n",
    "\n",
    "### Popular Vector Databases:\n",
    "\n",
    "- **Pinecone**: Fully managed, cloud-based, easy to use\n",
    "- **Weaviate**: Open-source, supports hybrid search\n",
    "- **Chroma**: Simple, lightweight, great for prototyping\n",
    "- **Qdrant**: Fast, Rust-based, good for production\n",
    "- **Milvus**: Open-source, scalable, enterprise-ready\n",
    "\n",
    "### üöÄ What Vector Databases Provide:\n",
    "\n",
    "1. **Fast Search**:\n",
    "   - Approximate Nearest Neighbor (ANN) algorithms\n",
    "   - HNSW, IVF, etc.\n",
    "   - Search millions of vectors in milliseconds\n",
    "\n",
    "2. **Scalability**:\n",
    "   - Handle billions of vectors\n",
    "   - Distributed storage\n",
    "   - Horizontal scaling\n",
    "\n",
    "3. **Persistence**:\n",
    "   - Store embeddings permanently\n",
    "   - Add/update/delete documents\n",
    "   - No need to regenerate\n",
    "\n",
    "4. **Advanced Features**:\n",
    "   - Metadata filtering\n",
    "   - Hybrid search (semantic + keyword)\n",
    "   - Multi-vector search\n",
    "   - Analytics and monitoring\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Other Production Considerations\n",
    "\n",
    "### 1. Document Chunking\n",
    "\n",
    "**Problem**: Long documents don't fit in context windows\n",
    "\n",
    "**Solution**: Break documents into smaller chunks\n",
    "- Chunk size: 200-500 words typical\n",
    "- Overlap: 10-20% to maintain context\n",
    "- Methods: Sentence-based, semantic chunking, fixed-size\n",
    "\n",
    "### 2. Metadata Filtering\n",
    "\n",
    "**Example**: \"Find cybersecurity companies in the USA\"\n",
    "- Pre-filter by country = USA\n",
    "- Then semantic search within filtered set\n",
    "- Faster and more accurate\n",
    "\n",
    "### 3. Hybrid Search\n",
    "\n",
    "Combine semantic + keyword search:\n",
    "- Semantic: Understands meaning\n",
    "- Keyword: Exact matches (names, IDs, codes)\n",
    "- Best of both worlds!\n",
    "\n",
    "### 4. Re-ranking\n",
    "\n",
    "Two-stage retrieval:\n",
    "1. Fast retrieval: Get top 50 documents (fast, approximate)\n",
    "2. Re-ranking: Use more sophisticated model on top 50\n",
    "3. Return top 5 after re-ranking\n",
    "\n",
    "### 5. Evaluation\n",
    "\n",
    "How do you know if your RAG system is working well?\n",
    "\n",
    "**Retrieval Metrics**:\n",
    "- Precision@k: How many retrieved docs are relevant?\n",
    "- Recall@k: Did we retrieve all relevant docs?\n",
    "- MRR (Mean Reciprocal Rank): Where do relevant docs appear?\n",
    "\n",
    "**Generation Metrics**:\n",
    "- Faithfulness: Is answer grounded in context?\n",
    "- Relevance: Does answer address the question?\n",
    "- Human evaluation: Still the gold standard!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFmAjKVZW0k0"
   },
   "source": [
    "---\n",
    "\n",
    "# 9. Try It Yourself!\n",
    "\n",
    "Now it's your turn to experiment! Use the playground below to:\n",
    "- Try different questions\n",
    "- Experiment with top_k values\n",
    "- Test questions that shouldn't be answerable from the data\n",
    "- See how the system handles edge cases\n",
    "\n",
    "## üí° Suggested Experiments:\n",
    "\n",
    "1. **Specific company questions**:\n",
    "   - \"What does Wiz do?\"\n",
    "   - \"Who invested in Notion?\"\n",
    "   - \"When was Databricks founded?\"\n",
    "\n",
    "2. **Comparative questions**:\n",
    "   - \"Which companies are in the cybersecurity industry?\"\n",
    "   - \"What fintech companies are in the database?\"\n",
    "   - \"Compare Pentera and Wiz\"\n",
    "\n",
    "3. **Questions that SHOULD fail** (not in our data):\n",
    "   - \"What does Apple do?\"\n",
    "   - \"Tell me about restaurants in New York\"\n",
    "   - \"What's the weather today?\"\n",
    "\n",
    "4. **Different top_k values**:\n",
    "   - Try top_k=1, top_k=3, top_k=5\n",
    "   - See how answers change\n",
    "\n",
    "Have fun experimenting! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZPZj6ctW0k0"
   },
   "outputs": [],
   "source": [
    "# üéÆ PLAYGROUND: Try your own questions!\n",
    "\n",
    "# Modify these variables:\n",
    "your_question = \"Which companies are in the cybersecurity industry?\"  # ‚Üê Change this!\n",
    "your_top_k = 2  # ‚Üê Try different values (1, 2, 3, 5)\n",
    "\n",
    "# Run the RAG pipeline\n",
    "result = rag_pipeline(your_question, df, top_k=your_top_k)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(f\"‚ùì YOUR QUESTION:\")\n",
    "print(f\"   {result['query']}\\n\")\n",
    "\n",
    "print(f\"üìÑ RETRIEVED DOCUMENTS (top-{your_top_k}):\")\n",
    "for idx, row in result['retrieved_docs'].iterrows():\n",
    "    print(f\"   {idx+1}. {row['name']} (similarity: {row['similarity']:.4f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ ANSWER:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmcsaqtgW0k0"
   },
   "outputs": [],
   "source": [
    "# üß™ ADVANCED: Test a question that SHOULDN'T be answerable\n",
    "# This tests how well the model handles missing information\n",
    "\n",
    "off_topic_question = \"What does Apple Inc do and who is their CEO?\"\n",
    "\n",
    "print(\"üß™ Testing off-topic question (not in our database):\\n\")\n",
    "result = rag_pipeline(off_topic_question, df, top_k=1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"‚ùì Question: {off_topic_question}\\n\")\n",
    "print(f\"üìÑ Top Retrieved Document: {result['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"   Similarity Score: {result['retrieved_docs'].iloc[0]['similarity']:.4f}\")\n",
    "print(f\"\\n‚úÖ Answer:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   - Low similarity score indicates poor match\")\n",
    "print(\"   - The answer should indicate insufficient information\")\n",
    "print(\"   - In production, you might reject queries with similarity < 0.7\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRXkf13YW0k0"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "### üìñ Additional Resources:\n",
    "\n",
    "- **OpenAI Embeddings Guide**: https://platform.openai.com/docs/guides/embeddings\n",
    "- **RAG Papers**: Look up \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lw71EgMRbPRt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
