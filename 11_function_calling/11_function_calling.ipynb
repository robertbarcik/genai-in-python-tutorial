{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Function Calling: Connecting LLMs to the Real World\n",
    "\n",
    "Welcome to Exercise #13! In this notebook, you'll learn how to extend Large Language Models beyond text generation by teaching them to interact with external functions and tools.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Understand function calling fundamentals** - What it is, why it matters, and when to use it\n",
    "2. **Define tool schemas** - Create structured descriptions that help LLMs understand available functions\n",
    "3. **Master the function calling loop** - Execute the complete request-response-execution-response cycle\n",
    "4. **Handle responses correctly** - Distinguish between text responses and function call requests\n",
    "5. **Build production-ready implementations** - Write reusable, error-handled function calling code\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Business Value: Why Function Calling Matters\n",
    "\n",
    "Function calling bridges the gap between conversational AI and real-world actions. Instead of LLMs just generating text, they can now:\n",
    "\n",
    "- **Access live data** - Fetch current weather, stock prices, database records\n",
    "- **Perform actions** - Book appointments, send emails, update records\n",
    "- **Query systems** - Search knowledge bases, run database queries, call APIs\n",
    "- **Automate workflows** - Chain multiple operations together intelligently\n",
    "\n",
    "### üåü Real-World Examples:\n",
    "\n",
    "- **Customer Service Chatbots** - Check order status, process refunds, schedule callbacks\n",
    "- **Personal Assistants** - Book meetings, set reminders, check calendars\n",
    "- **Data Analysis Tools** - Query databases, generate reports, visualize data\n",
    "- **Smart Home Control** - Adjust temperature, turn on lights, check security cameras\n",
    "- **E-commerce** - Search products, check inventory, process orders\n",
    "\n",
    "Without function calling, LLMs can only *suggest* what to do. With function calling, they can actually *do it*.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö 1. Theory: What is Function Calling?\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "**Function calling** is a capability that allows Large Language Models to intelligently request the execution of external functions. Instead of the LLM trying to generate an answer from its training data alone, it can recognize when it needs external information or actions, and request specific function calls with appropriate parameters.\n",
    "\n",
    "Think of it like this: Imagine you're a manager (the LLM) with a team of specialists (functions). When someone asks you a question, you can either answer from your own knowledge, or you can ask a specialist on your team to help. Function calling is how you ask your specialists for help.\n",
    "\n",
    "## Why It Matters\n",
    "\n",
    "LLMs are trained on static data with a knowledge cutoff. They don't have access to:\n",
    "- Real-time information (current weather, stock prices, etc.)\n",
    "- Private data (your database, user records, etc.)\n",
    "- System capabilities (sending emails, updating databases, etc.)\n",
    "\n",
    "Function calling solves this by letting LLMs:\n",
    "1. **Recognize** when external data or actions are needed\n",
    "2. **Request** specific functions with the right parameters\n",
    "3. **Incorporate** the results into their responses\n",
    "\n",
    "## When to Use Function Calling vs. Just Prompting\n",
    "\n",
    "**Use regular prompting when:**\n",
    "- The answer can be generated from the LLM's training data\n",
    "- You need creative or analytical responses\n",
    "- No real-time or private data is required\n",
    "\n",
    "**Use function calling when:**\n",
    "- You need real-time data (weather, prices, availability)\n",
    "- You need to query private systems (databases, APIs)\n",
    "- You need to perform actions (book, send, update, delete)\n",
    "- You want structured, reliable data access\n",
    "\n",
    "## The Request-Response Cycle\n",
    "\n",
    "Here's how function calling works at a high level:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ User Query  ‚îÇ  \"What's the weather in Paris?\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   LLM Decides   ‚îÇ  \"I need to call get_current_weather(location='Paris, France')\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Function Call   ‚îÇ  Execute: get_current_weather(location='Paris, France')\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    Execute      ‚îÇ  Return: {\"temperature\": 18, \"condition\": \"Cloudy\"}\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Result Back    ‚îÇ  Send function result back to LLM\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  LLM Response   ‚îÇ  \"The weather in Paris is currently 18¬∞C and cloudy.\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üí° Key Point: Function Calling vs. Text Generation\n",
    "\n",
    "**Important distinction:**\n",
    "- **Without function calling:** The LLM would try to guess the weather based on general knowledge (\"Paris typically has...\")\n",
    "- **With function calling:** The LLM recognizes it needs current data, requests the function, and uses the actual result\n",
    "\n",
    "The LLM doesn't *execute* the function itself - it *requests* that you execute it. You're still in control!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways:\n",
    "\n",
    "- Function calling lets LLMs request execution of external functions instead of just generating text\n",
    "- It bridges LLMs with real-time data, private systems, and actionable capabilities\n",
    "- The LLM decides WHEN to call a function and WHICH parameters to use\n",
    "- You (the developer) execute the actual function and return results to the LLM\n",
    "- This enables building chatbots that can actually DO things, not just talk about them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß 2. Setup\n",
    "\n",
    "Before we dive into function calling, let's set up our environment. We'll need:\n",
    "1. OpenAI API access\n",
    "2. The OpenAI Python library\n",
    "3. A few supporting libraries\n",
    "\n",
    "## üì¶ Install Dependencies\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI library\n",
    "!pip install -q openai\n",
    "\n",
    "# Suppress deprecation warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your OpenAI API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Cost-efficient model for function calling\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize OpenAI Client\n",
    "\n",
    "Now let's import the necessary libraries and create our OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "# This client will be used to make all API calls\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized successfully!\")\n",
    "print(\"\\nüéì Ready to learn about function calling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã 3. Understanding Tool Schemas\n",
    "\n",
    "## What is a Tool Schema?\n",
    "\n",
    "A **tool schema** is a JSON specification that describes a function to the LLM. Think of it as a menu that tells the LLM:\n",
    "- What functions are available\n",
    "- What each function does\n",
    "- What parameters each function accepts\n",
    "- What types those parameters should be\n",
    "- Which parameters are required\n",
    "\n",
    "## Why We Need Structured Schemas\n",
    "\n",
    "The LLM needs to understand:\n",
    "1. **When** to use each function (based on the description)\n",
    "2. **How** to use it (what parameters to provide)\n",
    "3. **What** values are acceptable (types, enums, etc.)\n",
    "\n",
    "Without a clear schema, the LLM wouldn't know which function to call or how to call it correctly.\n",
    "\n",
    "## üí° Key Point: Schema vs. Implementation\n",
    "\n",
    "**Critical distinction:**\n",
    "- **Tool Schema (JSON):** A *description* of the function for the LLM to read\n",
    "- **Function Implementation (Python):** The *actual code* that executes when called\n",
    "\n",
    "The schema is like a restaurant menu - it describes the dish. The implementation is like the kitchen - it actually makes the dish.\n",
    "\n",
    "The LLM only sees the schema. You execute the implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## üå§Ô∏è Step 1: Define the Actual Functions (Implementation)\n",
    "\n",
    "Let's start by creating our actual Python functions. For this tutorial, we'll use mock weather functions that simulate API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION: These are the actual Python functions\n",
    "# The LLM will REQUEST these functions, and we will EXECUTE them\n",
    "\n",
    "def get_current_weather(location: str, format: str) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to simulate getting the current weather.\n",
    "    In a real application, this would call an actual weather API.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA.\n",
    "        format (str): The temperature format, either 'celsius' or 'fahrenheit'.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A mock response with random weather data.\n",
    "    \"\"\"\n",
    "    # Generate random weather data for demonstration\n",
    "    temperature = random.uniform(15.0, 35.0) if format == \"celsius\" else random.uniform(60.0, 95.0)\n",
    "    weather_conditions = random.choice([\"Sunny\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Windy\"])\n",
    "    \n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": round(temperature, 2),\n",
    "        \"unit\": format,\n",
    "        \"condition\": weather_conditions\n",
    "    }\n",
    "\n",
    "\n",
    "def get_n_day_weather_forecast(location: str, format: str, num_days: int) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to simulate getting an N-day weather forecast.\n",
    "    In a real application, this would call an actual weather API.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA.\n",
    "        format (str): The temperature format, either 'celsius' or 'fahrenheit'.\n",
    "        num_days (int): The number of days to forecast.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A mock response with random weather data for each day.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    for day in range(num_days):\n",
    "        temperature = random.uniform(15.0, 35.0) if format == \"celsius\" else random.uniform(60.0, 95.0)\n",
    "        weather_conditions = random.choice([\"Sunny\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Windy\"])\n",
    "        forecast.append({\n",
    "            \"day\": f\"Day {day + 1}\",\n",
    "            \"temperature\": round(temperature, 2),\n",
    "            \"unit\": format,\n",
    "            \"condition\": weather_conditions\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Weather functions defined!\")\n",
    "print(\"\\nüìù Note: These are MOCK functions - they generate random data for learning purposes.\")\n",
    "print(\"   In production, these would call real weather APIs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test our functions to see how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the current weather function\n",
    "test_result = get_current_weather(\"Paris, France\", \"celsius\")\n",
    "print(\"üß™ Test: get_current_weather('Paris, France', 'celsius')\")\n",
    "print(json.dumps(test_result, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test the forecast function\n",
    "test_forecast = get_n_day_weather_forecast(\"Tokyo, Japan\", \"celsius\", 3)\n",
    "print(\"üß™ Test: get_n_day_weather_forecast('Tokyo, Japan', 'celsius', 3)\")\n",
    "print(json.dumps(test_forecast, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú Step 2: Define Tool Schemas (Descriptions for the LLM)\n",
    "\n",
    "Now that we have our actual functions, we need to describe them to the LLM using JSON schemas. The LLM will read these descriptions to understand when and how to call our functions.\n",
    "\n",
    "Let's create the tool schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMAS: These are descriptions that tell the LLM about our functions\n",
    "# The LLM reads these to decide when and how to call functions\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        # Type: Specifies this is a function tool\n",
    "        \"type\": \"function\",\n",
    "\n",
    "        # Name: Must match the actual Python function name EXACTLY\n",
    "        \"name\": \"get_current_weather\",\n",
    "\n",
    "        # Description: Tells the LLM WHEN to use this function\n",
    "        # Be clear and specific - this guides the LLM's decision-making\n",
    "        \"description\": \"Get the current weather in a given location. Use this when the user asks about current or present weather conditions.\",\n",
    "\n",
    "        # Parameters: JSON Schema describing the function inputs\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "\n",
    "            # Properties: Each parameter and its details\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    # Description helps LLM know what format to use\n",
    "                    \"description\": \"The city and state, e.g., San Francisco, CA or Paris, France\"\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    # Enum restricts to specific valid values\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Use 'celsius' for metric and 'fahrenheit' for imperial.\"\n",
    "                }\n",
    "            },\n",
    "\n",
    "            # Required: List of mandatory parameters\n",
    "            # The LLM will try to gather these before calling the function\n",
    "            \"required\": [\"location\", \"format\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "\n",
    "        # Second function: Weather forecast\n",
    "        \"name\": \"get_n_day_weather_forecast\",\n",
    "\n",
    "        # Clear description distinguishes this from get_current_weather\n",
    "        \"description\": \"Get an N-day weather forecast for a given location. Use this when the user asks about future weather or multi-day forecasts.\",\n",
    "\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g., San Francisco, CA or Paris, France\"\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Use 'celsius' for metric and 'fahrenheit' for imperial.\"\n",
    "                },\n",
    "                \"num_days\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The number of days to forecast, between 1 and 7 days\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\", \"num_days\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Tool schemas defined!\")\n",
    "print(f\"\\nüìã Number of tools available to LLM: {len(tools)}\")\n",
    "print(\"\\nüîç Tool names:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Schema Deep Dive\n",
    "\n",
    "Let's break down what each part of the schema does:\n",
    "\n",
    "### Structure Breakdown\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"function\",  // Tells OpenAI this is a function tool\n",
    "  \"name\": \"get_current_weather\",  // Must match Python function name\n",
    "  \"description\": \"...\",  // Helps LLM decide WHEN to use this\n",
    "  \"parameters\": {  // JSON Schema format\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {  // Define each parameter\n",
    "        \"location\": {\n",
    "          \"type\": \"string\",  // Data type\n",
    "          \"description\": \"...\"  // Helps LLM know what to provide\n",
    "        },\n",
    "        \"format\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"celsius\", \"fahrenheit\"]  // Restricts to valid values\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"location\", \"format\"]  // Mandatory parameters\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Key Components Explained\n",
    "\n",
    "1. **`type: \"function\"`** - Tells the LLM this is a function it can call\n",
    "\n",
    "2. **`name`** - The function identifier. **Must match your Python function name exactly!**\n",
    "\n",
    "3. **`description`** - Critical for helping the LLM decide when to use this function. Be specific about:\n",
    "   - What the function does\n",
    "   - When it should be used\n",
    "   - How it differs from similar functions\n",
    "\n",
    "4. **`parameters`** - Uses JSON Schema format to define inputs:\n",
    "   - `type: \"object\"` - Parameters are provided as an object\n",
    "   - `properties` - Each parameter's definition\n",
    "   - Each property has a `type` and `description`\n",
    "\n",
    "5. **`required`** - Array of mandatory parameter names. The LLM will try to gather these before calling.\n",
    "\n",
    "6. **`enum`** - Restricts parameter values to a specific list. Helps prevent invalid inputs.\n",
    "\n",
    "### ‚ö†Ô∏è Common Mistake: Mismatched Names\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# Python function\n",
    "def get_weather(location, format):\n",
    "    ...\n",
    "\n",
    "# Schema\n",
    "{\n",
    "    \"name\": \"getCurrentWeather\",  # ‚ùå Doesn't match!\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# Python function\n",
    "def get_weather(location, format):\n",
    "    ...\n",
    "\n",
    "# Schema\n",
    "{\n",
    "    \"name\": \"get_weather\",  # ‚úÖ Exact match!\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways: Tool Schemas\n",
    "\n",
    "- Tool schemas are JSON descriptions that tell the LLM about available functions\n",
    "- Schemas are NOT the functions themselves - they're instructions for the LLM\n",
    "- The `name` field must match your Python function name exactly\n",
    "- The `description` field is critical - it helps the LLM decide when to use the function\n",
    "- Parameter descriptions guide the LLM on what values to provide\n",
    "- Use `enum` to restrict parameters to specific valid values\n",
    "- The `required` array tells the LLM which parameters are mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÑ 4. Understanding LLM Responses\n",
    "\n",
    "## Response Types\n",
    "\n",
    "When you make an API call with function calling enabled, the LLM can respond in two ways:\n",
    "\n",
    "1. **Regular text response** - The LLM provides a conversational answer (no function needed)\n",
    "2. **Function call request** - The LLM requests to execute one or more functions\n",
    "\n",
    "Understanding which type of response you received is crucial for handling function calling correctly.\n",
    "\n",
    "## Response Structure\n",
    "\n",
    "The response object contains:\n",
    "- **`content`** - Text content (if the LLM is providing a conversational response)\n",
    "- **`tool_calls`** - List of function calls (if the LLM wants to call functions)\n",
    "\n",
    "We need to check which one is present to know how to proceed.\n",
    "\n",
    "## üí° Key Point: LLMs Can Ask for Clarification\n",
    "\n",
    "The LLM won't just blindly call functions. If it doesn't have enough information, it will ask the user for clarification. This is intelligent behavior!\n",
    "\n",
    "For example:\n",
    "- User: \"What's the weather?\"\n",
    "- LLM: \"I'd be happy to check the weather for you. Which city are you interested in?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example 1: LLM Asks for Clarification\n",
    "\n",
    "Let's see what happens when we ask a vague question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new conversation\n",
    "# In function calling, we maintain a conversation history with messages\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like today?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make API call with function calling enabled\n",
    "# The 'tools' parameter tells the LLM what functions are available\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"  # Let the LLM decide whether to call a function\n",
    ")\n",
    "\n",
    "# Extract the assistant's message\n",
    "# Get function calls from response\n",
    "function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "\n",
    "print(\"ü§ñ Assistant Response:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if the response contains tool calls or regular content\n",
    "if function_calls:\n",
    "    print(\"üìû Response type: FUNCTION CALL\")\n",
    "    print(f\"Number of function calls: {len(function_calls)}\")\n",
    "    for tool_call in function_calls:\n",
    "        print(f\"  - Function: {tool_call.name}\")\n",
    "else:\n",
    "    print(\"üí¨ Response type: TEXT CONTENT\")\n",
    "    print(f\"\\nContent: {response.output_text}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "The LLM received our question \"What's the weather like today?\" but noticed it's **missing required information**:\n",
    "- We didn't specify a location\n",
    "- We didn't specify celsius or fahrenheit\n",
    "\n",
    "Instead of making assumptions, the LLM asked for clarification. This is the **right behavior** - the system prompt told it not to guess!\n",
    "\n",
    "The response contains **`content`** (text), not **`tool_calls`** (function requests).\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example 2: LLM Calls a Function\n",
    "\n",
    "Now let's provide complete information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation by adding the user's clarification\n",
    "# We're building on the previous messages - this is ONE conversation\n",
    "# Note: For Responses API, we add function calls via tool messages  # Add the assistant's clarification question\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I'm in Glasgow, Scotland and prefer Celsius.\"\n",
    "})\n",
    "\n",
    "# Make another API call with the updated conversation\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# Get function calls from response\n",
    "function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "\n",
    "print(\"ü§ñ Assistant Response:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check the response type\n",
    "if function_calls:\n",
    "    print(\"üìû Response type: FUNCTION CALL\")\n",
    "    print(f\"\\nThe LLM decided to call a function!\\n\")\n",
    "    print(f\"Number of function calls: {len(function_calls)}\")\n",
    "    \n",
    "    for i, tool_call in enumerate(function_calls, 1):\n",
    "        print(f\"\\nFunction Call #{i}:\")\n",
    "        print(f\"  - Function name: {tool_call.name}\")\n",
    "        print(f\"  - Arguments: {tool_call.arguments}\")\n",
    "        print(f\"  - Tool call ID: {tool_call.id}\")\n",
    "else:\n",
    "    print(\"üí¨ Response type: TEXT CONTENT\")\n",
    "    print(f\"\\nContent: {response.output_text}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Anatomy of a Tool Call Response\n",
    "\n",
    "Perfect! Now the LLM has all the information it needs, so it requested a function call. Let's break down the response structure:\n",
    "\n",
    "### Tool Call Components:\n",
    "\n",
    "1. **`tool_calls`** - A list containing function call requests (can be multiple)\n",
    "\n",
    "2. **`tool_calls[0].id`** - Unique identifier for this specific function call\n",
    "   - Used to track which function result corresponds to which request\n",
    "   - Critical for the response loop\n",
    "\n",
    "3. **`tool_calls[0].type`** - Should always be `\"function\"` for function calls\n",
    "\n",
    "4. **`tool_calls[0].function.name`** - The name of the function to call\n",
    "   - This tells us WHICH function to execute\n",
    "   - Will match one of the names in our tool schemas\n",
    "\n",
    "5. **`tool_calls[0].function.arguments`** - A JSON string containing the parameters\n",
    "   - ‚ö†Ô∏è Important: This is a STRING, not a dict!\n",
    "   - We need to parse it with `json.loads()` before using it\n",
    "\n",
    "Let's extract these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tool call information\n",
    "# We're accessing the FIRST tool call (index 0)\n",
    "tool_call = function_calls[0]\n",
    "\n",
    "print(\"üîç Extracting Tool Call Information:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Tool call ID (for tracking)\n",
    "tool_call_id = tool_call.call_id\n",
    "print(f\"\\n1Ô∏è‚É£ Tool Call ID: {tool_call_id}\")\n",
    "print(\"   (This unique ID tracks this specific function call)\")\n",
    "\n",
    "# 2. Function name (which function to call)\n",
    "function_name = tool_call.name\n",
    "print(f\"\\n2Ô∏è‚É£ Function Name: {function_name}\")\n",
    "print(\"   (This tells us WHICH function to execute)\")\n",
    "\n",
    "# 3. Arguments (the parameters - comes as a JSON string)\n",
    "function_arguments_string = tool_call.arguments\n",
    "print(f\"\\n3Ô∏è‚É£ Arguments (as JSON string): {function_arguments_string}\")\n",
    "print(f\"   Type: {type(function_arguments_string)}\")\n",
    "\n",
    "# 4. Parse the arguments from JSON string to dictionary\n",
    "function_arguments = json.loads(function_arguments_string)\n",
    "print(f\"\\n4Ô∏è‚É£ Arguments (parsed to dict): {function_arguments}\")\n",
    "print(f\"   Type: {type(function_arguments)}\")\n",
    "print(f\"\\n   Location: {function_arguments.get('location')}\")\n",
    "print(f\"   Format: {function_arguments.get('format')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ Now we know WHAT function to call and WITH WHAT parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Point: Arguments Are JSON Strings\n",
    "\n",
    "**Important detail:**\n",
    "- `tool_call.function.arguments` is a **JSON string**, not a Python dict\n",
    "- You must parse it with `json.loads()` before using it\n",
    "- This is easy to forget and causes errors!\n",
    "\n",
    "```python\n",
    "# ‚ùå Wrong - arguments is a string\n",
    "location = tool_call.function.arguments['location']  # Error!\n",
    "\n",
    "# ‚úÖ Correct - parse first, then use\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "location = args['location']  # Works!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways: LLM Responses\n",
    "\n",
    "- LLM responses can contain either text content OR function calls (or both)\n",
    "- Always check if `tool_calls` exists before trying to access it\n",
    "- LLMs will ask for clarification if they don't have required information (smart behavior!)\n",
    "- Each tool call has: `id` (tracking), `function.name` (which function), `function.arguments` (parameters)\n",
    "- Function arguments come as JSON strings - you must parse them with `json.loads()`\n",
    "- The `tool_call_id` is critical - you'll need it when sending the function result back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîÅ 5. The Complete Function Calling Loop\n",
    "\n",
    "## Understanding the Full Cycle\n",
    "\n",
    "Now we're ready to see the complete function calling flow. This is where everything comes together!\n",
    "\n",
    "### The 5-Step Process:\n",
    "\n",
    "```\n",
    "STEP 1: Initial API Call\n",
    "‚îú‚îÄ Send user query to LLM with available tools\n",
    "‚îî‚îÄ LLM responds with function call request\n",
    "\n",
    "STEP 2: Extract Function Details\n",
    "‚îú‚îÄ Get tool_call_id (for tracking)\n",
    "‚îú‚îÄ Get function name (which function to call)\n",
    "‚îî‚îÄ Parse arguments (what parameters to use)\n",
    "\n",
    "STEP 3: Execute the Function\n",
    "‚îú‚îÄ Match function name to actual Python function\n",
    "‚îú‚îÄ Call the function with extracted arguments\n",
    "‚îî‚îÄ Get the function result\n",
    "\n",
    "STEP 4: Send Result Back to LLM\n",
    "‚îú‚îÄ Create a \"user\" message with the function result\n",
    "‚îú‚îÄ Format the result clearly for the LLM\n",
    "‚îî‚îÄ Append to conversation history\n",
    "\n",
    "STEP 5: Get Final Response\n",
    "‚îú‚îÄ Make another API call with updated conversation\n",
    "‚îú‚îÄ LLM uses function result to create natural language response\n",
    "‚îî‚îÄ Return final answer to user\n",
    "```\n",
    "\n",
    "## üí° Key Point: This is ONE Conversation\n",
    "\n",
    "**Critical concept:**\n",
    "- We're NOT starting a new conversation for each step\n",
    "- We're CONTINUING the same conversation by appending messages\n",
    "- The messages list grows: User ‚Üí Assistant (tool call) ‚Üí Tool (result) ‚Üí Assistant (final answer)\n",
    "\n",
    "Think of it like a real conversation:\n",
    "1. User: \"What's the weather?\"\n",
    "2. Assistant: *checks weather* (function call)\n",
    "3. System: *weather data* (function result)\n",
    "4. Assistant: \"It's 22¬∞C and sunny!\" (final response)\n",
    "\n",
    "\n",
    "\n",
    "## üí° Key Point: Responses API Difference\n",
    "\n",
    "**Important for gpt-5-nano (Responses API):**\n",
    "- The Responses API does NOT support the `'tool'` role\n",
    "- Supported roles: `'assistant'`, `'system'`, `'developer'`, and `'user'`\n",
    "- Function results must be sent as **'user'** messages\n",
    "- Format: `{\"role\": \"user\", \"content\": f\"Function {name} returned: {result}\"}`\n",
    "\n",
    "This is different from the Chat Completions API which uses a dedicated `'tool'` role.\n",
    "## ‚ö†Ô∏è Common Mistake: Creating New Conversations\n",
    "\n",
    "**Wrong approach:**\n",
    "```python\n",
    "# ‚ùå Starting fresh conversations\n",
    "response1 = client.responses.create(input=[{\"role\": \"user\", \"content\": \"Weather?\"}])\n",
    "# ... execute function ...\n",
    "response2 = client.responses.create(input=[{\"role\": \"user\", \"content\": result}])  # Wrong!\n",
    "```\n",
    "\n",
    "**Correct approach:**\n",
    "```python\n",
    "# ‚úÖ Continuing the same conversation\n",
    "messages = [{\"role\": \"user\", \"content\": \"Weather?\"}]\n",
    "response1 = client.responses.create(input=messages)\n",
    "messages.append(response1.output)  # Add assistant's function call\n",
    "messages.append(tool_result)  # Add function result\n",
    "response2 = client.responses.create(input=messages)  # Continue conversation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Let's Execute the Complete Loop!\n",
    "\n",
    "We'll go through each step clearly with detailed explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ STEP 1: Initial API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî∑ STEP 1: Making initial API call with function calling enabled\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start fresh conversation with a clear, complete query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate weather data.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Bratislava right now? Use Celsius.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üì§ Sending request to LLM...\")\n",
    "print(f\"   User query: '{messages[1]['content']}'\")\n",
    "print(f\"   Available tools: {len(tools)}\")\n",
    "\n",
    "# Make the API call with tools enabled\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"  # Let LLM decide\n",
    ")\n",
    "\n",
    "# Store the assistant's message (which will contain the function call)\n",
    "# Get function calls from response\n",
    "function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "\n",
    "print(\"\\nüì• Response received!\")\n",
    "print(f\"   Response type: {'FUNCTION CALL' if function_calls else 'TEXT'}\")\n",
    "\n",
    "if function_calls:\n",
    "    print(f\"   Function to call: {function_calls[0].name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Step 1 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ STEP 2: Extract Tool Call Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî∑ STEP 2: Extracting function call details\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Access the first (and in this case, only) tool call\n",
    "tool_call = function_calls[0]\n",
    "\n",
    "# Extract the three key pieces of information\n",
    "tool_call_id = tool_call.call_id\n",
    "function_name = tool_call.name\n",
    "function_arguments = json.loads(tool_call.arguments)\n",
    "\n",
    "print(\"üìã Extracted information:\")\n",
    "print(f\"   Tool Call ID: {tool_call_id}\")\n",
    "print(f\"   Function Name: {function_name}\")\n",
    "print(f\"   Function Arguments: {json.dumps(function_arguments, indent=6)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Step 2 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ STEP 3: Execute the Actual Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî∑ STEP 3: Executing the actual Python function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Match the function name to the actual Python function and execute it\n",
    "# We use if/elif to route to the correct function\n",
    "if function_name == \"get_current_weather\":\n",
    "    print(f\"üå§Ô∏è  Calling: get_current_weather()\")\n",
    "    print(f\"   Parameters: location='{function_arguments['location']}', format='{function_arguments['format']}'\")\n",
    "    \n",
    "    # Execute the function with the extracted arguments\n",
    "    # The ** operator unpacks the dictionary as keyword arguments\n",
    "    function_result = get_current_weather(**function_arguments)\n",
    "    \n",
    "elif function_name == \"get_n_day_weather_forecast\":\n",
    "    print(f\"üìÖ Calling: get_n_day_weather_forecast()\")\n",
    "    print(f\"   Parameters: {function_arguments}\")\n",
    "    function_result = get_n_day_weather_forecast(**function_arguments)\n",
    "else:\n",
    "    # This shouldn't happen if our schemas are correct, but good to handle\n",
    "    print(f\"‚ùå Unknown function: {function_name}\")\n",
    "    function_result = {\"error\": \"Unknown function\"}\n",
    "\n",
    "print(\"\\nüìä Function result:\")\n",
    "print(json.dumps(function_result, indent=3))\n",
    "\n",
    "print(\"\\n‚úÖ Step 3 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ STEP 4: Add Function Result to Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî∑ STEP 4: Adding function result to conversation history\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, append the assistant's message (the function call request)\n",
    "# This is important for maintaining conversation context!\n",
    "# Note: With Responses API, we don't need to append the function call request\n",
    "# We only append the tool message with results\n",
    "\n",
    "# Now, create a \"tool\" message with the function result\n",
    "# This tells the LLM \"here's the result of the function you requested\"\n",
    "tool_message = {\n",
    "    \"role\": \"user\",  # Responses API requires 'user' role for function results\n",
    "    \"content\": f\"Function {function_name} returned: {json.dumps(function_result)}\"\n",
    "}\n",
    "\n",
    "# Append the tool message to the conversation\n",
    "messages.append(tool_message)\n",
    "print(\"‚úÖ Appended function result to messages\")\n",
    "\n",
    "print(\"\\nüìù Current conversation structure:\")\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    role = msg.get('role', 'N/A')\n",
    "    if role == 'tool':\n",
    "        print(f\"   {i}. {role.upper()} - Function result from {msg['name']}\")\n",
    "    # (function calls are handled via tool messages)\n",
    "    else:\n",
    "        content_preview = str(msg.get('content', ''))[:50]\n",
    "        print(f\"   {i}. {role.upper()} - {content_preview}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Step 4 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∑ STEP 5: Get Final Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî∑ STEP 5: Getting final natural language response\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make another API call with the updated conversation\n",
    "# Now the conversation includes the function result!\n",
    "# This time we DON'T pass 'tools' because we don't want another function call\n",
    "print(\"üì§ Sending conversation (with function result) back to LLM...\")\n",
    "\n",
    "final_response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages\n",
    "    # Note: No 'tools' parameter - we're done with function calling\n",
    ")\n",
    "\n",
    "# Extract the final response\n",
    "final_message = final_response.output_text\n",
    "\n",
    "print(\"\\nüì• Final response received!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"ü§ñ ASSISTANT:\")\n",
    "print(final_message)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Step 5 complete!\")\n",
    "print(\"\\nüéâ FUNCTION CALLING LOOP COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What Just Happened? Complete Flow Recap\n",
    "\n",
    "Let's trace through the entire conversation flow:\n",
    "\n",
    "### Message Flow:\n",
    "\n",
    "1. **User Message** (Step 1)\n",
    "   - \"What's the weather in Bratislava right now? Use Celsius.\"\n",
    "\n",
    "2. **Assistant Message - Function Call Request** (Step 1 response)\n",
    "   - \"I need to call `get_current_weather` with `location='Bratislava, Slovakia'` and `format='celsius'`\"\n",
    "\n",
    "3. **We Execute the Function** (Steps 2-3)\n",
    "   - Extract function name and parameters\n",
    "   - Call the actual Python function\n",
    "   - Get result: `{\"location\": \"Bratislava\", \"temperature\": 22.5, \"condition\": \"Sunny\"}`\n",
    "\n",
    "4. **User Message - Function Result** (Step 4)\n",
    "   - Send the function result back to the LLM as a user message\n",
    "   - Formatted as: \"Function {name} returned: {result}\"\n",
    "\n",
    "5. **Assistant Message - Final Response** (Step 5)\n",
    "   - LLM uses the function result to create natural language response\n",
    "   - \"The current weather in Bratislava is 22.5¬∞C and sunny!\"\n",
    "\n",
    "### Why This Works:\n",
    "\n",
    "- **Conversation continuity:** We maintained one conversation thread throughout\n",
    "- **Proper message roles:** System ‚Üí User ‚Üí Assistant (tool call) ‚Üí Tool (result) ‚Üí Assistant (answer)\n",
    "- **Tool call tracking:** The `tool_call_id` linked the function result to the original request\n",
    "- **Context preservation:** Each API call had access to the full conversation history\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways: Function Calling Loop\n",
    "\n",
    "- Function calling is a 5-step process: Request ‚Üí Extract ‚Üí Execute ‚Üí Return ‚Üí Respond\n",
    "- This is ONE continuous conversation, not separate interactions\n",
    "- Always append messages to maintain conversation history\n",
    "- The assistant message (with function call) AND the tool message (with result) both get added\n",
    "- The `tool_call_id` is critical for linking results to requests\n",
    "- The final API call doesn't include `tools` parameter (we don't want more function calls)\n",
    "- The LLM uses the function result to create a natural, conversational response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß 6. Helper Function for Reusability\n",
    "\n",
    "Now that we understand the complete flow, let's wrap it into a reusable function. This makes function calling much easier to use in real applications!\n",
    "\n",
    "## Why Create a Helper Function?\n",
    "\n",
    "Instead of manually coding the 5-step process every time, we can:\n",
    "- Encapsulate the logic once\n",
    "- Handle errors gracefully\n",
    "- Make it easier to use function calling throughout our application\n",
    "- Support multiple function calls in one response\n",
    "\n",
    "Let's create a robust helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_function_execution(messages, tools, available_functions):\n",
    "    \"\"\"\n",
    "    Execute a chat completion with automatic function calling.\n",
    "    \n",
    "    This function handles the complete function calling loop:\n",
    "    1. Makes initial API call\n",
    "    2. If LLM requests function calls, executes them\n",
    "    3. Sends results back to LLM\n",
    "    4. Returns final response\n",
    "    \n",
    "    Args:\n",
    "        messages (list): Conversation history\n",
    "        tools (list): Available tool schemas\n",
    "        available_functions (dict): Mapping of function names to actual Python functions\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_response_text, updated_messages)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Starting chat with function execution...\\n\")\n",
    "    \n",
    "    # STEP 1: Initial API call\n",
    "    print(\"üì§ Step 1: Making initial API call...\")\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Get function calls from response\n",
    "    function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "    \n",
    "    # Check if LLM wants to call functions\n",
    "    if not function_calls:\n",
    "        # No function calls - just return the text response\n",
    "        print(\"üí¨ No function calls needed. Returning text response.\\n\")\n",
    "        # Note: For Responses API, we add function calls via tool messages\n",
    "        return response.output_text, messages\n",
    "    \n",
    "    # LLM wants to call one or more functions\n",
    "    print(f\"üìû LLM requested {len(function_calls)} function call(s)\\n\")\n",
    "    \n",
    "    # Append the assistant's function call request\n",
    "    # Note: For Responses API, we add function calls via tool messages\n",
    "    \n",
    "    # STEPS 2-4: Process each function call\n",
    "    for i, tool_call in enumerate(function_calls, 1):\n",
    "        # STEP 2: Extract function details\n",
    "        function_name = tool_call.name\n",
    "        function_arguments = json.loads(tool_call.arguments)\n",
    "        tool_call_id = tool_call.call_id\n",
    "        \n",
    "        print(f\"üîß Function call {i}: {function_name}\")\n",
    "        print(f\"   Arguments: {function_arguments}\")\n",
    "        \n",
    "        # STEP 3: Execute the function\n",
    "        try:\n",
    "            # Check if function exists in our available functions\n",
    "            if function_name not in available_functions:\n",
    "                function_result = {\n",
    "                    \"error\": f\"Function '{function_name}' not found\"\n",
    "                }\n",
    "                print(f\"   ‚ùå Error: Function not found\")\n",
    "            else:\n",
    "                # Execute the function\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_result = function_to_call(**function_arguments)\n",
    "                print(f\"   ‚úÖ Function executed successfully\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle execution errors gracefully\n",
    "            function_result = {\n",
    "                \"error\": f\"Function execution failed: {str(e)}\"\n",
    "            }\n",
    "            print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        \n",
    "        # STEP 4: Add function result to conversation\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Function {function_name} returned: {json.dumps(function_result)}\"\n",
    "        })\n",
    "        print(f\"   üìù Result added to conversation\\n\")\n",
    "    \n",
    "    # STEP 5: Get final response with function results\n",
    "    print(\"üì§ Step 5: Getting final response from LLM...\\n\")\n",
    "    final_response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=messages\n",
    "    )\n",
    "    \n",
    "    final_message = final_response.output_text\n",
    "    \n",
    "    print(\"‚úÖ Function calling complete!\\n\")\n",
    "    \n",
    "    return final_message, messages\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper function created!\")\n",
    "print(\"\\nüìù Usage:\")\n",
    "print(\"   response, updated_messages = chat_with_function_execution(messages, tools, available_functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Helper Function\n",
    "\n",
    "Let's test our helper function with a multi-day forecast request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping function names to actual Python functions\n",
    "# This makes it easy for our helper to route function calls\n",
    "available_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"get_n_day_weather_forecast\": get_n_day_weather_forecast\n",
    "}\n",
    "\n",
    "# Start a new conversation\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate weather data.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the 5-day forecast for San Francisco in Fahrenheit?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing helper function with forecast request\")\n",
    "print(\"=\"*60)\n",
    "print(f\"User query: {test_messages[1]['content']}\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use our helper function!\n",
    "response, updated_messages = chat_with_function_execution(\n",
    "    messages=test_messages,\n",
    "    tools=tools,\n",
    "    available_functions=available_functions\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ü§ñ FINAL RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Success!\n",
    "\n",
    "Our helper function:\n",
    "- ‚úÖ Detected the LLM wanted to call `get_n_day_weather_forecast`\n",
    "- ‚úÖ Extracted the parameters (location, format, num_days)\n",
    "- ‚úÖ Executed the actual Python function\n",
    "- ‚úÖ Sent the result back to the LLM\n",
    "- ‚úÖ Got a natural language response\n",
    "\n",
    "All in one simple function call!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚öôÔ∏è 7. Understanding the `tool_choice` Parameter\n",
    "\n",
    "## What is `tool_choice`?\n",
    "\n",
    "The `tool_choice` parameter gives you control over **whether and how** the LLM uses function calling. It tells the LLM:\n",
    "- Can you call functions?\n",
    "- Should you call functions?\n",
    "- Must you call functions?\n",
    "\n",
    "## Available Options\n",
    "\n",
    "### 1. `tool_choice=\"auto\"` (Default)\n",
    "- **Behavior:** LLM decides whether to call a function or respond with text\n",
    "- **When to use:** Most of the time! This gives the LLM freedom to choose the best approach\n",
    "- **Example:** User asks \"What's the weather?\" ‚Üí LLM calls function. User asks \"Tell me a joke\" ‚Üí LLM responds with text\n",
    "\n",
    "### 2. `tool_choice=\"none\"`\n",
    "- **Behavior:** LLM will NOT call any functions, only text responses\n",
    "- **When to use:** When you want to disable function calling temporarily\n",
    "- **Example:** After getting function results, you don't want another function call\n",
    "\n",
    "### 3. `tool_choice=\"required\"`\n",
    "- **Behavior:** LLM MUST call at least one function (cannot respond with text only)\n",
    "- **When to use:** When you need to force a function call, even if the LLM might normally just answer with text\n",
    "- **Example:** Data extraction tasks where you always need structured output\n",
    "\n",
    "### 4. `tool_choice={\"type\": \"function\", \"name\": \"function_name\"}`\n",
    "- **Behavior:** Forces the LLM to call a SPECIFIC function\n",
    "- **When to use:** When you know exactly which function should be called\n",
    "- **Example:** User clicked a \"Get Weather\" button - you want to force `get_current_weather`\n",
    "\n",
    "## üí° Key Point: When to Force Function Calls\n",
    "\n",
    "Forcing function calls is useful when:\n",
    "- Building structured data extraction tools\n",
    "- Creating button-driven interfaces (\"Check Weather\" button = force weather function)\n",
    "- Ensuring consistent behavior for specific commands\n",
    "- Testing functions during development\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example: `tool_choice=\"none\"`\n",
    "\n",
    "Let's see what happens when we disable function calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing with tool_choice='none'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Paris right now?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make API call with tool_choice=\"none\" - LLM CANNOT call functions\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"none\"  # Disable function calling\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Assistant response (tool_choice='none'):\")\n",
    "print(response.output_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüìù Notice: The LLM responded with text instead of calling the weather function!\")\n",
    "print(\"   Even though the function would provide accurate data, tool_choice='none' prevented it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Example: `tool_choice=\"required\"`\n",
    "\n",
    "Now let's force the LLM to call a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing with tool_choice='required'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm in London and want to know if I need an umbrella. Use Celsius.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make API call with tool_choice=\"required\" - LLM MUST call a function\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"required\"  # Force function calling\n",
    ")\n",
    "\n",
    "# Get function calls from response\n",
    "function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "\n",
    "if function_calls:\n",
    "    print(\"‚úÖ LLM was forced to call a function!\")\n",
    "    print(f\"\\nFunction called: {function_calls[0].name}\")\n",
    "    print(f\"Arguments: {function_calls[0].arguments}\")\n",
    "else:\n",
    "    print(\"‚ùå Unexpected: No function call despite tool_choice='required'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüìù Notice: Even though the user didn't explicitly ask for weather,\")\n",
    "print(\"   tool_choice='required' forced the LLM to call a function to answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Example: Force a Specific Function\n",
    "\n",
    "Let's force the LLM to call a specific function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing with tool_choice forcing specific function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I need weather info for Tokyo in Celsius\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Force the LLM to call get_current_weather specifically\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_current_weather\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get function calls from response\n",
    "function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "\n",
    "if function_calls:\n",
    "    print(\"‚úÖ LLM called the forced function!\")\n",
    "    print(f\"\\nFunction called: {function_calls[0].name}\")\n",
    "    print(f\"Arguments: {function_calls[0].arguments}\")\n",
    "    print(\"\\nüìù Even if the user had asked for a forecast, this would call get_current_weather\")\n",
    "    print(\"   because we forced that specific function.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways: tool_choice Parameter\n",
    "\n",
    "- `tool_choice=\"auto\"` - Default, LLM decides (use this most of the time)\n",
    "- `tool_choice=\"none\"` - Disable function calling (useful after getting function results)\n",
    "- `tool_choice=\"required\"` - LLM must call at least one function\n",
    "- Specific function forcing - Useful for button-driven interfaces and structured extraction\n",
    "- Generally, \"auto\" is the best choice - let the LLM be intelligent about when to call functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üõ°Ô∏è 8. Error Handling & Edge Cases\n",
    "\n",
    "## Why Error Handling Matters\n",
    "\n",
    "In production applications, many things can go wrong:\n",
    "- The LLM might request a function that doesn't exist\n",
    "- Function arguments might be invalid or malformed\n",
    "- The function itself might throw an error\n",
    "- The LLM might generate invalid JSON\n",
    "- Network issues might cause API failures\n",
    "\n",
    "Robust error handling ensures your application doesn't crash and provides useful feedback.\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes to Avoid\n",
    "\n",
    "### 1. Not Checking if `tool_calls` Exists\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# ‚ùå This will crash if no tool calls\n",
    "tool_call = response.output.tool_calls[0]\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# ‚úÖ Always check first\n",
    "if hasattr(response.output, 'tool_calls') and response.output.tool_calls:\n",
    "    tool_call = response.output.tool_calls[0]\n",
    "```\n",
    "\n",
    "### 2. Not Handling JSON Parse Errors\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# ‚ùå Will crash if JSON is invalid\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# ‚úÖ Handle parse errors\n",
    "try:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing arguments: {e}\")\n",
    "    args = {}\n",
    "```\n",
    "\n",
    "### 3. Not Validating Function Exists\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# ‚ùå Will crash if function doesn't exist\n",
    "result = available_functions[function_name](**args)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# ‚úÖ Check function exists\n",
    "if function_name in available_functions:\n",
    "    result = available_functions[function_name](**args)\n",
    "else:\n",
    "    result = {\"error\": f\"Function {function_name} not found\"}\n",
    "```\n",
    "\n",
    "### 4. Forgetting to Convert Function Result to String\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# ‚ùå Tool message content must be a string\n",
    "tool_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": function_result  # ‚ùå This is a dict!\n",
    "}\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# ‚úÖ Convert to JSON string\n",
    "tool_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps(function_result)  # ‚úÖ String\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Robust Function Execution Handler\n",
    "\n",
    "Let's create an improved function handler with comprehensive error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function_call_safely(tool_call, available_functions):\n",
    "    \"\"\"\n",
    "    Safely execute a function call with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        tool_call: The tool call object from LLM response\n",
    "        available_functions (dict): Mapping of function names to Python functions\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'success' boolean and either 'result' or 'error'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Extract function name\n",
    "        function_name = tool_call.name\n",
    "        \n",
    "        # Check if function exists\n",
    "        if function_name not in available_functions:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Function '{function_name}' not found in available functions\",\n",
    "                \"error_type\": \"function_not_found\"\n",
    "            }\n",
    "        \n",
    "        # Parse arguments with error handling\n",
    "        try:\n",
    "            function_arguments = json.loads(tool_call.arguments)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Invalid JSON in function arguments: {str(e)}\",\n",
    "                \"error_type\": \"json_parse_error\"\n",
    "            }\n",
    "        \n",
    "        # Execute the function with error handling\n",
    "        try:\n",
    "            function_to_call = available_functions[function_name]\n",
    "            result = function_to_call(**function_arguments)\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"result\": result\n",
    "            }\n",
    "        \n",
    "        except TypeError as e:\n",
    "            # Wrong arguments provided\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Invalid arguments for function '{function_name}': {str(e)}\",\n",
    "                \"error_type\": \"invalid_arguments\"\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Function execution error\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Function '{function_name}' execution failed: {str(e)}\",\n",
    "                \"error_type\": \"execution_error\"\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch-all for unexpected errors\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Unexpected error: {str(e)}\",\n",
    "            \"error_type\": \"unknown_error\"\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Robust function execution handler created!\")\n",
    "print(\"\\nüõ°Ô∏è This handler protects against:\")\n",
    "print(\"   - Function not found errors\")\n",
    "print(\"   - JSON parsing errors\")\n",
    "print(\"   - Invalid argument errors\")\n",
    "print(\"   - Function execution errors\")\n",
    "print(\"   - Unexpected errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Testing Error Handling\n",
    "\n",
    "Let's test our robust handler with different error scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock tool call for testing\n",
    "class MockToolCall:\n",
    "    def __init__(self, function_name, arguments_json):\n",
    "        self.function = type('obj', (object,), {\n",
    "            'name': function_name,\n",
    "            'arguments': arguments_json\n",
    "        })\n",
    "\n",
    "print(\"üß™ Testing error handling scenarios\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Valid function call\n",
    "print(\"\\nTest 1: Valid function call\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris, France\", \"format\": \"celsius\"}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "if result['success']:\n",
    "    print(f\"Weather data: {result['result']}\")\n",
    "\n",
    "# Test 2: Function doesn't exist\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 2: Non-existent function\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_stock_price\",  # This function doesn't exist\n",
    "    '{\"symbol\": \"AAPL\"}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "# Test 3: Invalid JSON\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 3: Invalid JSON in arguments\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris\", invalid json here}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "# Test 4: Missing required argument\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 4: Missing required argument\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris, France\"}'  # Missing 'format' argument\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚úÖ All error scenarios handled gracefully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways: Error Handling\n",
    "\n",
    "- Always check if `tool_calls` exists before accessing it\n",
    "- Wrap JSON parsing in try/except blocks\n",
    "- Validate that requested functions exist before calling them\n",
    "- Handle function execution errors gracefully\n",
    "- Convert function results to JSON strings for tool messages\n",
    "- Return meaningful error messages that help debug issues\n",
    "- Consider error types to handle different failures appropriately\n",
    "- Never let unhandled exceptions crash your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ 9. Complete End-to-End Example\n",
    "\n",
    "Let's put everything together with a complete, production-ready example. This demonstrates:\n",
    "- Proper conversation flow\n",
    "- Error handling\n",
    "- Multiple function calls\n",
    "- Natural conversation\n",
    "\n",
    "## üåü Complete Weather Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_assistant(user_query):\n",
    "    \"\"\"\n",
    "    Complete weather assistant with function calling.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's question\n",
    "    \n",
    "    Returns:\n",
    "        str: The assistant's response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize conversation\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate, real-time weather data. Be friendly and informative.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Available functions\n",
    "    available_functions = {\n",
    "        \"get_current_weather\": get_current_weather,\n",
    "        \"get_n_day_weather_forecast\": get_n_day_weather_forecast\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Initial API call\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Get function calls from response\n",
    "        function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n",
    "        \n",
    "        # Check if function calling is needed\n",
    "        if not function_calls:\n",
    "            # No function call needed - return text response\n",
    "            return response.output_text\n",
    "        \n",
    "        # Step 2-4: Process function calls\n",
    "        # Note: For Responses API, we add function calls via tool messages\n",
    "        \n",
    "        for tool_call in function_calls:\n",
    "            # Execute function safely\n",
    "            execution_result = execute_function_call_safely(tool_call, available_functions)\n",
    "            \n",
    "            # Prepare result for LLM\n",
    "            if execution_result['success']:\n",
    "                result_content = json.dumps(execution_result['result'])\n",
    "            else:\n",
    "                result_content = json.dumps({\n",
    "                    \"error\": execution_result['error']\n",
    "                })\n",
    "            \n",
    "            # Add to conversation\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Function {tool_call.name} returned: {result_content}\"\n",
    "            })\n",
    "        \n",
    "        # Step 5: Get final response\n",
    "        final_response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=messages\n",
    "        )\n",
    "        \n",
    "        return final_response.output_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Complete weather assistant created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Complete Assistant\n",
    "\n",
    "Let's test with different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üå§Ô∏è  WEATHER ASSISTANT - DEMO\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Current weather\n",
    "print(\"\\nüë§ User: What's the current weather in London? Use Celsius.\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"What's the current weather in London? Use Celsius.\")\n",
    "print(f\"ü§ñ Assistant: {response}\")\n",
    "\n",
    "# Test 2: Forecast\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüë§ User: Give me a 3-day forecast for New York in Fahrenheit.\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"Give me a 3-day forecast for New York in Fahrenheit.\")\n",
    "print(f\"ü§ñ Assistant: {response}\")\n",
    "\n",
    "# Test 3: Vague query (should ask for clarification)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüë§ User: What's the weather like?\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"What's the weather like?\")\n",
    "print(f\"ü§ñ Assistant: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüéâ Complete weather assistant working perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö 10. Best Practices & Key Takeaways\n",
    "\n",
    "## ‚úÖ Best Practices for Function Calling\n",
    "\n",
    "### 1. **Write Clear, Descriptive Function Descriptions**\n",
    "   - The `description` field is critical for the LLM's decision-making\n",
    "   - Be specific about WHEN to use each function\n",
    "   - Distinguish similar functions clearly\n",
    "   - Example: \"Get the current weather\" vs \"Get an N-day weather forecast\"\n",
    "\n",
    "### 2. **Be Specific About Parameter Types and Constraints**\n",
    "   - Use `enum` to restrict values when possible\n",
    "   - Provide examples in descriptions (\"e.g., San Francisco, CA\")\n",
    "   - Specify format requirements clearly\n",
    "   - Mark required parameters appropriately\n",
    "\n",
    "### 3. **Always Validate Function Exists Before Calling**\n",
    "   - Check if function name is in your available_functions dict\n",
    "   - Return meaningful error messages if not found\n",
    "   - Don't let unknown functions crash your application\n",
    "\n",
    "### 4. **Handle Errors Gracefully**\n",
    "   - Wrap JSON parsing in try/except\n",
    "   - Catch function execution errors\n",
    "   - Return errors to the LLM in the tool message\n",
    "   - Let the LLM explain errors to users naturally\n",
    "\n",
    "### 5. **Keep Functions Focused and Simple**\n",
    "   - Each function should do ONE thing well\n",
    "   - Avoid complex multi-purpose functions\n",
    "   - Make functions easy to test independently\n",
    "   - Keep parameter lists manageable (3-5 parameters max)\n",
    "\n",
    "### 6. **Use tool_choice Strategically**\n",
    "   - Default to \"auto\" for natural behavior\n",
    "   - Use \"required\" for structured data extraction\n",
    "   - Use \"none\" when you don't want function calls\n",
    "   - Force specific functions only when necessary\n",
    "\n",
    "### 7. **Test With Various Queries**\n",
    "   - Clear queries with all information\n",
    "   - Vague queries missing details\n",
    "   - Edge cases and error scenarios\n",
    "   - Multiple function calls in one request\n",
    "\n",
    "### 8. **Consider Cost**\n",
    "   - Each function call = multiple API requests (initial + final)\n",
    "   - Monitor token usage carefully\n",
    "   - Cache results when appropriate\n",
    "   - Use cheaper models (like gpt-5-nano) when possible\n",
    "\n",
    "### 9. **Use Mock Functions for Learning/Testing**\n",
    "   - Develop with mock data before connecting real APIs\n",
    "   - Test function calling logic without external dependencies\n",
    "   - Add real API calls only after logic is solid\n",
    "\n",
    "### 10. **Maintain Conversation Context Properly**\n",
    "   - Always append messages, don't create new conversations\n",
    "   - Include both assistant message (tool call) and tool message (result)\n",
    "   - Preserve conversation history throughout the loop\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Pro Tips\n",
    "\n",
    "### 1. **Batch Function Definitions**\n",
    "Create a separate file for function definitions and schemas:\n",
    "```python\n",
    "# functions.py\n",
    "def get_weather(...):\n",
    "    ...\n",
    "\n",
    "WEATHER_TOOLS = [ ... ]\n",
    "```\n",
    "This keeps your main code clean and organized.\n",
    "\n",
    "### 2. **Log Function Calls**\n",
    "In production, log:\n",
    "- Which functions are being called\n",
    "- What parameters are being used\n",
    "- Execution time and results\n",
    "- Any errors that occur\n",
    "\n",
    "This helps debug issues and understand usage patterns.\n",
    "\n",
    "### 3. **Version Your Function Schemas**\n",
    "If you change function signatures, version them:\n",
    "```python\n",
    "\"get_weather_v2\"\n",
    "```\n",
    "This prevents breaking changes for existing implementations.\n",
    "\n",
    "### 4. **Add Parameter Validation**\n",
    "Even though the LLM should provide valid parameters, validate them in your functions:\n",
    "```python\n",
    "def get_weather(location, format):\n",
    "    if format not in [\"celsius\", \"fahrenheit\"]:\n",
    "        raise ValueError(\"Invalid format\")\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è What NOT to Do\n",
    "\n",
    "### 1. **Don't Start New Conversations**\n",
    "‚ùå Creating a fresh message list for each API call\n",
    "‚úÖ Append to the same messages list throughout the loop\n",
    "\n",
    "### 2. **Don't Forget to Parse Arguments**\n",
    "‚ùå Using `tool_call.function.arguments` directly (it's a string!)\n",
    "‚úÖ Parse with `json.loads()` first\n",
    "\n",
    "### 3. **Don't Return Non-String Content in Tool Messages**\n",
    "‚ùå `\"content\": function_result` (dict)\n",
    "‚úÖ `\"content\": json.dumps(function_result)` (string)\n",
    "\n",
    "### 4. **Don't Make Function Descriptions Too Vague**\n",
    "‚ùå \"Get weather\" (too vague, when should this be used?)\n",
    "‚úÖ \"Get the current weather in a given location. Use this for present conditions.\"\n",
    "\n",
    "### 5. **Don't Ignore Errors**\n",
    "‚ùå Letting exceptions crash your application\n",
    "‚úÖ Catch errors and handle them gracefully\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Summary: What You've Learned\n",
    "\n",
    "You now know how to:\n",
    "- ‚úÖ Understand what function calling is and why it's powerful\n",
    "- ‚úÖ Define tool schemas that describe functions to the LLM\n",
    "- ‚úÖ Distinguish between schemas (descriptions) and implementations (actual code)\n",
    "- ‚úÖ Parse and understand LLM responses (text vs tool calls)\n",
    "- ‚úÖ Execute the complete 5-step function calling loop\n",
    "- ‚úÖ Maintain conversation context properly\n",
    "- ‚úÖ Use the tool_choice parameter effectively\n",
    "- ‚úÖ Handle errors gracefully in production code\n",
    "- ‚úÖ Build reusable helper functions\n",
    "- ‚úÖ Apply best practices for robust function calling\n",
    "\n",
    "**Congratulations!** You're now ready to build AI applications that can interact with the real world! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üèãÔ∏è 11. Practice Exercises\n",
    "\n",
    "Now it's your turn to practice! Try these exercises to solidify your understanding:\n",
    "\n",
    "## üìù Exercise 1: Calculator Functions\n",
    "\n",
    "**Goal:** Create a calculator assistant that can perform basic math operations.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create four functions: `add()`, `subtract()`, `multiply()`, `divide()`\n",
    "2. Each function should take two numbers as parameters\n",
    "3. Create tool schemas for each function\n",
    "4. Build a calculator assistant that uses these functions\n",
    "5. Test with queries like:\n",
    "   - \"What's 15 + 27?\"\n",
    "   - \"Divide 144 by 12\"\n",
    "   - \"What's 8 times 9?\"\n",
    "\n",
    "**Bonus:** Add a `power()` function for exponentiation.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Exercise 2: Database Query Functions\n",
    "\n",
    "**Goal:** Create mock database query functions for a product catalog.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create mock functions:\n",
    "   - `search_products(query, category, max_price)`\n",
    "   - `get_product_details(product_id)`\n",
    "   - `check_inventory(product_id)`\n",
    "2. Make functions return mock data (use random values)\n",
    "3. Create tool schemas with clear descriptions\n",
    "4. Build an assistant that helps users find products\n",
    "5. Test with queries like:\n",
    "   - \"Show me laptops under $1000\"\n",
    "   - \"What are the details for product ID 12345?\"\n",
    "   - \"Is product 67890 in stock?\"\n",
    "\n",
    "**Bonus:** Add a `compare_products()` function that takes multiple product IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Tips for Exercises:\n",
    "\n",
    "- Start with the function implementations first\n",
    "- Then create the tool schemas\n",
    "- Test each function independently before integrating\n",
    "- Use the helper functions we created in this notebook\n",
    "- Add error handling as you go\n",
    "- Test with both clear and vague queries\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge: Multi-Step Workflow\n",
    "\n",
    "For an advanced challenge, try creating a booking system that requires multiple function calls:\n",
    "\n",
    "1. `check_availability(date, service_type)` - Check if time slots are available\n",
    "2. `book_appointment(date, time, service_type, customer_name)` - Make a booking\n",
    "3. `cancel_booking(booking_id)` - Cancel a booking\n",
    "\n",
    "The assistant should:\n",
    "- First check availability before booking\n",
    "- Confirm details with the user\n",
    "- Handle the booking\n",
    "- Provide a booking confirmation\n",
    "\n",
    "This requires chaining multiple function calls together intelligently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì 12. Summary & Next Steps\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the Function Calling tutorial! You now have the foundational knowledge to build AI applications that can:\n",
    "- Access real-time data\n",
    "- Query databases and APIs\n",
    "- Perform actions on behalf of users\n",
    "- Integrate with existing systems\n",
    "\n",
    "## üìö What We Covered\n",
    "\n",
    "1. **Theory:** Understanding what function calling is and why it matters\n",
    "2. **Tool Schemas:** Defining function descriptions for the LLM\n",
    "3. **Response Handling:** Parsing and understanding LLM responses\n",
    "4. **Function Loop:** The complete 5-step execution cycle\n",
    "5. **Helper Functions:** Building reusable, production-ready code\n",
    "6. **tool_choice:** Controlling function calling behavior\n",
    "7. **Error Handling:** Building robust, fault-tolerant applications\n",
    "8. **Best Practices:** Professional patterns and anti-patterns\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "### Immediate Practice:\n",
    "1. Complete the practice exercises above\n",
    "2. Modify the weather assistant with your own functions\n",
    "3. Build a simple chatbot for your use case\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "1. **Parallel Function Calling:** Handling multiple function calls simultaneously\n",
    "2. **Function Chaining:** Using output from one function as input to another\n",
    "3. **Real API Integration:** Connect to actual APIs (weather, databases, etc.)\n",
    "4. **Streaming Responses:** Handle function calling with streaming\n",
    "5. **State Management:** Maintain conversation state across sessions\n",
    "6. **Security:** Validate and sanitize function parameters\n",
    "7. **Rate Limiting:** Handle API quotas and rate limits\n",
    "8. **Caching:** Cache function results for efficiency\n",
    "\n",
    "### Related Topics:\n",
    "- **Prompt Engineering:** Craft better system prompts for function calling\n",
    "- **RAG (Retrieval Augmented Generation):** Combine function calling with knowledge bases\n",
    "- **Agents:** Build autonomous agents that plan and execute multi-step tasks\n",
    "- **Embeddings:** Use function calling with semantic search\n",
    "\n",
    "## üìñ Resources for Further Learning\n",
    "\n",
    "- **OpenAI Function Calling Guide:** https://platform.openai.com/docs/guides/function-calling\n",
    "- **OpenAI Cookbook:** https://cookbook.openai.com\n",
    "- **API Reference:** https://platform.openai.com/docs/api-reference\n",
    "\n",
    "## üí° Remember:\n",
    "\n",
    "- Function calling bridges conversational AI with real-world actions\n",
    "- The LLM requests functions, YOU execute them (you're in control)\n",
    "- Maintain conversation context by appending to the messages list\n",
    "- Always handle errors gracefully\n",
    "- Start with mock functions, add real integrations later\n",
    "- Test thoroughly with various query types\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Thank You!\n",
    "\n",
    "Thank you for completing this tutorial. Function calling is a powerful capability that opens up countless possibilities for AI applications. \n",
    "\n",
    "Keep building, keep experimenting, and most importantly - have fun creating amazing AI-powered applications! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
