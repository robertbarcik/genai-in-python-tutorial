{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé• Extracting Knowledge from Videos\n",
    "\n",
    "## What This Notebook Teaches\n",
    "\n",
    "In this notebook, you'll learn how to **automatically extract knowledge from educational and training videos** by converting them to text and cleaning the transcripts using AI. This is a powerful workflow for processing video content at scale.\n",
    "\n",
    "### üéØ The Scenario\n",
    "\n",
    "Imagine you're a lecturer designing an educational course, and you have a collection of old educational videos from previous seminars, workshops, or lectures. You want to extract the knowledge from these videos to:\n",
    "- Create course materials\n",
    "- Generate lecture notes\n",
    "- Build a knowledge base\n",
    "- Repurpose content for different formats\n",
    "\n",
    "### ‚ö†Ô∏è The Problem\n",
    "\n",
    "**Manually transcribing video content is extremely time-consuming!**\n",
    "\n",
    "Consider this: If you have 6 hours of video content, manual transcription could take:\n",
    "- **4-5 person days** of work (for typing)\n",
    "- Additional time for proofreading and formatting\n",
    "- Risk of transcription errors and inconsistencies\n",
    "\n",
    "This is simply not scalable or cost-effective.\n",
    "\n",
    "### ‚ú® The Solution\n",
    "\n",
    "We'll **automate this entire process** using OpenAI's powerful AI models:\n",
    "1. **Whisper** - For accurate speech-to-text transcription\n",
    "2. **GPT-5-nano** - For cleaning and formatting the transcripts\n",
    "\n",
    "What would take days manually can be completed in minutes!\n",
    "\n",
    "### üìπ About the Videos\n",
    "\n",
    "üí° **Note:** Open and watch one minute from the videos you'll upload. They may have varying quality in terms of delivery - some speakers may be more articulate than others, there might be background noise, or different recording conditions.\n",
    "\n",
    "**In this notebook, we'll work with 2 videos to demonstrate the complete workflow.** This allows you to see the entire process from start to finish, understand each step, and then apply it to larger video collections.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ File Paths Setup\n",
    "\n",
    "Before we begin, let's define where our files will be stored:\n",
    "- **Input:** Videos will be uploaded to `/content/videos/`\n",
    "- **Intermediate:** Audio files will be saved to `/content/audio_files/`\n",
    "- **Intermediate:** Raw transcripts will be saved to `/content/transcripts/`\n",
    "- **Output:** Cleaned transcripts will be saved to `/content/cleaned_transcripts/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_folder_path = '/content/videos'\n",
    "audio_files_dir = '/content/audio_files'\n",
    "transcripts_dir = '/content/transcripts'\n",
    "cleaned_dir = '/content/cleaned_transcripts'\n",
    "\n",
    "print(\"‚úÖ File paths configured:\")\n",
    "print(f\"  üì• Input videos: {input_folder_path}\")\n",
    "print(f\"  üîä Audio files: {audio_files_dir}\")\n",
    "print(f\"  üìù Raw transcripts: {transcripts_dir}\")\n",
    "print(f\"  ‚ú® Cleaned transcripts: {cleaned_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Setup\n",
    "\n",
    "### Installing Required Dependencies\n",
    "\n",
    "We'll need two main Python packages:\n",
    "- **`openai`** - To interact with OpenAI's Whisper and GPT models\n",
    "- **`moviepy`** - To extract audio from video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai moviepy\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë API Key Configuration\n",
    "\n",
    "To use OpenAI's APIs, you need an API key. We'll set this up with two methods:\n",
    "\n",
    "**Method 1 (Recommended):** Store your API key in Colab Secrets\n",
    "- Click the üîë icon in the left sidebar\n",
    "- Click \"Add new secret\"\n",
    "- Name: `OPENAI_API_KEY`\n",
    "- Value: Your API key\n",
    "- Toggle \"Notebook access\" ON\n",
    "\n",
    "**Method 2 (Fallback):** Enter your API key manually when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use for cleaning\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model for text cleaning: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Suppress warnings from moviepy library (these are harmless compatibility warnings)\nimport warnings\nwarnings.filterwarnings('ignore', category=SyntaxWarning)\n\n# Import necessary libraries\nimport os\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom moviepy.editor import VideoFileClip\n\n# Initialize OpenAI client\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nprint(\"‚úÖ All libraries imported and OpenAI client initialized!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Planning Our Work\n",
    "\n",
    "Before diving into the code, let's break down this task into **three clear steps**:\n",
    "\n",
    "### Step 1: Convert Videos to Audio Format Only\n",
    "**Why?** Video files contain both visual and audio information, but we only need the audio for transcription. By extracting just the audio:\n",
    "- We reduce file sizes significantly (audio files are much smaller)\n",
    "- Processing becomes faster and cheaper\n",
    "- Whisper only analyzes audio anyway, so we're not losing any relevant information\n",
    "\n",
    "### Step 2: Send Audio Files to Whisper Model to Obtain Transcriptions\n",
    "**Why?** Whisper is OpenAI's state-of-the-art speech-to-text model. It will convert our audio files into text transcripts automatically.\n",
    "\n",
    "### Step 3: Use GPT to Clean These Transcriptions\n",
    "**Why?** Raw transcripts from speech-to-text models often contain:\n",
    "- Filler words (\"um\", \"uh\", \"you know\")\n",
    "- Run-on sentences without proper punctuation\n",
    "- Repetitions and false starts\n",
    "- Lack of proper formatting\n",
    "\n",
    "GPT models excel at text refinement and can transform these raw transcripts into clean, professional lecture notes.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Creating Working Directories\n",
    "\n",
    "Due to the breakdown into these three steps, let's **create directories to store created audio files and intermediate transcriptions**. This keeps our workflow organized and makes it easy to review outputs at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(input_folder_path, exist_ok=True)\n",
    "os.makedirs(audio_files_dir, exist_ok=True)\n",
    "os.makedirs(transcripts_dir, exist_ok=True)\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ All working directories created:\")\n",
    "print(f\"  üìÅ {input_folder_path}\")\n",
    "print(f\"  üìÅ {audio_files_dir}\")\n",
    "print(f\"  üìÅ {transcripts_dir}\")\n",
    "print(f\"  üìÅ {cleaned_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì§ Upload Your Videos\n",
    "\n",
    "Now, let's upload the 2 video files you want to process. These should be `.mp4` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Please upload your video files (.mp4)...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to the videos directory\n",
    "for filename in uploaded.keys():\n",
    "    src = filename\n",
    "    dst = os.path.join(input_folder_path, filename)\n",
    "    os.rename(src, dst)\n",
    "    print(f\"  ‚úÖ Moved {filename} to {input_folder_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All videos uploaded to {input_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéµ Step 1: Video to Audio Conversion\n",
    "\n",
    "### Why Audio-Only?\n",
    "\n",
    "**Video files are large and contain visual information we don't need.** When transcribing speech, only the audio track matters. By extracting just the audio:\n",
    "- Audio files are **significantly smaller** than videos (typically 10-20x smaller)\n",
    "- Processing is **cheaper** (less data to transfer and store)\n",
    "- Whisper **only analyzes audio** anyway, so we're not losing any information\n",
    "\n",
    "For example, a 100MB video might contain only 5-10MB of audio data. Why send 90MB of unnecessary visual data to the API?\n",
    "\n",
    "### üí° Key Points:\n",
    "- We use `moviepy` library to extract audio from video files\n",
    "- Videos are in `.mp4` format\n",
    "- Audio files are saved as `.mp3` (compressed, efficient format)\n",
    "- Each video gets a corresponding audio file with the same base name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéµ Step 1: Converting videos to audio...\\n\")\n",
    "\n",
    "# Loop through all files in the input folder\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    # Check if the file is a video file (.mp4)\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(input_folder_path, filename)\n",
    "        \n",
    "        # Create audio filename (replace .mp4 with .mp3)\n",
    "        audio_filename = filename.replace('.mp4', '.mp3')\n",
    "        audio_path = os.path.join(audio_files_dir, audio_filename)\n",
    "        \n",
    "        try:\n",
    "            print(f\"  üîÑ Processing: {filename}\")\n",
    "            \n",
    "            # Load the video file\n",
    "            video_clip = VideoFileClip(video_path)\n",
    "            \n",
    "            # Extract audio from video\n",
    "            audio_clip = video_clip.audio\n",
    "            \n",
    "            # Save audio as MP3 file\n",
    "            audio_clip.write_audiofile(audio_path, verbose=False, logger=None)\n",
    "            \n",
    "            # Close clips to free up resources\n",
    "            audio_clip.close()\n",
    "            video_clip.close()\n",
    "            \n",
    "            print(f\"  ‚úÖ Audio extracted: {audio_filename}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {filename}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n‚úÖ Step 1 Complete: All videos converted to audio files!\")\n",
    "print(f\"   Audio files saved to: {audio_files_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé§ Step 2: Speech-to-Text via Whisper\n",
    "\n",
    "### What is Whisper?\n",
    "\n",
    "**Whisper is OpenAI's state-of-the-art automatic speech recognition (ASR) system.** It was trained on 680,000 hours of multilingual and multitask supervised data collected from the web. This massive training dataset leads to:\n",
    "- **High accuracy** across different accents and speaking styles\n",
    "- **Multilingual support** for 98 languages\n",
    "- **Robust performance** even with background noise or audio quality issues\n",
    "\n",
    "### üìö Whisper API Documentation\n",
    "\n",
    "The Whisper API accepts audio files and returns transcriptions. Here are the key parameters:\n",
    "\n",
    "- **`model`**: `\"whisper-1\"` (the current Whisper model version)\n",
    "- **`file`**: Audio file object (supports mp3, mp4, wav, and more)\n",
    "- **`response_format`**: `'text'` returns plain text, `'json'` returns detailed JSON with timestamps\n",
    "- **File size limit**: 25 MB (for larger files, split them into chunks)\n",
    "- **Supported formats**: mp3, mp4, mpeg, mpga, m4a, wav, webm\n",
    "\n",
    "### üí° Key Points:\n",
    "- Whisper automatically detects the language (no need to specify)\n",
    "- It adds punctuation and formatting automatically\n",
    "- Processing time is typically 20-30% of the audio duration\n",
    "- We save raw transcripts to review before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé§ Step 2: Transcribing audio files with Whisper...\\n\")\n",
    "\n",
    "# Loop through all audio files\n",
    "for filename in os.listdir(audio_files_dir):\n",
    "    # Check if the file is an audio file (.mp3)\n",
    "    if filename.endswith('.mp3'):\n",
    "        audio_path = os.path.join(audio_files_dir, filename)\n",
    "        \n",
    "        # Create transcript filename (replace .mp3 with .txt)\n",
    "        transcript_filename = filename.replace('.mp3', '_transcript.txt')\n",
    "        transcript_path = os.path.join(transcripts_dir, transcript_filename)\n",
    "        \n",
    "        try:\n",
    "            print(f\"  üîÑ Transcribing: {filename}\")\n",
    "            \n",
    "            # Open audio file and send to Whisper API\n",
    "            with open(audio_path, 'rb') as audio_file:\n",
    "                transcript_text = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file,\n",
    "                    response_format='text'\n",
    "                )\n",
    "            \n",
    "            # Save the transcript to a text file\n",
    "            with open(transcript_path, 'w', encoding='utf-8') as transcript_file:\n",
    "                transcript_file.write(transcript_text)\n",
    "            \n",
    "            print(f\"  ‚úÖ Transcript saved: {transcript_filename}\")\n",
    "            print(f\"     Preview: {transcript_text[:100]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error transcribing {filename}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n‚úÖ Step 2 Complete: All audio files transcribed!\")\n",
    "print(f\"   Transcripts saved to: {transcripts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ú® Step 3: Cleaning Transcriptions via GPT\n",
    "\n",
    "### Why Clean Transcripts?\n",
    "\n",
    "**Whisper captures speech accurately but includes imperfections that make transcripts hard to read and use.** Raw speech-to-text output often contains:\n",
    "\n",
    "- **Filler words**: \"um\", \"uh\", \"you know\", \"like\", \"so\"\n",
    "- **Run-on sentences**: Lack of proper sentence breaks and punctuation\n",
    "- **Repetitions**: People often repeat words or rephrase thoughts mid-sentence\n",
    "- **False starts**: Beginning a sentence one way, then starting over\n",
    "- **Poor formatting**: Missing paragraph breaks, inconsistent capitalization\n",
    "\n",
    "**GPT models excel at text refinement.** They can transform raw transcripts into clean, professional lecture notes that are:\n",
    "- Easy to read and understand\n",
    "- Properly formatted with clear structure\n",
    "- Free of distracting filler words\n",
    "- Suitable for use as course materials\n",
    "\n",
    "### ü§ñ About GPT-5-nano\n",
    "\n",
    "We're using **GPT-5-nano** for this task because:\n",
    "- It's **cost-efficient** ($0.05 per 1M input tokens, $0.40 per 1M output tokens)\n",
    "- It's **fast** - processes text quickly\n",
    "- It's **sufficient** for text cleaning tasks (we don't need the largest model for this)\n",
    "\n",
    "### üí° Key Points:\n",
    "- We use a carefully crafted prompt to guide the cleaning process\n",
    "- The model is instructed to stay faithful to original content (no hallucinations)\n",
    "- Results are saved as separate \"cleaned\" files for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cleaning task description\n",
    "task_description = (\n",
    "    \"You are a helpful assistant tasked with cleaning up lecture notes. \"\n",
    "    \"Make the text coherent, correct any typos, and format it for a lecturer \"\n",
    "    \"to use as speaking notes. Keep the text friendly, to the point, and ensure \"\n",
    "    \"it does not deviate from the original content.\"\n",
    ")\n",
    "\n",
    "print(\"‚ú® Step 3: Cleaning transcripts with GPT-5-nano...\\n\")\n",
    "print(f\"üéØ Task: {task_description}\\n\")\n",
    "\n",
    "# Loop through all transcript files\n",
    "for filename in os.listdir(transcripts_dir):\n",
    "    # Check if the file is a transcript (.txt)\n",
    "    if filename.endswith('_transcript.txt'):\n",
    "        transcript_path = os.path.join(transcripts_dir, filename)\n",
    "        \n",
    "        # Create cleaned filename\n",
    "        cleaned_filename = filename.replace('_transcript.txt', '_cleaned.txt')\n",
    "        cleaned_path = os.path.join(cleaned_dir, cleaned_filename)\n",
    "        \n",
    "        try:\n",
    "            print(f\"  üîÑ Cleaning: {filename}\")\n",
    "            \n",
    "            # Read the raw transcript\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcript_content = f.read()\n",
    "            \n",
    "            # Create the input for GPT with task description and transcript\n",
    "            input_text = f\"{task_description}\\n\\nTranscript to clean:\\n{transcript_content}\"\n",
    "            \n",
    "            # Call GPT-5-nano API to clean the transcript\n",
    "            response = client.responses.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                input=input_text\n",
    "            )\n",
    "            \n",
    "            # Extract the cleaned text from response\n",
    "            cleaned_text = response.output_text\n",
    "            \n",
    "            # Save the cleaned transcript\n",
    "            with open(cleaned_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text)\n",
    "            \n",
    "            print(f\"  ‚úÖ Cleaned transcript saved: {cleaned_filename}\")\n",
    "            print(f\"     Preview: {cleaned_text[:100]}...\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error cleaning {filename}: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n‚úÖ Step 3 Complete: All transcripts cleaned!\")\n",
    "print(f\"   Cleaned transcripts saved to: {cleaned_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Results Comparison\n",
    "\n",
    "Let's compare the raw transcript from Whisper with the cleaned version from GPT to see the improvement!\n",
    "\n",
    "### Example: Before vs. After Cleaning\n",
    "\n",
    "**Raw Transcript (from Whisper):**\n",
    "```\n",
    "Um, so today we're going to, uh, talk about machine learning and, you know, \n",
    "how it's used in like different applications and stuff. So basically, um, \n",
    "machine learning is when you have algorithms that can learn from data without \n",
    "being explicitly programmed and, uh, yeah, it's really powerful. So, um, \n",
    "there are different types like supervised learning where you have labeled data \n",
    "and unsupervised learning where you don't and, uh, reinforcement learning too.\n",
    "```\n",
    "\n",
    "**Cleaned Version (from GPT):**\n",
    "```\n",
    "Today we'll discuss machine learning and its applications in various domains. \n",
    "Machine learning refers to algorithms that can learn from data without being \n",
    "explicitly programmed. This technology is remarkably powerful.\n",
    "\n",
    "There are three main types of machine learning:\n",
    "1. Supervised learning - uses labeled data\n",
    "2. Unsupervised learning - works with unlabeled data  \n",
    "3. Reinforcement learning - learns through trial and error\n",
    "```\n",
    "\n",
    "### üéØ Key Improvements:\n",
    "- **Removed filler words**: \"um\", \"uh\", \"like\", \"you know\", \"stuff\"\n",
    "- **Better structure**: Clear paragraphs and formatting\n",
    "- **Professional tone**: More suitable for lecture notes\n",
    "- **Enhanced readability**: Easier to follow and understand\n",
    "- **Same content**: No deviation from the original meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• View Your Results\n",
    "\n",
    "Let's display a side-by-side comparison of one of your actual transcripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first transcript file for comparison\n",
    "transcript_files = [f for f in os.listdir(transcripts_dir) if f.endswith('_transcript.txt')]\n",
    "\n",
    "if transcript_files:\n",
    "    # Read the first raw transcript\n",
    "    sample_transcript = transcript_files[0]\n",
    "    transcript_path = os.path.join(transcripts_dir, sample_transcript)\n",
    "    \n",
    "    with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    # Read the corresponding cleaned transcript\n",
    "    cleaned_sample = sample_transcript.replace('_transcript.txt', '_cleaned.txt')\n",
    "    cleaned_path = os.path.join(cleaned_dir, cleaned_sample)\n",
    "    \n",
    "    with open(cleaned_path, 'r', encoding='utf-8') as f:\n",
    "        cleaned_text = f.read()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÑ File: {sample_transcript}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîç RAW TRANSCRIPT (First 500 characters):\")\n",
    "    print(\"-\"*80)\n",
    "    print(raw_text[:500])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\n‚ú® CLEANED TRANSCRIPT (First 500 characters):\")\n",
    "    print(\"-\"*80)\n",
    "    print(cleaned_text[:500])\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No transcripts found to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ Download Your Cleaned Transcripts\n",
    "\n",
    "You can download all the cleaned transcripts to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create a zip file with all cleaned transcripts\n",
    "zip_filename = '/content/cleaned_transcripts.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    for filename in os.listdir(cleaned_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(cleaned_dir, filename)\n",
    "            zipf.write(file_path, filename)\n",
    "\n",
    "print(\"üì¶ Created zip file with all cleaned transcripts\")\n",
    "print(\"‚¨áÔ∏è Downloading...\")\n",
    "files.download(zip_filename)\n",
    "print(\"‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitations and Considerations\n",
    "\n",
    "While this workflow is powerful and automated, it's important to understand its limitations:\n",
    "\n",
    "### üé§ Whisper Limitations:\n",
    "\n",
    "1. **Audio Quality Matters**\n",
    "   - Best results with clear speech and minimal background noise\n",
    "   - Struggles with heavy accents, mumblings, or poor recording quality\n",
    "   - Multiple overlapping speakers can cause confusion\n",
    "\n",
    "2. **File Size Constraint**\n",
    "   - **25MB file size limit** per API request\n",
    "   - Longer audio files (typically >1 hour of good quality audio) need to be split into chunks\n",
    "   - For production use, implement audio chunking logic\n",
    "\n",
    "3. **Language Detection**\n",
    "   - While Whisper supports 98 languages, accuracy varies by language\n",
    "   - English has the highest accuracy due to more training data\n",
    "   - Code-switching (mixing languages) can be challenging\n",
    "\n",
    "### ü§ñ GPT Cleaning Limitations:\n",
    "\n",
    "1. **Context Length**\n",
    "   - Very long transcripts may exceed model context limits\n",
    "   - May need to process in sections for multi-hour videos\n",
    "\n",
    "2. **Potential Over-Editing**\n",
    "   - Model might occasionally rephrase content too much\n",
    "   - Important to review critical transcripts manually\n",
    "\n",
    "3. **Domain-Specific Terms**\n",
    "   - May occasionally misinterpret technical jargon or specialized terminology\n",
    "   - Consider adding domain-specific instructions to the prompt\n",
    "\n",
    "### üí° Best Practices:\n",
    "\n",
    "- **Always review critical content** - Don't rely solely on automated processing for important materials\n",
    "- **Test with sample videos** first to ensure quality meets your needs\n",
    "- **Use good source material** - Better input quality = better output\n",
    "- **Keep original files** - Don't delete raw transcripts; they're useful for comparison\n",
    "- **Iterate on prompts** - Customize the cleaning prompt for your specific use case\n",
    "\n",
    "### üéØ When This Workflow Works Best:\n",
    "\n",
    "‚úÖ Educational lectures with clear speakers  \n",
    "‚úÖ Podcast transcription  \n",
    "‚úÖ Interview recordings  \n",
    "‚úÖ Training video documentation  \n",
    "‚úÖ Webinar content extraction  \n",
    "\n",
    "‚ùå Not ideal for: Multi-speaker debates, heavily accented speech, very low-quality audio, or content with critical legal/medical importance requiring 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully learned how to:\n",
    "1. ‚úÖ Extract audio from video files\n",
    "2. ‚úÖ Transcribe speech to text using Whisper\n",
    "3. ‚úÖ Clean and format transcripts using GPT-5-nano\n",
    "4. ‚úÖ Automate a process that would take days manually\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "- Apply this workflow to your entire video library\n",
    "- Customize the cleaning prompt for your specific needs\n",
    "- Explore additional OpenAI features like:\n",
    "  - Translation (Whisper can translate to English)\n",
    "  - Summarization (use GPT to create summaries)\n",
    "  - Q&A generation (create quiz questions from transcripts)\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [OpenAI Whisper Documentation](https://platform.openai.com/docs/guides/speech-to-text)\n",
    "- [OpenAI GPT Documentation](https://platform.openai.com/docs/guides/text-generation)\n",
    "- [MoviePy Documentation](https://zulko.github.io/moviepy/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy learning!** üéì"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}