{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# üìä Extracting Structured Information from Unstructured Text\n",
    "\n",
    "Welcome! This comprehensive tutorial will teach you how to extract structured, actionable data from unstructured text using OpenAI's API.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Extract structured data** from messy, unstructured text (emails, tickets, reports)\n",
    "2. **Choose the right format** - CSV, JSON, or Pydantic models for your use case\n",
    "3. **Parse complex information** - Handle nested data, arrays, and multiple items\n",
    "4. **Validate extractions** - Ensure data quality and catch errors early\n",
    "5. **Save results** - Store extracted data in files for downstream use\n",
    "6. **Handle edge cases** - Deal with missing info, contradictions, and ambiguity\n",
    "7. **Build production systems** - Create robust, scalable extraction pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-theory",
   "metadata": {},
   "source": [
    "## üéØ Why Extract Structured Data?\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "In IT support and services, information flows in **unstructured formats**:\n",
    "\n",
    "- üìß **Emails**: \"Hi, my laptop won't start. I think it's the battery. Can someone help? - John from Marketing\"\n",
    "- üí¨ **Chat messages**: \"printer broken room 304 need toner asap\"\n",
    "- üìû **Verbal reports**: \"Sarah mentioned something about the network being slow in Building B\"\n",
    "- üìù **Handwritten notes**: Notes from a phone call or site visit\n",
    "\n",
    "But to **take action**, this information needs to be **structured**:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_name\": \"John\",\n",
    "  \"department\": \"Marketing\",\n",
    "  \"issue\": \"Laptop won't start\",\n",
    "  \"suspected_cause\": \"Battery\",\n",
    "  \"urgency\": \"medium\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Current Challenges\n",
    "\n",
    "‚ùå **Manual data entry is:**\n",
    "- **Slow** - Takes time away from actual problem-solving\n",
    "- **Error-prone** - Typos, missed fields, inconsistent formatting\n",
    "- **Doesn't scale** - Can't handle high ticket volumes\n",
    "- **Inconsistent** - Different people extract different information\n",
    "\n",
    "### The Solution: LLM-Powered Extraction\n",
    "\n",
    "‚úÖ **Large Language Models can:**\n",
    "- Extract key information accurately and consistently\n",
    "- Handle natural language variations (\"urgent\", \"asap\", \"critical\")\n",
    "- Infer missing information from context\n",
    "- Structure data in your required format (JSON, CSV, database schema)\n",
    "- Process hundreds of items in minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-concepts",
   "metadata": {},
   "source": [
    "## üîë Key Concepts\n",
    "\n",
    "### Structured vs. Unstructured Data\n",
    "\n",
    "**Unstructured Data:**\n",
    "- Free-form text, no fixed format\n",
    "- Examples: Emails, chat messages, documents, verbal reports\n",
    "- Hard for computers to process directly\n",
    "\n",
    "**Structured Data:**\n",
    "- Organized in a predefined format\n",
    "- Examples: Database tables, JSON objects, CSV files\n",
    "- Easy to query, analyze, and integrate with other systems\n",
    "\n",
    "### Data Extraction vs. Data Parsing\n",
    "\n",
    "**Data Extraction:**\n",
    "- Identifying and pulling out specific information from unstructured text\n",
    "- Requires understanding context and meaning\n",
    "- Example: Finding user name, issue type, urgency from an email\n",
    "\n",
    "**Data Parsing:**\n",
    "- Converting extracted information into a structured format\n",
    "- Example: Creating a JSON object with extracted fields\n",
    "\n",
    "LLMs excel at **both** - they understand context AND can output structured formats.\n",
    "\n",
    "### Token Costs for Extraction\n",
    "\n",
    "**Good news:** Extraction tasks work well with cheaper models!\n",
    "\n",
    "- **gpt-5-nano**: $0.05/1M input tokens, $0.40/1M output tokens\n",
    "- A typical support ticket extraction:\n",
    "  - Input: ~300 tokens (the email/ticket)\n",
    "  - Output: ~100 tokens (structured JSON)\n",
    "  - Cost: ~$0.00005 (less than 1/10th of a cent per ticket!)\n",
    "\n",
    "üí° **Key takeaway:** You can process thousands of tickets for just a few dollars.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-title",
   "metadata": {},
   "source": [
    "# üîß Setup\n",
    "\n",
    "Let's configure the environment and install required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-deps",
   "metadata": {},
   "source": "## üì¶ Install Dependencies\n\nWe'll install five libraries:\n- **openai**: Official OpenAI Python client for API access\n- **pydantic**: Data validation and settings management using Python type hints\n- **email-validator**: Email validation for Pydantic's EmailStr type\n- **tqdm**: Progress bars for batch processing\n- **pandas**: Data manipulation and CSV handling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q openai pydantic email-validator tqdm pandas"
  },
  {
   "cell_type": "markdown",
   "id": "setup-apikey",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-apikey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-imports",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport json\nimport csv\nfrom datetime import datetime\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nfrom pydantic import BaseModel, EmailStr, Field, field_validator\nimport pandas as pd\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"‚úÖ OpenAI client initialized!\")"
  },
  {
   "cell_type": "markdown",
   "id": "formats-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã Understanding Output Formats\n",
    "\n",
    "LLMs can extract data into different formats. Each format has specific use cases in IT support.\n",
    "\n",
    "We'll explore three main formats:\n",
    "1. **CSV** - Simple tabular data\n",
    "2. **JSON** - Complex, nested structures\n",
    "3. **Pydantic Models** - Type-safe, validated data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-csv-title",
   "metadata": {},
   "source": [
    "## Format A: CSV (Comma-Separated Values)\n",
    "\n",
    "### üìñ When to Use CSV\n",
    "\n",
    "**Best for:** Simple tabular data with similar items\n",
    "\n",
    "**Use cases in IT:**\n",
    "- üì¶ Asset inventories and equipment lists\n",
    "- üñ•Ô∏è Hardware tracking spreadsheets\n",
    "- üìä Simple databases that need Excel compatibility\n",
    "- üìà Reports for non-technical stakeholders\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Easy to open in Excel or Google Sheets\n",
    "- ‚úÖ Simple structure, widely supported\n",
    "- ‚úÖ Human-readable and editable\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå No nesting (can't represent complex relationships)\n",
    "- ‚ùå All values are strings (no native data types)\n",
    "- ‚ùå Harder to represent one-to-many relationships\n",
    "\n",
    "### üíª Practical Example: Conference Room Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data: Unstructured description of conference room equipment\n",
    "inventory_description = \"\"\"\n",
    "Conference Room A has a Samsung 65-inch display (serial: SAMS-2024-001), \n",
    "a Logitech Rally camera (serial: LOG-CAM-445), and a Poly Studio phone system (serial: POLY-899-X).\n",
    "\n",
    "Conference Room B contains two Dell OptiPlex 7090 computers (serials: DELL-PC-1023 and DELL-PC-1024),\n",
    "an LG 55-inch display (serial: LG-DSP-3301), and a Jabra Speak 750 speakerphone (serial: JAB-750-229).\n",
    "\n",
    "Conference Room C is equipped with a Microsoft Surface Hub 2S (serial: MSFT-HUB-8821),\n",
    "a Cisco Webex Room Kit (serial: CISCO-WX-4492), and an HP laptop (serial: HP-LT-9933).\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt for CSV format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract equipment inventory information from the text below and format it as CSV.\n",
    "\n",
    "Use these exact column headers: room,device_type,manufacturer,model,serial_number\n",
    "\n",
    "Rules:\n",
    "- One row per device\n",
    "- Include header row\n",
    "- Use commas as separators\n",
    "- No quotes around values unless they contain commas\n",
    "\n",
    "Text:\n",
    "{inventory_description}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting inventory data to CSV format...\\n\")\n",
    "\n",
    "# Make API call using Responses API\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}  # Low verbosity for structured output\n",
    ")\n",
    "\n",
    "csv_output = response.output_text.strip()\n",
    "\n",
    "print(\"üìä Extracted CSV Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(csv_output)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "csv_file_path = \"/content/inventory_data.csv\"\n",
    "with open(csv_file_path, 'w') as f:\n",
    "    f.write(csv_output)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {csv_file_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv-verify",
   "metadata": {},
   "outputs": [],
   "source": "# Verify the CSV is valid by parsing it back\nprint(\"üîç Verifying CSV format...\\n\")\n\n# Method 1: Using pandas\ndf = pd.read_csv(csv_file_path)\nprint(\"‚úÖ CSV is valid and readable!\\n\")\nprint(f\"üìä Found {len(df)} devices across {df['room'].nunique()} rooms\\n\")\nprint(\"Preview:\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)"
  },
  {
   "cell_type": "markdown",
   "id": "format-json-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Format B: JSON (JavaScript Object Notation)\n",
    "\n",
    "### üìñ When to Use JSON\n",
    "\n",
    "**Best for:** Complex, nested data structures\n",
    "\n",
    "**Use cases in IT:**\n",
    "- üé´ Support tickets with multiple fields and categories\n",
    "- üë§ User information with device specifications\n",
    "- üêõ Error reports with nested details\n",
    "- üîó API integration and data exchange\n",
    "- üìÅ Configuration files and settings\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Flexible structure, supports nesting\n",
    "- ‚úÖ Native data types (strings, numbers, booleans, arrays, objects)\n",
    "- ‚úÖ Standard format for APIs and modern applications\n",
    "- ‚úÖ Easy to parse in any programming language\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå More verbose than CSV\n",
    "- ‚ùå Less human-friendly for simple tables\n",
    "\n",
    "üí° **Key Point:** JSON is the preferred format for most extraction tasks due to its flexibility.\n",
    "\n",
    "### üíª Practical Example: Support Ticket Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data: Unstructured support ticket email\n",
    "support_ticket = \"\"\"\n",
    "From: jennifer.martinez@company.com\n",
    "Subject: Urgent - Cannot Access Email\n",
    "\n",
    "Hi IT Support,\n",
    "\n",
    "I'm Jennifer Martinez from the Sales department (employee ID: EMP-5834). \n",
    "My Outlook keeps crashing whenever I try to open it. I've tried restarting \n",
    "my computer twice but the problem persists.\n",
    "\n",
    "This is really urgent because I need to send quotes to clients today. \n",
    "My phone number is 555-0192 if you need to call me.\n",
    "\n",
    "Please help ASAP!\n",
    "\n",
    "Thanks,\n",
    "Jennifer\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt for JSON format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information from the email below and format it as JSON.\n",
    "\n",
    "Include these fields:\n",
    "- user_name: Full name of the user\n",
    "- department: User's department\n",
    "- employee_id: Employee ID if mentioned\n",
    "- contact_email: Email address\n",
    "- contact_phone: Phone number if mentioned\n",
    "- issue_summary: Brief summary of the issue\n",
    "- application: The application having issues\n",
    "- urgency: Low, Medium, High, or Critical (infer from context)\n",
    "- actions_tried: List of troubleshooting steps user already attempted\n",
    "\n",
    "Email:\n",
    "{support_ticket}\n",
    "\n",
    "Respond with ONLY valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting ticket data to JSON format...\\n\")\n",
    "\n",
    "# Make API call\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "json_output = response.output_text.strip()\n",
    "\n",
    "# Parse and pretty-print the JSON\n",
    "ticket_data = json.loads(json_output)\n",
    "\n",
    "print(\"üìã Extracted JSON Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(ticket_data, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "json_file_path = \"/content/ticket_data.json\"\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(ticket_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {json_file_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and demonstrate JSON parsing\n",
    "print(\"üîç Verifying JSON format and demonstrating access...\\n\")\n",
    "\n",
    "# Read back from file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "print(\"‚úÖ JSON is valid!\\n\")\n",
    "print(\"Accessing specific fields:\")\n",
    "print(f\"  User: {loaded_data['user_name']}\")\n",
    "print(f\"  Department: {loaded_data['department']}\")\n",
    "print(f\"  Issue: {loaded_data['issue_summary']}\")\n",
    "print(f\"  Urgency: {loaded_data['urgency']}\")\n",
    "print(f\"  Actions tried: {', '.join(loaded_data['actions_tried'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-pydantic-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Format C: Pydantic Models (Type-Safe Validation)\n",
    "\n",
    "### üìñ When to Use Pydantic\n",
    "\n",
    "**What it is:** Python library for data validation using type hints\n",
    "\n",
    "**Key benefit:** Automatic validation against a defined schema\n",
    "\n",
    "**When useful:**\n",
    "- üè≠ **Production systems** feeding databases\n",
    "- üîó **API integration** requiring specific formats\n",
    "- üêõ **Early error detection** before data reaches critical systems\n",
    "- üë• **Team collaboration** with clear data contracts\n",
    "- üìä **Type safety** ensuring fields are correct types\n",
    "\n",
    "### üîç Comparison: JSON vs. Pydantic\n",
    "\n",
    "Let's see the difference between plain JSON (no validation) and Pydantic (validated):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-problem",
   "metadata": {},
   "source": [
    "#### ‚ùå Problem: Plain JSON Without Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-no-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plain JSON - problems discovered later\n",
    "bad_ticket = {\n",
    "    \"user_name\": \"John\",  # Missing last name\n",
    "    \"employee_id\": \"12345\",  # Wrong format (should be EMP-####)\n",
    "    \"contact_email\": \"john.company.com\",  # Invalid email (missing @)\n",
    "    \"urgency\": \"super urgent\",  # Invalid value (should be Low/Medium/High/Critical)\n",
    "    # Missing required field: issue_summary\n",
    "}\n",
    "\n",
    "print(\"‚ùå Plain JSON - No validation happens:\")\n",
    "print(json.dumps(bad_ticket, indent=2))\n",
    "print(\"\\n‚ö†Ô∏è This bad data could be saved to database, causing errors later!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-solution",
   "metadata": {},
   "source": [
    "#### ‚úÖ Solution: Pydantic Model with Validation\n",
    "\n",
    "Let's create a Pydantic model that validates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define urgency levels as an Enum\n",
    "class UrgencyLevel(str, Enum):\n",
    "    LOW = \"Low\"\n",
    "    MEDIUM = \"Medium\"\n",
    "    HIGH = \"High\"\n",
    "    CRITICAL = \"Critical\"\n",
    "\n",
    "# Define the Pydantic model for a support ticket\n",
    "class SupportTicket(BaseModel):\n",
    "    # Required fields\n",
    "    user_name: str = Field(..., min_length=2, description=\"Full name of the user\")\n",
    "    employee_id: str = Field(..., pattern=r'^EMP-\\d{4}$', description=\"Employee ID in format EMP-####\")\n",
    "    contact_email: EmailStr = Field(..., description=\"Valid email address\")\n",
    "    issue_summary: str = Field(..., min_length=10, description=\"Brief summary of the issue\")\n",
    "    urgency: UrgencyLevel = Field(..., description=\"Urgency level\")\n",
    "    \n",
    "    # Optional fields with defaults\n",
    "    department: Optional[str] = None\n",
    "    contact_phone: Optional[str] = None\n",
    "    application: Optional[str] = None\n",
    "    actions_tried: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Custom validator for phone numbers\n",
    "    @field_validator('contact_phone')\n",
    "    @classmethod\n",
    "    def validate_phone(cls, v):\n",
    "        if v and len(v.replace('-', '').replace(' ', '')) < 7:\n",
    "            raise ValueError('Phone number too short')\n",
    "        return v\n",
    "\n",
    "print(\"‚úÖ Pydantic model defined with validation rules!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-demo-bad",
   "metadata": {},
   "source": [
    "#### üß™ Test 1: Invalid Data (Validation Catches Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-bad-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create ticket with bad data\n",
    "print(\"üß™ Testing with INVALID data...\\n\")\n",
    "\n",
    "try:\n",
    "    bad_ticket_validated = SupportTicket(\n",
    "        user_name=\"J\",  # Too short\n",
    "        employee_id=\"12345\",  # Wrong format\n",
    "        contact_email=\"invalid-email\",  # Invalid email\n",
    "        issue_summary=\"Broken\",  # Too short\n",
    "        urgency=\"super urgent\"  # Invalid urgency level\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Validation failed (as expected):\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")\n",
    "    print(f\"\\nErrors found:\")\n",
    "    \n",
    "    # Parse validation errors\n",
    "    if hasattr(e, 'errors'):\n",
    "        for error in e.errors():\n",
    "            field = error['loc'][0]\n",
    "            message = error['msg']\n",
    "            print(f\"  ‚Ä¢ {field}: {message}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Validation prevented bad data from being processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-demo-good",
   "metadata": {},
   "source": [
    "#### üß™ Test 2: Valid Data (Validation Succeeds)\n",
    "\n",
    "Now let's use LLM to extract data in Pydantic-compatible format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data for extraction\n",
    "ticket_email = \"\"\"\n",
    "From: robert.chen@company.com\n",
    "Subject: VPN Connection Issues - Need Help\n",
    "\n",
    "Hello IT,\n",
    "\n",
    "This is Robert Chen from Engineering (EMP-7821). My VPN client keeps \n",
    "disconnecting every 5-10 minutes. I've already tried:\n",
    "- Restarting the VPN client\n",
    "- Rebooting my laptop\n",
    "- Checking my internet connection\n",
    "\n",
    "This is blocking my work as I need to access the development servers.\n",
    "Please treat this as high priority.\n",
    "\n",
    "You can reach me at 555-0198.\n",
    "\n",
    "Thanks,\n",
    "Robert\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt that ensures Pydantic-compatible format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information and format as JSON matching this schema:\n",
    "\n",
    "Required fields:\n",
    "- user_name (string, min 2 chars): Full name\n",
    "- employee_id (string, format: EMP-####): Employee ID  \n",
    "- contact_email (string): Valid email address\n",
    "- issue_summary (string, min 10 chars): Brief issue description\n",
    "- urgency (string): Must be exactly one of: \"Low\", \"Medium\", \"High\", \"Critical\"\n",
    "\n",
    "Optional fields:\n",
    "- department (string or null): Department name\n",
    "- contact_phone (string or null): Phone number\n",
    "- application (string or null): Application name\n",
    "- actions_tried (array of strings): Steps user already tried\n",
    "\n",
    "Email:\n",
    "{ticket_email}\n",
    "\n",
    "Return ONLY valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting ticket with Pydantic validation...\\n\")\n",
    "\n",
    "# Extract data\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "extracted_json = response.output_text.strip()\n",
    "extracted_data = json.loads(extracted_json)\n",
    "\n",
    "print(\"üìã Extracted JSON:\")\n",
    "print(json.dumps(extracted_data, indent=2))\n",
    "print()\n",
    "\n",
    "# Validate with Pydantic\n",
    "try:\n",
    "    validated_ticket = SupportTicket(**extracted_data)\n",
    "    print(\"‚úÖ Pydantic validation PASSED!\\n\")\n",
    "    \n",
    "    print(\"Validated ticket details:\")\n",
    "    print(f\"  User: {validated_ticket.user_name}\")\n",
    "    print(f\"  Employee ID: {validated_ticket.employee_id}\")\n",
    "    print(f\"  Email: {validated_ticket.contact_email}\")\n",
    "    print(f\"  Urgency: {validated_ticket.urgency.value}\")\n",
    "    print(f\"  Issue: {validated_ticket.issue_summary}\")\n",
    "    print(f\"  Actions tried: {len(validated_ticket.actions_tried)} steps\")\n",
    "    \n",
    "    # Save validated ticket\n",
    "    validated_file_path = \"/content/validated_ticket.json\"\n",
    "    with open(validated_file_path, 'w') as f:\n",
    "        json.dump(validated_ticket.model_dump(), f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved validated ticket to: {validated_file_path}\")\n",
    "    print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-summary",
   "metadata": {},
   "source": [
    "### üìä Pydantic Benefits Summary\n",
    "\n",
    "**Pydantic provides:**\n",
    "\n",
    "‚úÖ **Type safety** - Fields must be correct types  \n",
    "‚úÖ **Format validation** - Email, patterns, length checks  \n",
    "‚úÖ **Required field checking** - No missing critical data  \n",
    "‚úÖ **Enum validation** - Only allowed values accepted  \n",
    "‚úÖ **Custom validators** - Business logic validation  \n",
    "‚úÖ **Clear error messages** - Know exactly what's wrong  \n",
    "‚úÖ **IDE support** - Auto-completion and type hints  \n",
    "\n",
    "üí° **When to use:** Production systems, database integration, team projects requiring data contracts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examples-title",
   "metadata": {},
   "source": [
    "# üíº Practical Examples\n",
    "\n",
    "Now let's apply what we've learned to realistic IT support scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-title",
   "metadata": {},
   "source": [
    "## Example 1: Support Ticket Parsing (Comprehensive)\n",
    "\n",
    "We'll extract information from support tickets, starting with basic extraction and progressing to complex nested structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1a-title",
   "metadata": {},
   "source": [
    "### Part A: Basic Ticket Extraction\n",
    "\n",
    "Extract standard ticket fields from a user email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic mock data: User email about laptop issue\n",
    "laptop_issue_email = \"\"\"\n",
    "From: sarah.williams@company.com\n",
    "Date: 2025-01-15 09:23 AM\n",
    "Subject: Laptop Battery Problem - Urgent\n",
    "\n",
    "Hi Support Team,\n",
    "\n",
    "I'm Sarah Williams from the Marketing department, employee number EMP-4567.\n",
    "My work laptop (Dell Latitude 5420) isn't holding a charge anymore. The battery \n",
    "drains completely within an hour even when I'm just using Word and email.\n",
    "\n",
    "I have a client presentation tomorrow afternoon at 2 PM and really need this \n",
    "working by then. I've tried using a different power outlet and the charger \n",
    "seems to work fine (the charging light comes on).\n",
    "\n",
    "Could someone please help? You can reach me at ext. 4523 or my cell 555-0167.\n",
    "\n",
    "Thank you!\n",
    "Sarah Williams\n",
    "Marketing Department\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information from this email and return as JSON.\n",
    "\n",
    "Include these fields:\n",
    "- user_name: Full name\n",
    "- department: Department name\n",
    "- employee_id: Employee ID\n",
    "- contact_email: Email address\n",
    "- contact_phone: Phone number (if mentioned)\n",
    "- device_info: Device description (manufacturer and model)\n",
    "- issue_summary: Concise summary of the issue\n",
    "- issue_details: Detailed description\n",
    "- urgency: Low, Medium, High, or Critical (infer from context and deadline)\n",
    "- deadline_context: Any time-sensitive information\n",
    "- troubleshooting_done: What user already tried\n",
    "\n",
    "Email:\n",
    "{laptop_issue_email}\n",
    "\n",
    "Return ONLY valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting basic ticket information...\\n\")\n",
    "\n",
    "# Make API call\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"medium\"}\n",
    ")\n",
    "\n",
    "ticket_json = response.output_text.strip()\n",
    "ticket_data = json.loads(ticket_json)\n",
    "\n",
    "print(\"üìã Extracted Ticket Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(ticket_data, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "basic_ticket_path = \"/content/basic_ticket.json\"\n",
    "with open(basic_ticket_path, 'w') as f:\n",
    "    json.dump(ticket_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {basic_ticket_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "\n",
    "# Display key insights\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(f\"  ‚Ä¢ User: {ticket_data['user_name']} ({ticket_data['department']})\")\n",
    "print(f\"  ‚Ä¢ Issue: {ticket_data['issue_summary']}\")\n",
    "print(f\"  ‚Ä¢ Urgency: {ticket_data['urgency']}\")\n",
    "print(f\"  ‚Ä¢ Deadline: {ticket_data['deadline_context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1b-title",
   "metadata": {},
   "source": [
    "### Part B: Advanced - Nested Device Specifications\n",
    "\n",
    "Now let's extract more complex data with nested objects and arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1b",
   "metadata": {},
   "outputs": [],
   "source": "# Complex mock data: Workstation issue with detailed system specs\nworkstation_issue = \"\"\"\nFrom: michael.thompson@company.com\nSubject: Workstation Performance Issues - Graphics Freezing\n\nHello IT,\n\nI'm Michael Thompson, CAD Engineer in the Design department (EMP-9012).\n\nMy workstation is experiencing severe performance problems. The system specs are:\n- Dell Precision 7920 Tower (Service Tag: 5XYZ789)\n- Intel Xeon Gold 6248R processor, 24 cores, running at 3.0 GHz\n- 128GB DDR4 RAM, ECC memory\n- NVIDIA Quadro RTX 5000 with 16GB GDDR6\n- Two storage drives: 1TB NVMe SSD (OS drive) and 4TB SATA SSD (data drive)\n- Running Windows 11 Pro for Workstations, version 23H2\n\nThe screen freezes when I'm rendering 3D models in SolidWorks. Sometimes the \nentire application crashes. I suspect it might be the graphics card overheating \nbecause I hear the fans going crazy.\n\nThis is blocking a project deadline on Friday. Please help!\n\nPhone: 555-0184\nEmail: michael.thompson@company.com\n\nThanks,\nMichael\n\"\"\"\n\n# Extraction prompt for nested structure - simplified and clearer\nextraction_prompt = f\"\"\"\nExtract support ticket information with nested device specifications from the email below.\n\nIMPORTANT: Return ONLY valid JSON with NO extra text before or after. Use this exact structure:\n\n{{\n  \"user_name\": \"full name\",\n  \"department\": \"department name\",\n  \"employee_id\": \"employee ID\",\n  \"contact_email\": \"email address\",\n  \"contact_phone\": \"phone number\",\n  \"issue_summary\": \"brief issue summary\",\n  \"suspected_cause\": \"suspected cause if mentioned\",\n  \"urgency\": \"Low or Medium or High or Critical\",\n  \"affected_application\": \"application name\",\n  \"device\": {{\n    \"manufacturer\": \"device manufacturer\",\n    \"model\": \"device model\",\n    \"service_tag\": \"service tag\",\n    \"processor\": {{\n      \"brand\": \"processor brand\",\n      \"model\": \"processor model\",\n      \"cores\": 24,\n      \"speed_ghz\": 3.0\n    }},\n    \"ram\": {{\n      \"capacity_gb\": 128,\n      \"type\": \"DDR4 ECC\"\n    }},\n    \"gpu\": {{\n      \"manufacturer\": \"GPU manufacturer\",\n      \"model\": \"GPU model\",\n      \"memory_gb\": 16\n    }},\n    \"storage\": [\n      {{\n        \"capacity_tb\": 1.0,\n        \"type\": \"NVMe SSD\",\n        \"purpose\": \"OS drive\"\n      }},\n      {{\n        \"capacity_tb\": 4.0,\n        \"type\": \"SATA SSD\",\n        \"purpose\": \"data drive\"\n      }}\n    ],\n    \"operating_system\": {{\n      \"name\": \"Windows 11 Pro for Workstations\",\n      \"version\": \"23H2\"\n    }}\n  }}\n}}\n\nEmail to extract from:\n{workstation_issue}\n\nReturn ONLY the JSON object with no additional text.\n\"\"\"\n\nprint(\"üîÑ Extracting complex nested ticket information...\\\\n\")\n\n# Make API call with lower verbosity for cleaner JSON output\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}  # Low verbosity for cleaner structured output\n)\n\ncomplex_json = response.output_text.strip()\n\n# Try to parse JSON with error handling\ntry:\n    complex_data = json.loads(complex_json)\n    \n    print(\"üìã Extracted Complex Nested Data:\")\n    print(\"=\" * 80)\n    print(json.dumps(complex_data, indent=2))\n    print(\"=\" * 80)\n    \n    # Save to file\n    complex_ticket_path = \"/content/complex_ticket_nested.json\"\n    with open(complex_ticket_path, 'w') as f:\n        json.dump(complex_data, f, indent=2)\n    \n    print(f\"\\\\nüíæ Saved to: {complex_ticket_path}\")\n    print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n    \n    # Demonstrate accessing nested data\n    print(\"\\\\nüîç Accessing Nested Data:\")\n    print(f\"  ‚Ä¢ User: {complex_data['user_name']}\")\n    print(f\"  ‚Ä¢ Device: {complex_data['device']['manufacturer']} {complex_data['device']['model']}\")\n    print(f\"  ‚Ä¢ CPU: {complex_data['device']['processor']['brand']} {complex_data['device']['processor']['model']}\")\n    print(f\"  ‚Ä¢ RAM: {complex_data['device']['ram']['capacity_gb']}GB {complex_data['device']['ram']['type']}\")\n    print(f\"  ‚Ä¢ GPU: {complex_data['device']['gpu']['manufacturer']} {complex_data['device']['gpu']['model']}\")\n    print(f\"  ‚Ä¢ Storage drives: {len(complex_data['device']['storage'])}\")\n    for i, drive in enumerate(complex_data['device']['storage'], 1):\n        print(f\"    - Drive {i}: {drive['capacity_tb']}TB {drive['type']} ({drive['purpose']})\")\n\nexcept json.JSONDecodeError as e:\n    print(f\"‚ùå JSON Parsing Error: {e}\")\n    print(f\"\\\\nüìÑ Raw response from API:\")\n    print(\"=\" * 80)\n    print(complex_json)\n    print(\"=\" * 80)\n    print(f\"\\\\nüí° Tip: The model returned invalid JSON. Try adjusting the prompt or using a different model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "example1-summary",
   "metadata": {},
   "source": [
    "### üìä Key Teaching Points\n",
    "\n",
    "From Example 1, we learned:\n",
    "\n",
    "**Flat vs. Nested Structures:**\n",
    "- ‚úÖ Flat structure: Simple tickets with top-level fields only\n",
    "- ‚úÖ Nested structure: Complex tickets with related data grouped in objects\n",
    "\n",
    "**Handling Arrays:**\n",
    "- ‚úÖ Storage devices are represented as an array of objects\n",
    "- ‚úÖ Each item in the array has the same structure\n",
    "- ‚úÖ Easy to iterate through and process programmatically\n",
    "\n",
    "**Extracting Implied Information:**\n",
    "- ‚úÖ Urgency inferred from context (\"blocking project deadline on Friday\" ‚Üí High)\n",
    "- ‚úÖ Suspected cause identified from symptoms described\n",
    "- ‚úÖ Device purpose categorized (OS drive vs. data drive)\n",
    "\n",
    "üí° **Best Practice:** Use nested structures when data has clear relationships (device ‚Üí components). This keeps data organized and easier to work with.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration1-complete",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ ITERATION 1 COMPLETE\n",
    "\n",
    "## Sections Created:\n",
    "1. ‚úÖ Introduction (Theory, Business Problem, Key Concepts)\n",
    "2. ‚úÖ Setup (API Config, Dependencies, Imports)\n",
    "3. ‚úÖ Understanding Output Formats (CSV, JSON, Pydantic)\n",
    "4. ‚úÖ Practical Examples - Example 1 (Support Ticket Parsing)\n",
    "\n",
    "## Files Saved:\n",
    "- `/content/inventory_data.csv` - Conference room equipment inventory\n",
    "- `/content/ticket_data.json` - Basic support ticket extraction\n",
    "- `/content/validated_ticket.json` - Pydantic-validated ticket\n",
    "- `/content/basic_ticket.json` - Laptop issue ticket with standard fields\n",
    "- `/content/complex_ticket_nested.json` - Workstation ticket with nested device specs\n",
    "\n",
    "## What You've Learned:\n",
    "‚úÖ Why structured data extraction matters in IT support  \n",
    "‚úÖ When to use CSV, JSON, or Pydantic models  \n",
    "‚úÖ How to extract data using gpt-5-nano  \n",
    "‚úÖ Validation techniques with Pydantic  \n",
    "‚úÖ Handling nested structures and arrays  \n",
    "‚úÖ Saving extracted data to files  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ready for Iteration 2\n",
    "\n",
    "**Please review and approve before I continue.**\n",
    "\n",
    "Iteration 2 will add:\n",
    "- Example 2: Multiple Error Extraction (JSON Arrays)\n",
    "- Example 3: Hardware Inventory (CSV Format)\n",
    "- Data Validation Techniques\n",
    "- File Generation Patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2k7jlkmrkhs",
   "source": "## Example 2: Multiple Error Extraction (JSON Array)\n\nLet's extract multiple items from a single message into a structured array.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2965glitedd",
   "source": "# Mock data: User reporting multiple different errors\nmultiple_errors_report = \"\"\"\nFrom: david.kumar@company.com\nSubject: Multiple System Errors Today - Please Help\n\nHi IT Team,\n\nI've been experiencing several different errors on my computer today (EMP-3392):\n\n1. Around 9 AM, I got a Windows error saying \"DRIVER_IRQL_NOT_LESS_OR_EQUAL\" \n   with a blue screen. The computer restarted automatically.\n\n2. At 10:30 AM, Adobe Acrobat crashed with error code 0xc0000005 when I tried \n   to open a PDF file from a client.\n\n3. Just before lunch, my network printer showed \"Error 49.FF04\" and stopped \n   printing completely. Other people can still print to it.\n\n4. Around 2 PM, I got another blue screen with \"SYSTEM_SERVICE_EXCEPTION\" error.\n\n5. Finally, at 3:15 PM, Outlook gave me error 0x80040600 and won't send emails now.\n\nI'm really frustrated because this is affecting my work. These errors seem random \nbut I'm worried something serious is wrong with my computer.\n\nPlease help!\nDavid Kumar\nSales Department\n\"\"\"\n\n# Extraction prompt for array structure\nextraction_prompt = f\"\"\"\nExtract ALL errors from this user report and format as JSON.\n\nReturn JSON with this structure:\n{{\n  \"user_name\": \"string\",\n  \"employee_id\": \"string\", \n  \"department\": \"string\",\n  \"report_date\": \"infer from context or use null\",\n  \"total_errors\": number,\n  \"user_sentiment\": \"string (frustrated/concerned/neutral/etc)\",\n  \"errors\": [\n    {{\n      \"error_number\": number,\n      \"error_code\": \"string or null\",\n      \"error_message\": \"string\",\n      \"source\": \"string (Windows/Application name/Hardware)\",\n      \"timestamp_context\": \"string (time mentioned in report)\",\n      \"error_type\": \"string (Blue Screen/Application Crash/Hardware Error/etc)\"\n    }}\n  ]\n}}\n\nExtract each error into a separate array item with consistent structure.\n\nUser report:\n{multiple_errors_report}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üîÑ Extracting multiple errors into JSON array...\\\\n\")\n\n# Make API call\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"medium\"}\n)\n\nerrors_json = response.output_text.strip()\nerrors_data = json.loads(errors_json)\n\nprint(\"üìã Extracted Multiple Errors:\")\nprint(\"=\" * 80)\nprint(json.dumps(errors_data, indent=2))\nprint(\"=\" * 80)\n\n# Save to file\nmultiple_errors_path = \"/content/multiple_errors.json\"\nwith open(multiple_errors_path, 'w') as f:\n    json.dump(errors_data, f, indent=2)\n\nprint(f\"\\\\nüíæ Saved to: {multiple_errors_path}\")\nprint(f\"üìä Tokens used: {response.usage.total_tokens}\")\n\n# Display insights\nprint(f\"\\\\nüîç Analysis:\")\nprint(f\"  ‚Ä¢ User: {errors_data['user_name']} ({errors_data['department']})\")\nprint(f\"  ‚Ä¢ Total errors reported: {errors_data['total_errors']}\")\nprint(f\"  ‚Ä¢ User sentiment: {errors_data['user_sentiment']}\")\nprint(f\"\\\\n  Error breakdown:\")\nfor error in errors_data['errors']:\n    print(f\"    {error['error_number']}. {error['error_type']} - {error['source']}\")\n    print(f\"       Time: {error['timestamp_context']}\")\n    if error['error_code']:\n        print(f\"       Code: {error['error_code']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9rp5ufel5pl",
   "source": "### üìä Key Teaching Point: Extracting Multiple Items\n\n**What we learned:**\n- ‚úÖ How to extract MULTIPLE similar items into a structured array\n- ‚úÖ Each array item has consistent structure (same fields)\n- ‚úÖ LLM can identify and separate distinct errors from narrative text\n- ‚úÖ We can include metadata (total count, sentiment) alongside the array\n- ‚úÖ Easy to process programmatically (loop through errors)\n\nüí° **Use case:** Any scenario with multiple similar items - error logs, equipment lists, action items from meetings, multiple user requests in one message.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "vke0bz13n5",
   "source": "## Example 3: Hardware Inventory (CSV Format)\n\nNow let's extract equipment information into CSV format - perfect for importing into spreadsheets or inventory systems.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3rrtcxing",
   "source": "# Mock data: Description of conference room equipment with various details\nconference_rooms_description = \"\"\"\nConference Room Inventory Report:\n\nExecutive Boardroom (3rd Floor):\n- Two 75-inch Samsung QN75 displays mounted on the wall (serials: SAMQN-8821, SAMQN-8822)\n- One Logitech Rally Bar camera system, serial number LOGI-RB-3345\n- Polycom RealPresence Trio 8800 conference phone, serial POLY-8800-991\n- Dell OptiPlex 7090 PC for presentations, serial DELL-OPT-4455\n\nRoom 2A (2nd Floor):\n- Single 65-inch LG OLED display, serial LG-OLED-2293  \n- Jabra PanaCast camera, serial JAB-PC-7721\n- Microsoft Surface Hub 2S 50-inch, serial MSFT-HUB-1203\n- Two wireless presentation adapters, serials WRLSS-001 and WRLSS-002\n\nTraining Room B (1st Floor):\n- Four 55-inch Sony Bravia displays (serials: SONY-BR-5501, SONY-BR-5502, SONY-BR-5503, SONY-BR-5504)\n- Cisco Webex Room Kit Pro, serial CISCO-WX-8832\n- Three HP EliteDesk 800 computers (serials: HP-ED-9901, HP-ED-9902, HP-ED-9903)\n- One portable projector - Epson PowerLite, serial EPSON-PL-4456\n\nSmall Meeting Room 105:\n- Single 43-inch Dell monitor, serial DELL-MON-3344\n- Logitech MeetUp camera, serial LOGI-MU-6655\n- One laptop docking station, serial DOCK-STN-2234\n\"\"\"\n\n# Extraction prompt for CSV\nextraction_prompt = f\"\"\"\nExtract conference room equipment inventory as CSV.\n\nUse these exact column headers:\nroom,device_type,manufacturer,model,serial_number,location_detail,quantity\n\nRules:\n- One row per device item\n- Include header row\n- device_type should be category like \"Display\", \"Camera\", \"Computer\", \"Phone System\", etc.\n- location_detail should include floor or mounting info if mentioned\n- quantity should be 1 for individual items\n- Use commas as separators\n- Don't use quotes unless value contains comma\n\nText:\n{conference_rooms_description}\n\nReturn ONLY the CSV data, no additional text.\n\"\"\"\n\nprint(\"üîÑ Extracting conference room inventory to CSV...\\\\n\")\n\n# Make API call\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\ncsv_output = response.output_text.strip()\n\nprint(\"üìä Extracted CSV Data:\")\nprint(\"=\" * 100)\nprint(csv_output)\nprint(\"=\" * 100)\n\n# Save to file\ninventory_csv_path = \"/content/conference_room_inventory.csv\"\nwith open(inventory_csv_path, 'w') as f:\n    f.write(csv_output)\n\nprint(f\"\\\\nüíæ Saved to: {inventory_csv_path}\")\nprint(f\"üìä Tokens used: {response.usage.total_tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0inouxwnks3i",
   "source": "# Verify and analyze the CSV\nprint(\"üîç Verifying CSV and analyzing inventory...\\n\")\n\n# Read with pandas\ndf = pd.read_csv(inventory_csv_path)\n\nprint(\"‚úÖ CSV is valid!\\n\")\nprint(f\"üìä Inventory Summary:\")\nprint(f\"  ‚Ä¢ Total items: {len(df)}\")\nprint(f\"  ‚Ä¢ Rooms covered: {df['room'].nunique()}\")\nprint(f\"  ‚Ä¢ Device types: {df['device_type'].nunique()}\")\nprint(f\"\\nüè¢ Items per room:\")\nprint(df.groupby('room').size().to_string())\nprint(f\"\\nüñ•Ô∏è Device type breakdown:\")\nprint(df.groupby('device_type').size().to_string())\n\nprint(f\"\\nüìã Full Inventory:\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)\n\nprint(f\"\\nüí° This CSV can be imported into Excel, Google Sheets, or an inventory database!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8t45kdhsydm",
   "source": "---\n\n# üõ°Ô∏è Data Validation\n\nExtracted data needs validation before use in production systems.\n\n---\n\n## üéØ Why Validation Matters\n\n**The reality of LLM extraction:**\n- ‚úÖ LLMs are very good but **not 100% accurate**\n- ‚úÖ Input data may be incomplete or ambiguous\n- ‚úÖ Downstream systems expect specific formats\n- ‚úÖ Early detection prevents costly errors later\n\n**Benefits of validation:**\n- üêõ **Catch issues early** - Before bad data reaches critical systems\n- üîÑ **Provide feedback** - Know what's missing or incorrect\n- üìä **Quality metrics** - Track extraction accuracy over time\n- üö® **Trigger human review** - Flag uncertain extractions\n\nLet's explore three key validation techniques:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "inwxucpcv9g",
   "source": "## Technique 1: Required Fields Check\n\nEnsure all critical fields are present and not empty.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cq16q2qaq54",
   "source": "def check_required_fields(data, required_fields):\n    \"\"\"\n    Check if all required fields are present and not empty.\n    \n    Args:\n        data (dict): Extracted data dictionary\n        required_fields (list): List of field names that must be present\n        \n    Returns:\n        dict: Validation result with is_valid flag and missing_fields list\n    \"\"\"\n    missing_fields = []\n    \n    for field in required_fields:\n        # Check if field exists\n        if field not in data:\n            missing_fields.append(field)\n        # Check if field is empty (None, empty string, empty list)\n        elif data[field] is None or data[field] == \"\" or data[field] == []:\n            missing_fields.append(field)\n    \n    return {\n        \"is_valid\": len(missing_fields) == 0,\n        \"missing_fields\": missing_fields,\n        \"message\": \"All required fields present\" if len(missing_fields) == 0 else f\"Missing fields: {', '.join(missing_fields)}\"\n    }\n\n# Example: Incomplete ticket data\nincomplete_ticket = {\n    \"user_name\": \"Alex Johnson\",\n    \"employee_id\": \"\",  # Empty!\n    \"contact_email\": \"alex.johnson@company.com\",\n    \"issue_summary\": None,  # Missing!\n    # department field completely missing\n}\n\nprint(\"üß™ Testing Required Fields Validation\\\\n\")\nprint(\"Ticket data:\")\nprint(json.dumps(incomplete_ticket, indent=2))\n\n# Define what fields are required\nrequired = [\"user_name\", \"employee_id\", \"contact_email\", \"issue_summary\", \"department\"]\n\n# Validate\nresult = check_required_fields(incomplete_ticket, required)\n\nprint(f\"\\\\nüìã Validation Result:\")\nprint(f\"  Valid: {result['is_valid']}\")\nprint(f\"  Message: {result['message']}\")\n\nif not result['is_valid']:\n    print(f\"\\\\n‚ùå Cannot process this ticket - missing critical information!\")\n    print(f\"  Action needed: Request missing fields from user\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y4vd62426j",
   "source": "## Technique 2: Data Type & Format Validation\n\nCheck if data matches expected formats (email, employee ID, asset tag, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j12ky3cxm5",
   "source": "import re\n\ndef validate_data_formats(data):\n    \"\"\"\n    Validate data formats for common IT fields.\n    \n    Args:\n        data (dict): Extracted ticket data\n        \n    Returns:\n        dict: Validation results with specific format errors\n    \"\"\"\n    errors = []\n    \n    # Email format validation\n    if 'contact_email' in data and data['contact_email']:\n        email = data['contact_email']\n        if '@' not in email or '.' not in email.split('@')[-1]:\n            errors.append(f\"Invalid email format: {email}\")\n    \n    # Employee ID format validation (EMP-####)\n    if 'employee_id' in data and data['employee_id']:\n        emp_id = data['employee_id']\n        if not re.match(r'^EMP-\\\\d{4}$', emp_id):\n            errors.append(f\"Invalid employee ID format: {emp_id} (expected: EMP-####)\")\n    \n    # Asset tag validation (reasonable length)\n    if 'asset_tag' in data and data['asset_tag']:\n        asset_tag = data['asset_tag']\n        if len(asset_tag) < 5 or len(asset_tag) > 20:\n            errors.append(f\"Asset tag length invalid: {asset_tag} (expected: 5-20 chars)\")\n    \n    # Phone number validation (at least 7 digits)\n    if 'contact_phone' in data and data['contact_phone']:\n        phone = data['contact_phone']\n        digits_only = re.sub(r'\\\\D', '', phone)  # Remove non-digits\n        if len(digits_only) < 7:\n            errors.append(f\"Phone number too short: {phone}\")\n    \n    return {\n        \"is_valid\": len(errors) == 0,\n        \"errors\": errors,\n        \"message\": \"All formats valid\" if len(errors) == 0 else f\"Found {len(errors)} format error(s)\"\n    }\n\n# Example: Ticket with format errors\nticket_with_errors = {\n    \"user_name\": \"Patricia Lee\",\n    \"employee_id\": \"12345\",  # Wrong format (missing EMP- prefix)\n    \"contact_email\": \"patricia.lee.company.com\",  # Missing @\n    \"contact_phone\": \"555\",  # Too short\n    \"asset_tag\": \"PC\",  # Too short\n    \"issue_summary\": \"Computer won't start\"\n}\n\nprint(\"üß™ Testing Format Validation\\\\n\")\nprint(\"Ticket data:\")\nprint(json.dumps(ticket_with_errors, indent=2))\n\n# Validate formats\nresult = validate_data_formats(ticket_with_errors)\n\nprint(f\"\\\\nüìã Format Validation Result:\")\nprint(f\"  Valid: {result['is_valid']}\")\nprint(f\"  Message: {result['message']}\")\n\nif not result['is_valid']:\n    print(f\"\\\\n‚ùå Format Errors Detected:\")\n    for i, error in enumerate(result['errors'], 1):\n        print(f\"  {i}. {error}\")\n    print(f\"\\\\n‚ö†Ô∏è Action needed: Data needs correction before processing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "q3aw99csksl",
   "source": "## Technique 3: Confidence Scoring\n\nAsk the LLM to rate its own confidence and identify uncertain extractions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e50jyzy7n1v",
   "source": "# Mock data: Vague, incomplete ticket\nvague_ticket = \"\"\"\nFrom: someone@company.com\nSubject: Help\n\nSomething's wrong with my computer. It's not working right.\nCan you fix it?\n\"\"\"\n\n# Extraction with confidence scoring\nextraction_prompt = f\"\"\"\nExtract support ticket information from this email. Include a confidence score.\n\nReturn JSON with these fields:\n- user_name: Extract if possible, use null if not found\n- employee_id: Extract if mentioned, use null otherwise\n- contact_email: Email address\n- issue_summary: Brief summary of the issue\n- urgency: Low/Medium/High/Critical (infer from context)\n- confidence_score: Your confidence in this extraction (0-100)\n- missing_fields: List of critical information that's missing\n- notes: Any concerns or ambiguities about the extraction\n\nEmail:\n{vague_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üîÑ Extracting vague ticket with confidence scoring...\\\\n\")\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"medium\"}\n)\n\nextraction_json = response.output_text.strip()\nextraction_data = json.loads(extraction_json)\n\nprint(\"üìã Extracted Data with Confidence:\")\nprint(\"=\" * 80)\nprint(json.dumps(extraction_data, indent=2))\nprint(\"=\" * 80)\n\n# Check confidence threshold\nCONFIDENCE_THRESHOLD = 70\n\nprint(f\"\\\\nüéØ Confidence Analysis:\")\nprint(f\"  Confidence Score: {extraction_data.get('confidence_score', 0)}/100\")\nprint(f\"  Threshold: {CONFIDENCE_THRESHOLD}/100\")\n\nif extraction_data.get('confidence_score', 0) < CONFIDENCE_THRESHOLD:\n    print(f\"\\\\n‚ö†Ô∏è LOW CONFIDENCE - Human Review Required!\")\n    print(f\"  Missing fields: {', '.join(extraction_data.get('missing_fields', []))}\")\n    print(f\"  Notes: {extraction_data.get('notes', 'N/A')}\")\n    print(f\"\\\\n  Action: Request more information from user\")\nelse:\n    print(f\"\\\\n‚úÖ High confidence - Safe to process automatically\")\n\nprint(f\"\\\\nüìä Tokens used: {response.usage.total_tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bwr4x2gg2qv",
   "source": "---\n\n# üíæ File Generation\n\nExtracted data should be saved for downstream use, record-keeping, and team sharing.\n\n---\n\n## üéØ Why Save Extracted Data?\n\n**Use cases for saved files:**\n- üìä **Import to other systems** - Databases, ticketing systems, spreadsheets\n- üìÅ **Record keeping** - Maintain audit trails and historical records\n- üîÑ **Batch processing** - Process multiple extractions together\n- üë• **Team sharing** - Share structured data with colleagues\n- üìà **Analysis** - Aggregate data for reporting and insights\n\nLet's explore different file saving patterns:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "xfrpyjdjpmc",
   "source": "## Pattern 1: Saving JSON Files with Timestamps\n\nCreate unique filenames with timestamps to avoid overwriting data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wqtentj17to",
   "source": "def save_json_with_timestamp(data, prefix=\"extracted_data\"):\n    \"\"\"\n    Save JSON data with timestamp in filename.\n    \n    Args:\n        data (dict): Data to save\n        prefix (str): Filename prefix\n        \n    Returns:\n        str: Path to saved file\n    \"\"\"\n    # Generate timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create filename\n    filename = f\"/content/{prefix}_{timestamp}.json\"\n    \n    # Save file\n    with open(filename, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    return filename\n\n# Example: Save a ticket extraction\nexample_ticket = {\n    \"ticket_id\": \"TKT-2024-0157\",\n    \"user_name\": \"Emily Chen\",\n    \"employee_id\": \"EMP-8821\",\n    \"department\": \"Finance\",\n    \"issue_summary\": \"Excel crashes when opening large files\",\n    \"urgency\": \"Medium\",\n    \"extracted_at\": datetime.now().isoformat()\n}\n\nprint(\"üíæ Saving JSON with timestamp...\\\\n\")\n\n# Save the file\nfile_path = save_json_with_timestamp(example_ticket, prefix=\"ticket_extraction\")\n\nprint(f\"‚úÖ Saved to: {file_path}\")\nprint(f\"\\\\nüìã File contents:\")\nwith open(file_path, 'r') as f:\n    print(f.read())\n\n# Demonstrate saving multiple extractions\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"Saving multiple extractions...\\\\n\")\n\nimport time\n\nfor i in range(3):\n    ticket = {\n        \"ticket_id\": f\"TKT-2024-{100 + i}\",\n        \"user_name\": f\"User {i+1}\",\n        \"issue\": f\"Issue {i+1}\"\n    }\n    path = save_json_with_timestamp(ticket, prefix=\"ticket\")\n    print(f\"  Saved: {path}\")\n    time.sleep(1)  # Small delay to ensure different timestamps\n\nprint(\"\\\\n‚úÖ Each file has unique timestamp - no overwrites!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xxcybzlwefe",
   "source": "## Pattern 2: Saving CSV Files\n\nConvert extracted data to CSV format for spreadsheet compatibility.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v31udqjlpj",
   "source": "def save_to_csv(data_list, filename, fieldnames=None):\n    \"\"\"\n    Save list of dictionaries as CSV file.\n    \n    Args:\n        data_list (list): List of dictionaries to save\n        filename (str): Output filename\n        fieldnames (list): Optional list of field names (uses first dict keys if None)\n        \n    Returns:\n        str: Path to saved file\n    \"\"\"\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n    \n    # Use provided fieldnames or extract from first dictionary\n    if fieldnames is None:\n        fieldnames = list(data_list[0].keys())\n    \n    # Ensure filename includes path\n    if not filename.startswith('/content/'):\n        filename = f\"/content/{filename}\"\n    \n    # Write CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data_list)\n    \n    return filename\n\n# Example: Save inventory data\ninventory_items = [\n    {\n        \"room\": \"Conference A\",\n        \"device_type\": \"Display\",\n        \"manufacturer\": \"Samsung\",\n        \"model\": \"QN75\",\n        \"serial_number\": \"SAM-001\",\n        \"quantity\": 1\n    },\n    {\n        \"room\": \"Conference A\",\n        \"device_type\": \"Camera\",\n        \"manufacturer\": \"Logitech\",\n        \"model\": \"Rally Bar\",\n        \"serial_number\": \"LOG-445\",\n        \"quantity\": 1\n    },\n    {\n        \"room\": \"Conference B\",\n        \"device_type\": \"Display\",\n        \"manufacturer\": \"LG\",\n        \"model\": \"OLED65\",\n        \"serial_number\": \"LG-2293\",\n        \"quantity\": 1\n    }\n]\n\nprint(\"üíæ Saving inventory data to CSV...\\n\")\n\ncsv_path = save_to_csv(\n    inventory_items, \n    \"inventory_export.csv\",\n    fieldnames=[\"room\", \"device_type\", \"manufacturer\", \"model\", \"serial_number\", \"quantity\"]\n)\n\nprint(f\"‚úÖ Saved to: {csv_path}\")\n\n# Read and display\ndf = pd.read_csv(csv_path)\nprint(f\"\\nüìä CSV Contents ({len(df)} rows):\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)\n\nprint(f\"\\nüí° This CSV can be opened in Excel or imported into inventory systems!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m6s7supgzv8",
   "source": "## Pattern 3: Appending to Existing Files\n\nBuild up a log file with multiple extractions over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qfrf6xmv4jk",
   "source": "def append_to_ticket_log(ticket_data, log_file=\"/content/ticket_log.json\"):\n    \"\"\"\n    Append ticket data to existing log file.\n    \n    Args:\n        ticket_data (dict): Ticket data to append\n        log_file (str): Path to log file\n        \n    Returns:\n        dict: Status with total count\n    \"\"\"\n    # Add timestamp to ticket\n    ticket_data['logged_at'] = datetime.now().isoformat()\n    \n    # Check if file exists\n    if os.path.exists(log_file):\n        # Load existing data\n        with open(log_file, 'r') as f:\n            log_data = json.load(f)\n    else:\n        # Create new log structure\n        log_data = {\n            \"log_created\": datetime.now().isoformat(),\n            \"tickets\": []\n        }\n    \n    # Append new ticket\n    log_data[\"tickets\"].append(ticket_data)\n    log_data[\"total_tickets\"] = len(log_data[\"tickets\"])\n    log_data[\"last_updated\"] = datetime.now().isoformat()\n    \n    # Save back to file\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f, indent=2)\n    \n    return {\n        \"success\": True,\n        \"total_tickets\": log_data[\"total_tickets\"],\n        \"log_file\": log_file\n    }\n\nprint(\"üìù Demonstrating ticket log appending...\\\\n\")\n\n# Simulate processing multiple tickets\ntickets_to_process = [\n    {\n        \"ticket_id\": \"TKT-001\",\n        \"user_name\": \"Alice Smith\",\n        \"issue\": \"Password reset needed\",\n        \"urgency\": \"Low\"\n    },\n    {\n        \"ticket_id\": \"TKT-002\", \n        \"user_name\": \"Bob Johnson\",\n        \"issue\": \"VPN not connecting\",\n        \"urgency\": \"High\"\n    },\n    {\n        \"ticket_id\": \"TKT-003\",\n        \"user_name\": \"Carol Davis\",\n        \"issue\": \"Printer offline\",\n        \"urgency\": \"Medium\"\n    }\n]\n\n# Process and log each ticket\nfor ticket in tickets_to_process:\n    result = append_to_ticket_log(ticket)\n    print(f\"‚úÖ Logged {ticket['ticket_id']} - Total tickets in log: {result['total_tickets']}\")\n    time.sleep(0.5)  # Small delay\n\n# Read and display final log\nprint(f\"\\\\nüìã Final Ticket Log:\")\nprint(\"=\" * 80)\nwith open(\"/content/ticket_log.json\", 'r') as f:\n    log_contents = json.load(f)\n\nprint(f\"Log created: {log_contents['log_created']}\")\nprint(f\"Last updated: {log_contents['last_updated']}\")\nprint(f\"Total tickets: {log_contents['total_tickets']}\\\\n\")\n\nprint(\"Tickets in log:\")\nfor i, ticket in enumerate(log_contents['tickets'], 1):\n    print(f\"  {i}. {ticket['ticket_id']}: {ticket['issue']} (Urgency: {ticket['urgency']})\")\n\nprint(\"=\" * 80)\nprint(f\"\\\\nüí° Log file grows with each append - perfect for ongoing ticket tracking!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ly5jwhlxesg",
   "source": "---\n\n# ‚úÖ ITERATION 2 COMPLETE\n\n## Sections Added:\n5. ‚úÖ Practical Examples (Continued)\n   - Example 2: Multiple Error Extraction (JSON Arrays)\n   - Example 3: Hardware Inventory (CSV Format)\n6. ‚úÖ Data Validation\n   - Technique 1: Required Fields Check\n   - Technique 2: Data Type & Format Validation\n   - Technique 3: Confidence Scoring\n7. ‚úÖ File Generation\n   - Pattern 1: JSON Files with Timestamps\n   - Pattern 2: CSV File Export\n   - Pattern 3: Appending to Log Files\n\n## Additional Files Saved:\n- `/content/multiple_errors.json` - Multiple error extraction from single report\n- `/content/conference_room_inventory.csv` - Conference room equipment inventory\n- `/content/ticket_extraction_YYYYMMDD_HHMMSS.json` - Timestamped ticket files\n- `/content/inventory_export.csv` - Exported inventory data\n- `/content/ticket_log.json` - Appended ticket log\n\n## What You've Learned:\n‚úÖ Extract multiple items into structured arrays  \n‚úÖ Generate CSV files for spreadsheet import  \n‚úÖ Validate extracted data (required fields, formats, confidence)  \n‚úÖ Save data with timestamps for uniqueness  \n‚úÖ Append to log files for ongoing tracking  \n‚úÖ Handle low-confidence extractions appropriately  \n\n---\n\n## üéØ Ready for Iteration 3\n\n**Please review and approve before I continue.**\n\nIteration 3 will add:\n- Batch Processing with progress tracking\n- Error Handling & Edge Cases (5 scenarios)\n- Mini-Project: Complete Support Ticket Intake System\n- Best Practices & Key Takeaways\n- Student Exercises\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "w0gvtvdyp3c",
   "source": "# üîÑ Batch Processing\n\nOften you need to process multiple items at once - backlogs, audits, migrations. Let's build a batch processor with progress tracking.\n\n---\n\n## üéØ Why Batch Processing?\n\n**Use cases:**\n- üì¨ **Process email backlog** - Extract data from hundreds of support emails\n- üìä **Audit existing tickets** - Re-extract data with improved prompts\n- üîÑ **Data migration** - Convert old formats to new structured data\n- üìà **Bulk analysis** - Extract insights from large document sets\n\n**Benefits:**\n- ‚ö° More efficient than one-by-one processing\n- üìä Progress tracking with visual feedback\n- üêõ Error tracking - separate successes from failures\n- üìÅ Organized output with summary reports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ile4p2bmnpi",
   "source": "# Create array of 5 different mock support tickets\nticket_batch = [\n    \"\"\"From: anna.rodriguez@company.com\n    Subject: Laptop Battery Draining Fast\n    Hi, I'm Anna Rodriguez (EMP-3401) from Marketing. My laptop battery only lasts 30 minutes now.\n    This is urgent as I have client meetings all day tomorrow. Please help!\"\"\",\n    \n    \"\"\"From: kevin.park@company.com\n    Subject: Cannot Connect to WiFi\n    Kevin Park here, Sales dept, EMP-5623. My laptop won't connect to the office WiFi.\n    I've tried restarting but it still doesn't work. Medium priority.\"\"\",\n    \n    \"\"\"From: lisa.chen@company.com  \n    Subject: Printer Not Working\n    Lisa Chen, Finance, EMP-7834. The printer on floor 3 shows 'Paper Jam' but there's no paper stuck.\n    Multiple people are affected. Needs fixing ASAP!\"\"\",\n    \n    \"\"\"From: marcus.johnson@company.com\n    Subject: Slow Computer Performance\n    Marcus Johnson, IT Support team member EMP-2019. My workstation is running extremely slow.\n    All applications take forever to load. Low priority but annoying.\"\"\",\n    \n    \"\"\"From: sophia.williams@company.com\n    Subject: Email Account Locked\n    Sophia Williams, HR department, EMP-9102. I got locked out after entering wrong password.\n    Need access urgently for payroll processing today. CRITICAL!\"\"\"\n]\n\ndef batch_extract_tickets(ticket_list):\n    \"\"\"\n    Process multiple tickets in batch with progress tracking.\n    \n    Args:\n        ticket_list (list): List of ticket text strings\n        \n    Returns:\n        dict: Results with successes, failures, and summary\n    \"\"\"\n    results = []\n    failures = []\n    \n    # Process with progress bar\n    for i, ticket_text in enumerate(tqdm(ticket_list, desc=\"Processing tickets\")):\n        try:\n            # Extraction prompt\n            extraction_prompt = f\"\"\"\n            Extract ticket information from this email. Return ONLY valid JSON.\n            \n            {{\n              \"ticket_id\": \"TKT-{datetime.now().strftime('%Y')}-{str(i+1).zfill(4)}\",\n              \"user_name\": \"full name\",\n              \"employee_id\": \"employee ID\",\n              \"department\": \"department name\",\n              \"contact_email\": \"email address\",\n              \"issue_summary\": \"brief summary\",\n              \"urgency\": \"Low/Medium/High/Critical\"\n            }}\n            \n            Email:\n            {ticket_text}\n            \"\"\"\n            \n            # Make API call\n            response = client.responses.create(\n                model=OPENAI_MODEL,\n                input=extraction_prompt,\n                text={\"verbosity\": \"low\"}\n            )\n            \n            # Parse JSON\n            extracted_data = json.loads(response.output_text.strip())\n            \n            # Add metadata\n            extracted_data['processed_at'] = datetime.now().isoformat()\n            extracted_data['batch_index'] = i + 1\n            \n            results.append(extracted_data)\n            \n        except Exception as e:\n            failures.append({\n                \"batch_index\": i + 1,\n                \"error\": str(e),\n                \"ticket_preview\": ticket_text[:100] + \"...\"\n            })\n    \n    return {\n        \"total_processed\": len(ticket_list),\n        \"successful\": len(results),\n        \"failed\": len(failures),\n        \"success_rate\": f\"{(len(results)/len(ticket_list)*100):.1f}%\",\n        \"results\": results,\n        \"failures\": failures\n    }\n\nprint(\"üîÑ Starting batch processing of 5 tickets...\\\\n\")\n\n# Process the batch\nbatch_results = batch_extract_tickets(ticket_batch)\n\nprint(f\"\\\\n‚úÖ Batch Processing Complete!\\\\n\")\nprint(f\"üìä Summary:\")\nprint(f\"  ‚Ä¢ Total tickets: {batch_results['total_processed']}\")\nprint(f\"  ‚Ä¢ Successful: {batch_results['successful']}\")\nprint(f\"  ‚Ä¢ Failed: {batch_results['failed']}\")\nprint(f\"  ‚Ä¢ Success rate: {batch_results['success_rate']}\")\n\n# Display successful extractions\nif batch_results['results']:\n    print(f\"\\\\nüìã Successfully Extracted Tickets:\")\n    for ticket in batch_results['results']:\n        print(f\"  {ticket['ticket_id']}: {ticket['user_name']} - {ticket['issue_summary']} ({ticket['urgency']})\")\n\n# Display failures if any\nif batch_results['failures']:\n    print(f\"\\\\n‚ùå Failed Extractions:\")\n    for failure in batch_results['failures']:\n        print(f\"  Ticket {failure['batch_index']}: {failure['error']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}