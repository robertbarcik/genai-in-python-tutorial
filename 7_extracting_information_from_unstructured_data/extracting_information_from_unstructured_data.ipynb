{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# üìä Extracting Structured Information from Unstructured Text\n",
    "\n",
    "Welcome! This comprehensive tutorial will teach you how to extract structured, actionable data from unstructured text using OpenAI's API.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Extract structured data** from messy, unstructured text (emails, tickets, reports)\n",
    "2. **Choose the right format** - CSV, JSON, or Pydantic models for your use case\n",
    "3. **Parse complex information** - Handle nested data, arrays, and multiple items\n",
    "4. **Validate extractions** - Ensure data quality and catch errors early\n",
    "5. **Save results** - Store extracted data in files for downstream use\n",
    "6. **Handle edge cases** - Deal with missing info, contradictions, and ambiguity\n",
    "7. **Build production systems** - Create robust, scalable extraction pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-theory",
   "metadata": {},
   "source": [
    "## üéØ Why Extract Structured Data?\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "In IT support and services, information flows in **unstructured formats**:\n",
    "\n",
    "- üìß **Emails**: \"Hi, my laptop won't start. I think it's the battery. Can someone help? - John from Marketing\"\n",
    "- üí¨ **Chat messages**: \"printer broken room 304 need toner asap\"\n",
    "- üìû **Verbal reports**: \"Sarah mentioned something about the network being slow in Building B\"\n",
    "- üìù **Handwritten notes**: Notes from a phone call or site visit\n",
    "\n",
    "But to **take action**, this information needs to be **structured**:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_name\": \"John\",\n",
    "  \"department\": \"Marketing\",\n",
    "  \"issue\": \"Laptop won't start\",\n",
    "  \"suspected_cause\": \"Battery\",\n",
    "  \"urgency\": \"medium\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Current Challenges\n",
    "\n",
    "‚ùå **Manual data entry is:**\n",
    "- **Slow** - Takes time away from actual problem-solving\n",
    "- **Error-prone** - Typos, missed fields, inconsistent formatting\n",
    "- **Doesn't scale** - Can't handle high ticket volumes\n",
    "- **Inconsistent** - Different people extract different information\n",
    "\n",
    "### The Solution: LLM-Powered Extraction\n",
    "\n",
    "‚úÖ **Large Language Models can:**\n",
    "- Extract key information accurately and consistently\n",
    "- Handle natural language variations (\"urgent\", \"asap\", \"critical\")\n",
    "- Infer missing information from context\n",
    "- Structure data in your required format (JSON, CSV, database schema)\n",
    "- Process hundreds of items in minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-concepts",
   "metadata": {},
   "source": [
    "## üîë Key Concepts\n",
    "\n",
    "### Structured vs. Unstructured Data\n",
    "\n",
    "**Unstructured Data:**\n",
    "- Free-form text, no fixed format\n",
    "- Examples: Emails, chat messages, documents, verbal reports\n",
    "- Hard for computers to process directly\n",
    "\n",
    "**Structured Data:**\n",
    "- Organized in a predefined format\n",
    "- Examples: Database tables, JSON objects, CSV files\n",
    "- Easy to query, analyze, and integrate with other systems\n",
    "\n",
    "### Data Extraction vs. Data Parsing\n",
    "\n",
    "**Data Extraction:**\n",
    "- Identifying and pulling out specific information from unstructured text\n",
    "- Requires understanding context and meaning\n",
    "- Example: Finding user name, issue type, urgency from an email\n",
    "\n",
    "**Data Parsing:**\n",
    "- Converting extracted information into a structured format\n",
    "- Example: Creating a JSON object with extracted fields\n",
    "\n",
    "LLMs excel at **both** - they understand context AND can output structured formats.\n",
    "\n",
    "### Token Costs for Extraction\n",
    "\n",
    "**Good news:** Extraction tasks work well with cheaper models!\n",
    "\n",
    "- **gpt-5-nano**: $0.05/1M input tokens, $0.40/1M output tokens\n",
    "- A typical support ticket extraction:\n",
    "  - Input: ~300 tokens (the email/ticket)\n",
    "  - Output: ~100 tokens (structured JSON)\n",
    "  - Cost: ~$0.00005 (less than 1/10th of a cent per ticket!)\n",
    "\n",
    "üí° **Key takeaway:** You can process thousands of tickets for just a few dollars.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-title",
   "metadata": {},
   "source": [
    "# üîß Setup\n",
    "\n",
    "Let's configure the environment and install required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-deps",
   "metadata": {},
   "source": "## üì¶ Install Dependencies\n\nWe'll install five libraries:\n- **openai**: Official OpenAI Python client for API access\n- **pydantic**: Data validation and settings management using Python type hints\n- **email-validator**: Email validation for Pydantic's EmailStr type\n- **tqdm**: Progress bars for batch processing\n- **pandas**: Data manipulation and CSV handling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q openai pydantic email-validator tqdm pandas"
  },
  {
   "cell_type": "markdown",
   "id": "setup-apikey",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-apikey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-imports",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport json\nimport csv\nfrom datetime import datetime\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nfrom pydantic import BaseModel, EmailStr, Field, field_validator\nimport pandas as pd\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"‚úÖ OpenAI client initialized!\")"
  },
  {
   "cell_type": "markdown",
   "id": "formats-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìã Understanding Output Formats\n",
    "\n",
    "LLMs can extract data into different formats. Each format has specific use cases in IT support.\n",
    "\n",
    "We'll explore three main formats:\n",
    "1. **CSV** - Simple tabular data\n",
    "2. **JSON** - Complex, nested structures\n",
    "3. **Pydantic Models** - Type-safe, validated data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-csv-title",
   "metadata": {},
   "source": [
    "## Format A: CSV (Comma-Separated Values)\n",
    "\n",
    "### üìñ When to Use CSV\n",
    "\n",
    "**Best for:** Simple tabular data with similar items\n",
    "\n",
    "**Use cases in IT:**\n",
    "- üì¶ Asset inventories and equipment lists\n",
    "- üñ•Ô∏è Hardware tracking spreadsheets\n",
    "- üìä Simple databases that need Excel compatibility\n",
    "- üìà Reports for non-technical stakeholders\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Easy to open in Excel or Google Sheets\n",
    "- ‚úÖ Simple structure, widely supported\n",
    "- ‚úÖ Human-readable and editable\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå No nesting (can't represent complex relationships)\n",
    "- ‚ùå All values are strings (no native data types)\n",
    "- ‚ùå Harder to represent one-to-many relationships\n",
    "\n",
    "### üíª Practical Example: Conference Room Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data: Unstructured description of conference room equipment\n",
    "inventory_description = \"\"\"\n",
    "Conference Room A has a Samsung 65-inch display (serial: SAMS-2024-001), \n",
    "a Logitech Rally camera (serial: LOG-CAM-445), and a Poly Studio phone system (serial: POLY-899-X).\n",
    "\n",
    "Conference Room B contains two Dell OptiPlex 7090 computers (serials: DELL-PC-1023 and DELL-PC-1024),\n",
    "an LG 55-inch display (serial: LG-DSP-3301), and a Jabra Speak 750 speakerphone (serial: JAB-750-229).\n",
    "\n",
    "Conference Room C is equipped with a Microsoft Surface Hub 2S (serial: MSFT-HUB-8821),\n",
    "a Cisco Webex Room Kit (serial: CISCO-WX-4492), and an HP laptop (serial: HP-LT-9933).\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt for CSV format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract equipment inventory information from the text below and format it as CSV.\n",
    "\n",
    "Use these exact column headers: room,device_type,manufacturer,model,serial_number\n",
    "\n",
    "Rules:\n",
    "- One row per device\n",
    "- Include header row\n",
    "- Use commas as separators\n",
    "- No quotes around values unless they contain commas\n",
    "\n",
    "Text:\n",
    "{inventory_description}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting inventory data to CSV format...\\n\")\n",
    "\n",
    "# Make API call using Responses API\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}  # Low verbosity for structured output\n",
    ")\n",
    "\n",
    "csv_output = response.output_text.strip()\n",
    "\n",
    "print(\"üìä Extracted CSV Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(csv_output)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "csv_file_path = \"/content/inventory_data.csv\"\n",
    "with open(csv_file_path, 'w') as f:\n",
    "    f.write(csv_output)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {csv_file_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csv-verify",
   "metadata": {},
   "outputs": [],
   "source": "# Verify the CSV is valid by parsing it back\nprint(\"üîç Verifying CSV format...\\n\")\n\n# Method 1: Using pandas\ndf = pd.read_csv(csv_file_path)\nprint(\"‚úÖ CSV is valid and readable!\\n\")\nprint(f\"üìä Found {len(df)} devices across {df['room'].nunique()} rooms\\n\")\nprint(\"Preview:\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)"
  },
  {
   "cell_type": "markdown",
   "id": "format-json-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Format B: JSON (JavaScript Object Notation)\n",
    "\n",
    "### üìñ When to Use JSON\n",
    "\n",
    "**Best for:** Complex, nested data structures\n",
    "\n",
    "**Use cases in IT:**\n",
    "- üé´ Support tickets with multiple fields and categories\n",
    "- üë§ User information with device specifications\n",
    "- üêõ Error reports with nested details\n",
    "- üîó API integration and data exchange\n",
    "- üìÅ Configuration files and settings\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Flexible structure, supports nesting\n",
    "- ‚úÖ Native data types (strings, numbers, booleans, arrays, objects)\n",
    "- ‚úÖ Standard format for APIs and modern applications\n",
    "- ‚úÖ Easy to parse in any programming language\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå More verbose than CSV\n",
    "- ‚ùå Less human-friendly for simple tables\n",
    "\n",
    "üí° **Key Point:** JSON is the preferred format for most extraction tasks due to its flexibility.\n",
    "\n",
    "### üíª Practical Example: Support Ticket Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data: Unstructured support ticket email\n",
    "support_ticket = \"\"\"\n",
    "From: jennifer.martinez@company.com\n",
    "Subject: Urgent - Cannot Access Email\n",
    "\n",
    "Hi IT Support,\n",
    "\n",
    "I'm Jennifer Martinez from the Sales department (employee ID: EMP-5834). \n",
    "My Outlook keeps crashing whenever I try to open it. I've tried restarting \n",
    "my computer twice but the problem persists.\n",
    "\n",
    "This is really urgent because I need to send quotes to clients today. \n",
    "My phone number is 555-0192 if you need to call me.\n",
    "\n",
    "Please help ASAP!\n",
    "\n",
    "Thanks,\n",
    "Jennifer\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt for JSON format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information from the email below and format it as JSON.\n",
    "\n",
    "Include these fields:\n",
    "- user_name: Full name of the user\n",
    "- department: User's department\n",
    "- employee_id: Employee ID if mentioned\n",
    "- contact_email: Email address\n",
    "- contact_phone: Phone number if mentioned\n",
    "- issue_summary: Brief summary of the issue\n",
    "- application: The application having issues\n",
    "- urgency: Low, Medium, High, or Critical (infer from context)\n",
    "- actions_tried: List of troubleshooting steps user already attempted\n",
    "\n",
    "Email:\n",
    "{support_ticket}\n",
    "\n",
    "Respond with ONLY valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting ticket data to JSON format...\\n\")\n",
    "\n",
    "# Make API call\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "json_output = response.output_text.strip()\n",
    "\n",
    "# Parse and pretty-print the JSON\n",
    "ticket_data = json.loads(json_output)\n",
    "\n",
    "print(\"üìã Extracted JSON Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(ticket_data, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "json_file_path = \"/content/ticket_data.json\"\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(ticket_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {json_file_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and demonstrate JSON parsing\n",
    "print(\"üîç Verifying JSON format and demonstrating access...\\n\")\n",
    "\n",
    "# Read back from file\n",
    "with open(json_file_path, 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "print(\"‚úÖ JSON is valid!\\n\")\n",
    "print(\"Accessing specific fields:\")\n",
    "print(f\"  User: {loaded_data['user_name']}\")\n",
    "print(f\"  Department: {loaded_data['department']}\")\n",
    "print(f\"  Issue: {loaded_data['issue_summary']}\")\n",
    "print(f\"  Urgency: {loaded_data['urgency']}\")\n",
    "print(f\"  Actions tried: {', '.join(loaded_data['actions_tried'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format-pydantic-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Format C: Pydantic Models (Type-Safe Validation)\n",
    "\n",
    "### üìñ When to Use Pydantic\n",
    "\n",
    "**What it is:** Python library for data validation using type hints\n",
    "\n",
    "**Key benefit:** Automatic validation against a defined schema\n",
    "\n",
    "**When useful:**\n",
    "- üè≠ **Production systems** feeding databases\n",
    "- üîó **API integration** requiring specific formats\n",
    "- üêõ **Early error detection** before data reaches critical systems\n",
    "- üë• **Team collaboration** with clear data contracts\n",
    "- üìä **Type safety** ensuring fields are correct types\n",
    "\n",
    "### üîç Comparison: JSON vs. Pydantic\n",
    "\n",
    "Let's see the difference between plain JSON (no validation) and Pydantic (validated):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-problem",
   "metadata": {},
   "source": [
    "#### ‚ùå Problem: Plain JSON Without Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json-no-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plain JSON - problems discovered later\n",
    "bad_ticket = {\n",
    "    \"user_name\": \"John\",  # Missing last name\n",
    "    \"employee_id\": \"12345\",  # Wrong format (should be EMP-####)\n",
    "    \"contact_email\": \"john.company.com\",  # Invalid email (missing @)\n",
    "    \"urgency\": \"super urgent\",  # Invalid value (should be Low/Medium/High/Critical)\n",
    "    # Missing required field: issue_summary\n",
    "}\n",
    "\n",
    "print(\"‚ùå Plain JSON - No validation happens:\")\n",
    "print(json.dumps(bad_ticket, indent=2))\n",
    "print(\"\\n‚ö†Ô∏è This bad data could be saved to database, causing errors later!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-solution",
   "metadata": {},
   "source": [
    "#### ‚úÖ Solution: Pydantic Model with Validation\n",
    "\n",
    "Let's create a Pydantic model that validates our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define urgency levels as an Enum\n",
    "class UrgencyLevel(str, Enum):\n",
    "    LOW = \"Low\"\n",
    "    MEDIUM = \"Medium\"\n",
    "    HIGH = \"High\"\n",
    "    CRITICAL = \"Critical\"\n",
    "\n",
    "# Define the Pydantic model for a support ticket\n",
    "class SupportTicket(BaseModel):\n",
    "    # Required fields\n",
    "    user_name: str = Field(..., min_length=2, description=\"Full name of the user\")\n",
    "    employee_id: str = Field(..., pattern=r'^EMP-\\d{4}$', description=\"Employee ID in format EMP-####\")\n",
    "    contact_email: EmailStr = Field(..., description=\"Valid email address\")\n",
    "    issue_summary: str = Field(..., min_length=10, description=\"Brief summary of the issue\")\n",
    "    urgency: UrgencyLevel = Field(..., description=\"Urgency level\")\n",
    "    \n",
    "    # Optional fields with defaults\n",
    "    department: Optional[str] = None\n",
    "    contact_phone: Optional[str] = None\n",
    "    application: Optional[str] = None\n",
    "    actions_tried: List[str] = Field(default_factory=list)\n",
    "    \n",
    "    # Custom validator for phone numbers\n",
    "    @field_validator('contact_phone')\n",
    "    @classmethod\n",
    "    def validate_phone(cls, v):\n",
    "        if v and len(v.replace('-', '').replace(' ', '')) < 7:\n",
    "            raise ValueError('Phone number too short')\n",
    "        return v\n",
    "\n",
    "print(\"‚úÖ Pydantic model defined with validation rules!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-demo-bad",
   "metadata": {},
   "source": [
    "#### üß™ Test 1: Invalid Data (Validation Catches Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-bad-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create ticket with bad data\n",
    "print(\"üß™ Testing with INVALID data...\\n\")\n",
    "\n",
    "try:\n",
    "    bad_ticket_validated = SupportTicket(\n",
    "        user_name=\"J\",  # Too short\n",
    "        employee_id=\"12345\",  # Wrong format\n",
    "        contact_email=\"invalid-email\",  # Invalid email\n",
    "        issue_summary=\"Broken\",  # Too short\n",
    "        urgency=\"super urgent\"  # Invalid urgency level\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Validation failed (as expected):\")\n",
    "    print(f\"\\nError type: {type(e).__name__}\")\n",
    "    print(f\"\\nErrors found:\")\n",
    "    \n",
    "    # Parse validation errors\n",
    "    if hasattr(e, 'errors'):\n",
    "        for error in e.errors():\n",
    "            field = error['loc'][0]\n",
    "            message = error['msg']\n",
    "            print(f\"  ‚Ä¢ {field}: {message}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Validation prevented bad data from being processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-demo-good",
   "metadata": {},
   "source": [
    "#### üß™ Test 2: Valid Data (Validation Succeeds)\n",
    "\n",
    "Now let's use LLM to extract data in Pydantic-compatible format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pydantic-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data for extraction\n",
    "ticket_email = \"\"\"\n",
    "From: robert.chen@company.com\n",
    "Subject: VPN Connection Issues - Need Help\n",
    "\n",
    "Hello IT,\n",
    "\n",
    "This is Robert Chen from Engineering (EMP-7821). My VPN client keeps \n",
    "disconnecting every 5-10 minutes. I've already tried:\n",
    "- Restarting the VPN client\n",
    "- Rebooting my laptop\n",
    "- Checking my internet connection\n",
    "\n",
    "This is blocking my work as I need to access the development servers.\n",
    "Please treat this as high priority.\n",
    "\n",
    "You can reach me at 555-0198.\n",
    "\n",
    "Thanks,\n",
    "Robert\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt that ensures Pydantic-compatible format\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information and format as JSON matching this schema:\n",
    "\n",
    "Required fields:\n",
    "- user_name (string, min 2 chars): Full name\n",
    "- employee_id (string, format: EMP-####): Employee ID  \n",
    "- contact_email (string): Valid email address\n",
    "- issue_summary (string, min 10 chars): Brief issue description\n",
    "- urgency (string): Must be exactly one of: \"Low\", \"Medium\", \"High\", \"Critical\"\n",
    "\n",
    "Optional fields:\n",
    "- department (string or null): Department name\n",
    "- contact_phone (string or null): Phone number\n",
    "- application (string or null): Application name\n",
    "- actions_tried (array of strings): Steps user already tried\n",
    "\n",
    "Email:\n",
    "{ticket_email}\n",
    "\n",
    "Return ONLY valid JSON, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting ticket with Pydantic validation...\\n\")\n",
    "\n",
    "# Extract data\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "\n",
    "extracted_json = response.output_text.strip()\n",
    "extracted_data = json.loads(extracted_json)\n",
    "\n",
    "print(\"üìã Extracted JSON:\")\n",
    "print(json.dumps(extracted_data, indent=2))\n",
    "print()\n",
    "\n",
    "# Validate with Pydantic\n",
    "try:\n",
    "    validated_ticket = SupportTicket(**extracted_data)\n",
    "    print(\"‚úÖ Pydantic validation PASSED!\\n\")\n",
    "    \n",
    "    print(\"Validated ticket details:\")\n",
    "    print(f\"  User: {validated_ticket.user_name}\")\n",
    "    print(f\"  Employee ID: {validated_ticket.employee_id}\")\n",
    "    print(f\"  Email: {validated_ticket.contact_email}\")\n",
    "    print(f\"  Urgency: {validated_ticket.urgency.value}\")\n",
    "    print(f\"  Issue: {validated_ticket.issue_summary}\")\n",
    "    print(f\"  Actions tried: {len(validated_ticket.actions_tried)} steps\")\n",
    "    \n",
    "    # Save validated ticket\n",
    "    validated_file_path = \"/content/validated_ticket.json\"\n",
    "    with open(validated_file_path, 'w') as f:\n",
    "        json.dump(validated_ticket.model_dump(), f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Saved validated ticket to: {validated_file_path}\")\n",
    "    print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-summary",
   "metadata": {},
   "source": [
    "### üìä Pydantic Benefits Summary\n",
    "\n",
    "**Pydantic provides:**\n",
    "\n",
    "‚úÖ **Type safety** - Fields must be correct types  \n",
    "‚úÖ **Format validation** - Email, patterns, length checks  \n",
    "‚úÖ **Required field checking** - No missing critical data  \n",
    "‚úÖ **Enum validation** - Only allowed values accepted  \n",
    "‚úÖ **Custom validators** - Business logic validation  \n",
    "‚úÖ **Clear error messages** - Know exactly what's wrong  \n",
    "‚úÖ **IDE support** - Auto-completion and type hints  \n",
    "\n",
    "üí° **When to use:** Production systems, database integration, team projects requiring data contracts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "examples-title",
   "metadata": {},
   "source": [
    "# üíº Practical Examples\n",
    "\n",
    "Now let's apply what we've learned to realistic IT support scenarios.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-title",
   "metadata": {},
   "source": [
    "## Example 1: Support Ticket Parsing (Comprehensive)\n",
    "\n",
    "We'll extract information from support tickets, starting with basic extraction and progressing to complex nested structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1a-title",
   "metadata": {},
   "source": [
    "### Part A: Basic Ticket Extraction\n",
    "\n",
    "Extract standard ticket fields from a user email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic mock data: User email about laptop issue\n",
    "laptop_issue_email = \"\"\"\n",
    "From: sarah.williams@company.com\n",
    "Date: 2025-01-15 09:23 AM\n",
    "Subject: Laptop Battery Problem - Urgent\n",
    "\n",
    "Hi Support Team,\n",
    "\n",
    "I'm Sarah Williams from the Marketing department, employee number EMP-4567.\n",
    "My work laptop (Dell Latitude 5420) isn't holding a charge anymore. The battery \n",
    "drains completely within an hour even when I'm just using Word and email.\n",
    "\n",
    "I have a client presentation tomorrow afternoon at 2 PM and really need this \n",
    "working by then. I've tried using a different power outlet and the charger \n",
    "seems to work fine (the charging light comes on).\n",
    "\n",
    "Could someone please help? You can reach me at ext. 4523 or my cell 555-0167.\n",
    "\n",
    "Thank you!\n",
    "Sarah Williams\n",
    "Marketing Department\n",
    "\"\"\"\n",
    "\n",
    "# Extraction prompt\n",
    "extraction_prompt = f\"\"\"\n",
    "Extract support ticket information from this email and return as JSON.\n",
    "\n",
    "Include these fields:\n",
    "- user_name: Full name\n",
    "- department: Department name\n",
    "- employee_id: Employee ID\n",
    "- contact_email: Email address\n",
    "- contact_phone: Phone number (if mentioned)\n",
    "- device_info: Device description (manufacturer and model)\n",
    "- issue_summary: Concise summary of the issue\n",
    "- issue_details: Detailed description\n",
    "- urgency: Low, Medium, High, or Critical (infer from context and deadline)\n",
    "- deadline_context: Any time-sensitive information\n",
    "- troubleshooting_done: What user already tried\n",
    "\n",
    "Email:\n",
    "{laptop_issue_email}\n",
    "\n",
    "Return ONLY valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Extracting basic ticket information...\\n\")\n",
    "\n",
    "# Make API call\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=extraction_prompt,\n",
    "    text={\"verbosity\": \"medium\"}\n",
    ")\n",
    "\n",
    "ticket_json = response.output_text.strip()\n",
    "ticket_data = json.loads(ticket_json)\n",
    "\n",
    "print(\"üìã Extracted Ticket Data:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(ticket_data, indent=2))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save to file\n",
    "basic_ticket_path = \"/content/basic_ticket.json\"\n",
    "with open(basic_ticket_path, 'w') as f:\n",
    "    json.dump(ticket_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved to: {basic_ticket_path}\")\n",
    "print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "\n",
    "# Display key insights\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(f\"  ‚Ä¢ User: {ticket_data['user_name']} ({ticket_data['department']})\")\n",
    "print(f\"  ‚Ä¢ Issue: {ticket_data['issue_summary']}\")\n",
    "print(f\"  ‚Ä¢ Urgency: {ticket_data['urgency']}\")\n",
    "print(f\"  ‚Ä¢ Deadline: {ticket_data['deadline_context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1b-title",
   "metadata": {},
   "source": [
    "### Part B: Advanced - Nested Device Specifications\n",
    "\n",
    "Now let's extract more complex data with nested objects and arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1b",
   "metadata": {},
   "outputs": [],
   "source": "# Complex mock data: Workstation issue with detailed system specs\nworkstation_issue = \"\"\"\nFrom: michael.thompson@company.com\nSubject: Workstation Performance Issues - Graphics Freezing\n\nHello IT,\n\nI'm Michael Thompson, CAD Engineer in the Design department (EMP-9012).\n\nMy workstation is experiencing severe performance problems. The system specs are:\n- Dell Precision 7920 Tower (Service Tag: 5XYZ789)\n- Intel Xeon Gold 6248R processor, 24 cores, running at 3.0 GHz\n- 128GB DDR4 RAM, ECC memory\n- NVIDIA Quadro RTX 5000 with 16GB GDDR6\n- Two storage drives: 1TB NVMe SSD (OS drive) and 4TB SATA SSD (data drive)\n- Running Windows 11 Pro for Workstations, version 23H2\n\nThe screen freezes when I'm rendering 3D models in SolidWorks. Sometimes the \nentire application crashes. I suspect it might be the graphics card overheating \nbecause I hear the fans going crazy.\n\nThis is blocking a project deadline on Friday. Please help!\n\nPhone: 555-0184\nEmail: michael.thompson@company.com\n\nThanks,\nMichael\n\"\"\"\n\n# Extraction prompt for nested structure - simplified and clearer\nextraction_prompt = f\"\"\"\nExtract support ticket information with nested device specifications from the email below.\n\nIMPORTANT: Return ONLY valid JSON with NO extra text before or after. Use this exact structure:\n\n{{\n  \"user_name\": \"full name\",\n  \"department\": \"department name\",\n  \"employee_id\": \"employee ID\",\n  \"contact_email\": \"email address\",\n  \"contact_phone\": \"phone number\",\n  \"issue_summary\": \"brief issue summary\",\n  \"suspected_cause\": \"suspected cause if mentioned\",\n  \"urgency\": \"Low or Medium or High or Critical\",\n  \"affected_application\": \"application name\",\n  \"device\": {{\n    \"manufacturer\": \"device manufacturer\",\n    \"model\": \"device model\",\n    \"service_tag\": \"service tag\",\n    \"processor\": {{\n      \"brand\": \"processor brand\",\n      \"model\": \"processor model\",\n      \"cores\": 24,\n      \"speed_ghz\": 3.0\n    }},\n    \"ram\": {{\n      \"capacity_gb\": 128,\n      \"type\": \"DDR4 ECC\"\n    }},\n    \"gpu\": {{\n      \"manufacturer\": \"GPU manufacturer\",\n      \"model\": \"GPU model\",\n      \"memory_gb\": 16\n    }},\n    \"storage\": [\n      {{\n        \"capacity_tb\": 1.0,\n        \"type\": \"NVMe SSD\",\n        \"purpose\": \"OS drive\"\n      }},\n      {{\n        \"capacity_tb\": 4.0,\n        \"type\": \"SATA SSD\",\n        \"purpose\": \"data drive\"\n      }}\n    ],\n    \"operating_system\": {{\n      \"name\": \"Windows 11 Pro for Workstations\",\n      \"version\": \"23H2\"\n    }}\n  }}\n}}\n\nEmail to extract from:\n{workstation_issue}\n\nReturn ONLY the JSON object with no additional text.\n\"\"\"\n\nprint(\"üîÑ Extracting complex nested ticket information...\\\\n\")\n\n# Make API call with lower verbosity for cleaner JSON output\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}  # Low verbosity for cleaner structured output\n)\n\ncomplex_json = response.output_text.strip()\n\n# Try to parse JSON with error handling\ntry:\n    complex_data = json.loads(complex_json)\n    \n    print(\"üìã Extracted Complex Nested Data:\")\n    print(\"=\" * 80)\n    print(json.dumps(complex_data, indent=2))\n    print(\"=\" * 80)\n    \n    # Save to file\n    complex_ticket_path = \"/content/complex_ticket_nested.json\"\n    with open(complex_ticket_path, 'w') as f:\n        json.dump(complex_data, f, indent=2)\n    \n    print(f\"\\\\nüíæ Saved to: {complex_ticket_path}\")\n    print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n    \n    # Demonstrate accessing nested data\n    print(\"\\\\nüîç Accessing Nested Data:\")\n    print(f\"  ‚Ä¢ User: {complex_data['user_name']}\")\n    print(f\"  ‚Ä¢ Device: {complex_data['device']['manufacturer']} {complex_data['device']['model']}\")\n    print(f\"  ‚Ä¢ CPU: {complex_data['device']['processor']['brand']} {complex_data['device']['processor']['model']}\")\n    print(f\"  ‚Ä¢ RAM: {complex_data['device']['ram']['capacity_gb']}GB {complex_data['device']['ram']['type']}\")\n    print(f\"  ‚Ä¢ GPU: {complex_data['device']['gpu']['manufacturer']} {complex_data['device']['gpu']['model']}\")\n    print(f\"  ‚Ä¢ Storage drives: {len(complex_data['device']['storage'])}\")\n    for i, drive in enumerate(complex_data['device']['storage'], 1):\n        print(f\"    - Drive {i}: {drive['capacity_tb']}TB {drive['type']} ({drive['purpose']})\")\n\nexcept json.JSONDecodeError as e:\n    print(f\"‚ùå JSON Parsing Error: {e}\")\n    print(f\"\\\\nüìÑ Raw response from API:\")\n    print(\"=\" * 80)\n    print(complex_json)\n    print(\"=\" * 80)\n    print(f\"\\\\nüí° Tip: The model returned invalid JSON. Try adjusting the prompt or using a different model.\")"
  },
  {
   "cell_type": "markdown",
   "id": "example1-summary",
   "metadata": {},
   "source": [
    "### üìä Key Teaching Points\n",
    "\n",
    "From Example 1, we learned:\n",
    "\n",
    "**Flat vs. Nested Structures:**\n",
    "- ‚úÖ Flat structure: Simple tickets with top-level fields only\n",
    "- ‚úÖ Nested structure: Complex tickets with related data grouped in objects\n",
    "\n",
    "**Handling Arrays:**\n",
    "- ‚úÖ Storage devices are represented as an array of objects\n",
    "- ‚úÖ Each item in the array has the same structure\n",
    "- ‚úÖ Easy to iterate through and process programmatically\n",
    "\n",
    "**Extracting Implied Information:**\n",
    "- ‚úÖ Urgency inferred from context (\"blocking project deadline on Friday\" ‚Üí High)\n",
    "- ‚úÖ Suspected cause identified from symptoms described\n",
    "- ‚úÖ Device purpose categorized (OS drive vs. data drive)\n",
    "\n",
    "üí° **Best Practice:** Use nested structures when data has clear relationships (device ‚Üí components). This keeps data organized and easier to work with.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iteration1-complete",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ ITERATION 1 COMPLETE\n",
    "\n",
    "## Sections Created:\n",
    "1. ‚úÖ Introduction (Theory, Business Problem, Key Concepts)\n",
    "2. ‚úÖ Setup (API Config, Dependencies, Imports)\n",
    "3. ‚úÖ Understanding Output Formats (CSV, JSON, Pydantic)\n",
    "4. ‚úÖ Practical Examples - Example 1 (Support Ticket Parsing)\n",
    "\n",
    "## Files Saved:\n",
    "- `/content/inventory_data.csv` - Conference room equipment inventory\n",
    "- `/content/ticket_data.json` - Basic support ticket extraction\n",
    "- `/content/validated_ticket.json` - Pydantic-validated ticket\n",
    "- `/content/basic_ticket.json` - Laptop issue ticket with standard fields\n",
    "- `/content/complex_ticket_nested.json` - Workstation ticket with nested device specs\n",
    "\n",
    "## What You've Learned:\n",
    "‚úÖ Why structured data extraction matters in IT support  \n",
    "‚úÖ When to use CSV, JSON, or Pydantic models  \n",
    "‚úÖ How to extract data using gpt-5-nano  \n",
    "‚úÖ Validation techniques with Pydantic  \n",
    "‚úÖ Handling nested structures and arrays  \n",
    "‚úÖ Saving extracted data to files  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ready for Iteration 2\n",
    "\n",
    "**Please review and approve before I continue.**\n",
    "\n",
    "Iteration 2 will add:\n",
    "- Example 2: Multiple Error Extraction (JSON Arrays)\n",
    "- Example 3: Hardware Inventory (CSV Format)\n",
    "- Data Validation Techniques\n",
    "- File Generation Patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2k7jlkmrkhs",
   "source": "## Example 2: Multiple Error Extraction (JSON Array)\n\nLet's extract multiple items from a single message into a structured array.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "2965glitedd",
   "source": "# Mock data: User reporting multiple different errors\nmultiple_errors_report = \"\"\"\nFrom: david.kumar@company.com\nSubject: Multiple System Errors Today - Please Help\n\nHi IT Team,\n\nI've been experiencing several different errors on my computer today (EMP-3392):\n\n1. Around 9 AM, I got a Windows error saying \"DRIVER_IRQL_NOT_LESS_OR_EQUAL\" \n   with a blue screen. The computer restarted automatically.\n\n2. At 10:30 AM, Adobe Acrobat crashed with error code 0xc0000005 when I tried \n   to open a PDF file from a client.\n\n3. Just before lunch, my network printer showed \"Error 49.FF04\" and stopped \n   printing completely. Other people can still print to it.\n\n4. Around 2 PM, I got another blue screen with \"SYSTEM_SERVICE_EXCEPTION\" error.\n\n5. Finally, at 3:15 PM, Outlook gave me error 0x80040600 and won't send emails now.\n\nI'm really frustrated because this is affecting my work. These errors seem random \nbut I'm worried something serious is wrong with my computer.\n\nPlease help!\nDavid Kumar\nSales Department\n\"\"\"\n\n# Extraction prompt for array structure\nextraction_prompt = f\"\"\"\nExtract ALL errors from this user report and format as JSON.\n\nReturn JSON with this structure:\n{{\n  \"user_name\": \"string\",\n  \"employee_id\": \"string\", \n  \"department\": \"string\",\n  \"report_date\": \"infer from context or use null\",\n  \"total_errors\": number,\n  \"user_sentiment\": \"string (frustrated/concerned/neutral/etc)\",\n  \"errors\": [\n    {{\n      \"error_number\": number,\n      \"error_code\": \"string or null\",\n      \"error_message\": \"string\",\n      \"source\": \"string (Windows/Application name/Hardware)\",\n      \"timestamp_context\": \"string (time mentioned in report)\",\n      \"error_type\": \"string (Blue Screen/Application Crash/Hardware Error/etc)\"\n    }}\n  ]\n}}\n\nExtract each error into a separate array item with consistent structure.\n\nUser report:\n{multiple_errors_report}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üîÑ Extracting multiple errors into JSON array...\\\\n\")\n\n# Make API call\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"medium\"}\n)\n\nerrors_json = response.output_text.strip()\nerrors_data = json.loads(errors_json)\n\nprint(\"üìã Extracted Multiple Errors:\")\nprint(\"=\" * 80)\nprint(json.dumps(errors_data, indent=2))\nprint(\"=\" * 80)\n\n# Save to file\nmultiple_errors_path = \"/content/multiple_errors.json\"\nwith open(multiple_errors_path, 'w') as f:\n    json.dump(errors_data, f, indent=2)\n\nprint(f\"\\\\nüíæ Saved to: {multiple_errors_path}\")\nprint(f\"üìä Tokens used: {response.usage.total_tokens}\")\n\n# Display insights\nprint(f\"\\\\nüîç Analysis:\")\nprint(f\"  ‚Ä¢ User: {errors_data['user_name']} ({errors_data['department']})\")\nprint(f\"  ‚Ä¢ Total errors reported: {errors_data['total_errors']}\")\nprint(f\"  ‚Ä¢ User sentiment: {errors_data['user_sentiment']}\")\nprint(f\"\\\\n  Error breakdown:\")\nfor error in errors_data['errors']:\n    print(f\"    {error['error_number']}. {error['error_type']} - {error['source']}\")\n    print(f\"       Time: {error['timestamp_context']}\")\n    if error['error_code']:\n        print(f\"       Code: {error['error_code']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9rp5ufel5pl",
   "source": "### üìä Key Teaching Point: Extracting Multiple Items\n\n**What we learned:**\n- ‚úÖ How to extract MULTIPLE similar items into a structured array\n- ‚úÖ Each array item has consistent structure (same fields)\n- ‚úÖ LLM can identify and separate distinct errors from narrative text\n- ‚úÖ We can include metadata (total count, sentiment) alongside the array\n- ‚úÖ Easy to process programmatically (loop through errors)\n\nüí° **Use case:** Any scenario with multiple similar items - error logs, equipment lists, action items from meetings, multiple user requests in one message.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "vke0bz13n5",
   "source": "## Example 3: Hardware Inventory (CSV Format)\n\nNow let's extract equipment information into CSV format - perfect for importing into spreadsheets or inventory systems.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3rrtcxing",
   "source": "# Mock data: Description of conference room equipment with various details\nconference_rooms_description = \"\"\"\nConference Room Inventory Report:\n\nExecutive Boardroom (3rd Floor):\n- Two 75-inch Samsung QN75 displays mounted on the wall (serials: SAMQN-8821, SAMQN-8822)\n- One Logitech Rally Bar camera system, serial number LOGI-RB-3345\n- Polycom RealPresence Trio 8800 conference phone, serial POLY-8800-991\n- Dell OptiPlex 7090 PC for presentations, serial DELL-OPT-4455\n\nRoom 2A (2nd Floor):\n- Single 65-inch LG OLED display, serial LG-OLED-2293  \n- Jabra PanaCast camera, serial JAB-PC-7721\n- Microsoft Surface Hub 2S 50-inch, serial MSFT-HUB-1203\n- Two wireless presentation adapters, serials WRLSS-001 and WRLSS-002\n\nTraining Room B (1st Floor):\n- Four 55-inch Sony Bravia displays (serials: SONY-BR-5501, SONY-BR-5502, SONY-BR-5503, SONY-BR-5504)\n- Cisco Webex Room Kit Pro, serial CISCO-WX-8832\n- Three HP EliteDesk 800 computers (serials: HP-ED-9901, HP-ED-9902, HP-ED-9903)\n- One portable projector - Epson PowerLite, serial EPSON-PL-4456\n\nSmall Meeting Room 105:\n- Single 43-inch Dell monitor, serial DELL-MON-3344\n- Logitech MeetUp camera, serial LOGI-MU-6655\n- One laptop docking station, serial DOCK-STN-2234\n\"\"\"\n\n# Extraction prompt for CSV\nextraction_prompt = f\"\"\"\nExtract conference room equipment inventory as CSV.\n\nUse these exact column headers:\nroom,device_type,manufacturer,model,serial_number,location_detail,quantity\n\nRules:\n- One row per device item\n- Include header row\n- device_type should be category like \"Display\", \"Camera\", \"Computer\", \"Phone System\", etc.\n- location_detail should include floor or mounting info if mentioned\n- quantity should be 1 for individual items\n- Use commas as separators\n- Don't use quotes unless value contains comma\n\nText:\n{conference_rooms_description}\n\nReturn ONLY the CSV data, no additional text.\n\"\"\"\n\nprint(\"üîÑ Extracting conference room inventory to CSV...\\\\n\")\n\n# Make API call\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\ncsv_output = response.output_text.strip()\n\nprint(\"üìä Extracted CSV Data:\")\nprint(\"=\" * 100)\nprint(csv_output)\nprint(\"=\" * 100)\n\n# Save to file\ninventory_csv_path = \"/content/conference_room_inventory.csv\"\nwith open(inventory_csv_path, 'w') as f:\n    f.write(csv_output)\n\nprint(f\"\\\\nüíæ Saved to: {inventory_csv_path}\")\nprint(f\"üìä Tokens used: {response.usage.total_tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0inouxwnks3i",
   "source": "# Verify and analyze the CSV\nprint(\"üîç Verifying CSV and analyzing inventory...\\n\")\n\n# Read with pandas\ndf = pd.read_csv(inventory_csv_path)\n\nprint(\"‚úÖ CSV is valid!\\n\")\nprint(f\"üìä Inventory Summary:\")\nprint(f\"  ‚Ä¢ Total items: {len(df)}\")\nprint(f\"  ‚Ä¢ Rooms covered: {df['room'].nunique()}\")\nprint(f\"  ‚Ä¢ Device types: {df['device_type'].nunique()}\")\nprint(f\"\\nüè¢ Items per room:\")\nprint(df.groupby('room').size().to_string())\nprint(f\"\\nüñ•Ô∏è Device type breakdown:\")\nprint(df.groupby('device_type').size().to_string())\n\nprint(f\"\\nüìã Full Inventory:\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)\n\nprint(f\"\\nüí° This CSV can be imported into Excel, Google Sheets, or an inventory database!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8t45kdhsydm",
   "source": "---\n\n# üõ°Ô∏è Data Validation\n\nExtracted data needs validation before use in production systems.\n\n---\n\n## üéØ Why Validation Matters\n\n**The reality of LLM extraction:**\n- ‚úÖ LLMs are very good but **not 100% accurate**\n- ‚úÖ Input data may be incomplete or ambiguous\n- ‚úÖ Downstream systems expect specific formats\n- ‚úÖ Early detection prevents costly errors later\n\n**Benefits of validation:**\n- üêõ **Catch issues early** - Before bad data reaches critical systems\n- üîÑ **Provide feedback** - Know what's missing or incorrect\n- üìä **Quality metrics** - Track extraction accuracy over time\n- üö® **Trigger human review** - Flag uncertain extractions\n\nLet's explore three key validation techniques:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "inwxucpcv9g",
   "source": "## Technique 1: Required Fields Check\n\nEnsure all critical fields are present and not empty.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cq16q2qaq54",
   "source": "def check_required_fields(data, required_fields):\n    \"\"\"\n    Check if all required fields are present and not empty.\n    \n    Args:\n        data (dict): Extracted data dictionary\n        required_fields (list): List of field names that must be present\n        \n    Returns:\n        dict: Validation result with is_valid flag and missing_fields list\n    \"\"\"\n    missing_fields = []\n    \n    for field in required_fields:\n        # Check if field exists\n        if field not in data:\n            missing_fields.append(field)\n        # Check if field is empty (None, empty string, empty list)\n        elif data[field] is None or data[field] == \"\" or data[field] == []:\n            missing_fields.append(field)\n    \n    return {\n        \"is_valid\": len(missing_fields) == 0,\n        \"missing_fields\": missing_fields,\n        \"message\": \"All required fields present\" if len(missing_fields) == 0 else f\"Missing fields: {', '.join(missing_fields)}\"\n    }\n\n# Example: Incomplete ticket data\nincomplete_ticket = {\n    \"user_name\": \"Alex Johnson\",\n    \"employee_id\": \"\",  # Empty!\n    \"contact_email\": \"alex.johnson@company.com\",\n    \"issue_summary\": None,  # Missing!\n    # department field completely missing\n}\n\nprint(\"üß™ Testing Required Fields Validation\\\\n\")\nprint(\"Ticket data:\")\nprint(json.dumps(incomplete_ticket, indent=2))\n\n# Define what fields are required\nrequired = [\"user_name\", \"employee_id\", \"contact_email\", \"issue_summary\", \"department\"]\n\n# Validate\nresult = check_required_fields(incomplete_ticket, required)\n\nprint(f\"\\\\nüìã Validation Result:\")\nprint(f\"  Valid: {result['is_valid']}\")\nprint(f\"  Message: {result['message']}\")\n\nif not result['is_valid']:\n    print(f\"\\\\n‚ùå Cannot process this ticket - missing critical information!\")\n    print(f\"  Action needed: Request missing fields from user\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "y4vd62426j",
   "source": "## Technique 2: Data Type & Format Validation\n\nCheck if data matches expected formats (email, employee ID, asset tag, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j12ky3cxm5",
   "source": "import re\n\ndef validate_data_formats(data):\n    \"\"\"\n    Validate data formats for common IT fields.\n    \n    Args:\n        data (dict): Extracted ticket data\n        \n    Returns:\n        dict: Validation results with specific format errors\n    \"\"\"\n    errors = []\n    \n    # Email format validation\n    if 'contact_email' in data and data['contact_email']:\n        email = data['contact_email']\n        if '@' not in email or '.' not in email.split('@')[-1]:\n            errors.append(f\"Invalid email format: {email}\")\n    \n    # Employee ID format validation (EMP-####)\n    if 'employee_id' in data and data['employee_id']:\n        emp_id = data['employee_id']\n        if not re.match(r'^EMP-\\\\d{4}$', emp_id):\n            errors.append(f\"Invalid employee ID format: {emp_id} (expected: EMP-####)\")\n    \n    # Asset tag validation (reasonable length)\n    if 'asset_tag' in data and data['asset_tag']:\n        asset_tag = data['asset_tag']\n        if len(asset_tag) < 5 or len(asset_tag) > 20:\n            errors.append(f\"Asset tag length invalid: {asset_tag} (expected: 5-20 chars)\")\n    \n    # Phone number validation (at least 7 digits)\n    if 'contact_phone' in data and data['contact_phone']:\n        phone = data['contact_phone']\n        digits_only = re.sub(r'\\\\D', '', phone)  # Remove non-digits\n        if len(digits_only) < 7:\n            errors.append(f\"Phone number too short: {phone}\")\n    \n    return {\n        \"is_valid\": len(errors) == 0,\n        \"errors\": errors,\n        \"message\": \"All formats valid\" if len(errors) == 0 else f\"Found {len(errors)} format error(s)\"\n    }\n\n# Example: Ticket with format errors\nticket_with_errors = {\n    \"user_name\": \"Patricia Lee\",\n    \"employee_id\": \"12345\",  # Wrong format (missing EMP- prefix)\n    \"contact_email\": \"patricia.lee.company.com\",  # Missing @\n    \"contact_phone\": \"555\",  # Too short\n    \"asset_tag\": \"PC\",  # Too short\n    \"issue_summary\": \"Computer won't start\"\n}\n\nprint(\"üß™ Testing Format Validation\\\\n\")\nprint(\"Ticket data:\")\nprint(json.dumps(ticket_with_errors, indent=2))\n\n# Validate formats\nresult = validate_data_formats(ticket_with_errors)\n\nprint(f\"\\\\nüìã Format Validation Result:\")\nprint(f\"  Valid: {result['is_valid']}\")\nprint(f\"  Message: {result['message']}\")\n\nif not result['is_valid']:\n    print(f\"\\\\n‚ùå Format Errors Detected:\")\n    for i, error in enumerate(result['errors'], 1):\n        print(f\"  {i}. {error}\")\n    print(f\"\\\\n‚ö†Ô∏è Action needed: Data needs correction before processing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "q3aw99csksl",
   "source": "## Technique 3: Confidence Scoring\n\nAsk the LLM to rate its own confidence and identify uncertain extractions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e50jyzy7n1v",
   "source": "# Mock data: Vague, incomplete ticket\nvague_ticket = \"\"\"\nFrom: someone@company.com\nSubject: Help\n\nSomething's wrong with my computer. It's not working right.\nCan you fix it?\n\"\"\"\n\n# Extraction with confidence scoring\nextraction_prompt = f\"\"\"\nExtract support ticket information from this email. Include a confidence score.\n\nReturn JSON with these fields:\n- user_name: Extract if possible, use null if not found\n- employee_id: Extract if mentioned, use null otherwise\n- contact_email: Email address\n- issue_summary: Brief summary of the issue\n- urgency: Low/Medium/High/Critical (infer from context)\n- confidence_score: Your confidence in this extraction (0-100)\n- missing_fields: List of critical information that's missing\n- notes: Any concerns or ambiguities about the extraction\n\nEmail:\n{vague_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üîÑ Extracting vague ticket with confidence scoring...\\\\n\")\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"medium\"}\n)\n\nextraction_json = response.output_text.strip()\nextraction_data = json.loads(extraction_json)\n\nprint(\"üìã Extracted Data with Confidence:\")\nprint(\"=\" * 80)\nprint(json.dumps(extraction_data, indent=2))\nprint(\"=\" * 80)\n\n# Check confidence threshold\nCONFIDENCE_THRESHOLD = 70\n\nprint(f\"\\\\nüéØ Confidence Analysis:\")\nprint(f\"  Confidence Score: {extraction_data.get('confidence_score', 0)}/100\")\nprint(f\"  Threshold: {CONFIDENCE_THRESHOLD}/100\")\n\nif extraction_data.get('confidence_score', 0) < CONFIDENCE_THRESHOLD:\n    print(f\"\\\\n‚ö†Ô∏è LOW CONFIDENCE - Human Review Required!\")\n    print(f\"  Missing fields: {', '.join(extraction_data.get('missing_fields', []))}\")\n    print(f\"  Notes: {extraction_data.get('notes', 'N/A')}\")\n    print(f\"\\\\n  Action: Request more information from user\")\nelse:\n    print(f\"\\\\n‚úÖ High confidence - Safe to process automatically\")\n\nprint(f\"\\\\nüìä Tokens used: {response.usage.total_tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bwr4x2gg2qv",
   "source": "---\n\n# üíæ File Generation\n\nExtracted data should be saved for downstream use, record-keeping, and team sharing.\n\n---\n\n## üéØ Why Save Extracted Data?\n\n**Use cases for saved files:**\n- üìä **Import to other systems** - Databases, ticketing systems, spreadsheets\n- üìÅ **Record keeping** - Maintain audit trails and historical records\n- üîÑ **Batch processing** - Process multiple extractions together\n- üë• **Team sharing** - Share structured data with colleagues\n- üìà **Analysis** - Aggregate data for reporting and insights\n\nLet's explore different file saving patterns:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "xfrpyjdjpmc",
   "source": "## Pattern 1: Saving JSON Files with Timestamps\n\nCreate unique filenames with timestamps to avoid overwriting data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wqtentj17to",
   "source": "def save_json_with_timestamp(data, prefix=\"extracted_data\"):\n    \"\"\"\n    Save JSON data with timestamp in filename.\n    \n    Args:\n        data (dict): Data to save\n        prefix (str): Filename prefix\n        \n    Returns:\n        str: Path to saved file\n    \"\"\"\n    # Generate timestamp\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    # Create filename\n    filename = f\"/content/{prefix}_{timestamp}.json\"\n    \n    # Save file\n    with open(filename, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    return filename\n\n# Example: Save a ticket extraction\nexample_ticket = {\n    \"ticket_id\": \"TKT-2024-0157\",\n    \"user_name\": \"Emily Chen\",\n    \"employee_id\": \"EMP-8821\",\n    \"department\": \"Finance\",\n    \"issue_summary\": \"Excel crashes when opening large files\",\n    \"urgency\": \"Medium\",\n    \"extracted_at\": datetime.now().isoformat()\n}\n\nprint(\"üíæ Saving JSON with timestamp...\\\\n\")\n\n# Save the file\nfile_path = save_json_with_timestamp(example_ticket, prefix=\"ticket_extraction\")\n\nprint(f\"‚úÖ Saved to: {file_path}\")\nprint(f\"\\\\nüìã File contents:\")\nwith open(file_path, 'r') as f:\n    print(f.read())\n\n# Demonstrate saving multiple extractions\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"Saving multiple extractions...\\\\n\")\n\nimport time\n\nfor i in range(3):\n    ticket = {\n        \"ticket_id\": f\"TKT-2024-{100 + i}\",\n        \"user_name\": f\"User {i+1}\",\n        \"issue\": f\"Issue {i+1}\"\n    }\n    path = save_json_with_timestamp(ticket, prefix=\"ticket\")\n    print(f\"  Saved: {path}\")\n    time.sleep(1)  # Small delay to ensure different timestamps\n\nprint(\"\\\\n‚úÖ Each file has unique timestamp - no overwrites!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xxcybzlwefe",
   "source": "## Pattern 2: Saving CSV Files\n\nConvert extracted data to CSV format for spreadsheet compatibility.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v31udqjlpj",
   "source": "def save_to_csv(data_list, filename, fieldnames=None):\n    \"\"\"\n    Save list of dictionaries as CSV file.\n    \n    Args:\n        data_list (list): List of dictionaries to save\n        filename (str): Output filename\n        fieldnames (list): Optional list of field names (uses first dict keys if None)\n        \n    Returns:\n        str: Path to saved file\n    \"\"\"\n    if not data_list:\n        raise ValueError(\"data_list cannot be empty\")\n    \n    # Use provided fieldnames or extract from first dictionary\n    if fieldnames is None:\n        fieldnames = list(data_list[0].keys())\n    \n    # Ensure filename includes path\n    if not filename.startswith('/content/'):\n        filename = f\"/content/{filename}\"\n    \n    # Write CSV\n    with open(filename, 'w', newline='') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data_list)\n    \n    return filename\n\n# Example: Save inventory data\ninventory_items = [\n    {\n        \"room\": \"Conference A\",\n        \"device_type\": \"Display\",\n        \"manufacturer\": \"Samsung\",\n        \"model\": \"QN75\",\n        \"serial_number\": \"SAM-001\",\n        \"quantity\": 1\n    },\n    {\n        \"room\": \"Conference A\",\n        \"device_type\": \"Camera\",\n        \"manufacturer\": \"Logitech\",\n        \"model\": \"Rally Bar\",\n        \"serial_number\": \"LOG-445\",\n        \"quantity\": 1\n    },\n    {\n        \"room\": \"Conference B\",\n        \"device_type\": \"Display\",\n        \"manufacturer\": \"LG\",\n        \"model\": \"OLED65\",\n        \"serial_number\": \"LG-2293\",\n        \"quantity\": 1\n    }\n]\n\nprint(\"üíæ Saving inventory data to CSV...\\n\")\n\ncsv_path = save_to_csv(\n    inventory_items, \n    \"inventory_export.csv\",\n    fieldnames=[\"room\", \"device_type\", \"manufacturer\", \"model\", \"serial_number\", \"quantity\"]\n)\n\nprint(f\"‚úÖ Saved to: {csv_path}\")\n\n# Read and display\ndf = pd.read_csv(csv_path)\nprint(f\"\\nüìä CSV Contents ({len(df)} rows):\")\n\n# Display as DataFrame (better formatting in Jupyter)\ndisplay(df)\n\nprint(f\"\\nüí° This CSV can be opened in Excel or imported into inventory systems!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "m6s7supgzv8",
   "source": "## Pattern 3: Appending to Existing Files\n\nBuild up a log file with multiple extractions over time.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qfrf6xmv4jk",
   "source": "def append_to_ticket_log(ticket_data, log_file=\"/content/ticket_log.json\"):\n    \"\"\"\n    Append ticket data to existing log file.\n    \n    Args:\n        ticket_data (dict): Ticket data to append\n        log_file (str): Path to log file\n        \n    Returns:\n        dict: Status with total count\n    \"\"\"\n    # Add timestamp to ticket\n    ticket_data['logged_at'] = datetime.now().isoformat()\n    \n    # Check if file exists\n    if os.path.exists(log_file):\n        # Load existing data\n        with open(log_file, 'r') as f:\n            log_data = json.load(f)\n    else:\n        # Create new log structure\n        log_data = {\n            \"log_created\": datetime.now().isoformat(),\n            \"tickets\": []\n        }\n    \n    # Append new ticket\n    log_data[\"tickets\"].append(ticket_data)\n    log_data[\"total_tickets\"] = len(log_data[\"tickets\"])\n    log_data[\"last_updated\"] = datetime.now().isoformat()\n    \n    # Save back to file\n    with open(log_file, 'w') as f:\n        json.dump(log_data, f, indent=2)\n    \n    return {\n        \"success\": True,\n        \"total_tickets\": log_data[\"total_tickets\"],\n        \"log_file\": log_file\n    }\n\nprint(\"üìù Demonstrating ticket log appending...\\\\n\")\n\n# Simulate processing multiple tickets\ntickets_to_process = [\n    {\n        \"ticket_id\": \"TKT-001\",\n        \"user_name\": \"Alice Smith\",\n        \"issue\": \"Password reset needed\",\n        \"urgency\": \"Low\"\n    },\n    {\n        \"ticket_id\": \"TKT-002\", \n        \"user_name\": \"Bob Johnson\",\n        \"issue\": \"VPN not connecting\",\n        \"urgency\": \"High\"\n    },\n    {\n        \"ticket_id\": \"TKT-003\",\n        \"user_name\": \"Carol Davis\",\n        \"issue\": \"Printer offline\",\n        \"urgency\": \"Medium\"\n    }\n]\n\n# Process and log each ticket\nfor ticket in tickets_to_process:\n    result = append_to_ticket_log(ticket)\n    print(f\"‚úÖ Logged {ticket['ticket_id']} - Total tickets in log: {result['total_tickets']}\")\n    time.sleep(0.5)  # Small delay\n\n# Read and display final log\nprint(f\"\\\\nüìã Final Ticket Log:\")\nprint(\"=\" * 80)\nwith open(\"/content/ticket_log.json\", 'r') as f:\n    log_contents = json.load(f)\n\nprint(f\"Log created: {log_contents['log_created']}\")\nprint(f\"Last updated: {log_contents['last_updated']}\")\nprint(f\"Total tickets: {log_contents['total_tickets']}\\\\n\")\n\nprint(\"Tickets in log:\")\nfor i, ticket in enumerate(log_contents['tickets'], 1):\n    print(f\"  {i}. {ticket['ticket_id']}: {ticket['issue']} (Urgency: {ticket['urgency']})\")\n\nprint(\"=\" * 80)\nprint(f\"\\\\nüí° Log file grows with each append - perfect for ongoing ticket tracking!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ly5jwhlxesg",
   "source": "---\n\n# ‚úÖ ITERATION 2 COMPLETE\n\n## Sections Added:\n5. ‚úÖ Practical Examples (Continued)\n   - Example 2: Multiple Error Extraction (JSON Arrays)\n   - Example 3: Hardware Inventory (CSV Format)\n6. ‚úÖ Data Validation\n   - Technique 1: Required Fields Check\n   - Technique 2: Data Type & Format Validation\n   - Technique 3: Confidence Scoring\n7. ‚úÖ File Generation\n   - Pattern 1: JSON Files with Timestamps\n   - Pattern 2: CSV File Export\n   - Pattern 3: Appending to Log Files\n\n## Additional Files Saved:\n- `/content/multiple_errors.json` - Multiple error extraction from single report\n- `/content/conference_room_inventory.csv` - Conference room equipment inventory\n- `/content/ticket_extraction_YYYYMMDD_HHMMSS.json` - Timestamped ticket files\n- `/content/inventory_export.csv` - Exported inventory data\n- `/content/ticket_log.json` - Appended ticket log\n\n## What You've Learned:\n‚úÖ Extract multiple items into structured arrays  \n‚úÖ Generate CSV files for spreadsheet import  \n‚úÖ Validate extracted data (required fields, formats, confidence)  \n‚úÖ Save data with timestamps for uniqueness  \n‚úÖ Append to log files for ongoing tracking  \n‚úÖ Handle low-confidence extractions appropriately  \n\n---\n\n## üéØ Ready for Iteration 3\n\n**Please review and approve before I continue.**\n\nIteration 3 will add:\n- Batch Processing with progress tracking\n- Error Handling & Edge Cases (5 scenarios)\n- Mini-Project: Complete Support Ticket Intake System\n- Best Practices & Key Takeaways\n- Student Exercises\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "w0gvtvdyp3c",
   "source": "# üîÑ Batch Processing\n\nOften you need to process multiple items at once - backlogs, audits, migrations. Let's build a batch processor with progress tracking.\n\n---\n\n## üéØ Why Batch Processing?\n\n**Use cases:**\n- üì¨ **Process email backlog** - Extract data from hundreds of support emails\n- üìä **Audit existing tickets** - Re-extract data with improved prompts\n- üîÑ **Data migration** - Convert old formats to new structured data\n- üìà **Bulk analysis** - Extract insights from large document sets\n\n**Benefits:**\n- ‚ö° More efficient than one-by-one processing\n- üìä Progress tracking with visual feedback\n- üêõ Error tracking - separate successes from failures\n- üìÅ Organized output with summary reports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ile4p2bmnpi",
   "source": "# Create array of 5 different mock support tickets\nticket_batch = [\n    \"\"\"From: anna.rodriguez@company.com\n    Subject: Laptop Battery Draining Fast\n    Hi, I'm Anna Rodriguez (EMP-3401) from Marketing. My laptop battery only lasts 30 minutes now.\n    This is urgent as I have client meetings all day tomorrow. Please help!\"\"\",\n    \n    \"\"\"From: kevin.park@company.com\n    Subject: Cannot Connect to WiFi\n    Kevin Park here, Sales dept, EMP-5623. My laptop won't connect to the office WiFi.\n    I've tried restarting but it still doesn't work. Medium priority.\"\"\",\n    \n    \"\"\"From: lisa.chen@company.com  \n    Subject: Printer Not Working\n    Lisa Chen, Finance, EMP-7834. The printer on floor 3 shows 'Paper Jam' but there's no paper stuck.\n    Multiple people are affected. Needs fixing ASAP!\"\"\",\n    \n    \"\"\"From: marcus.johnson@company.com\n    Subject: Slow Computer Performance\n    Marcus Johnson, IT Support team member EMP-2019. My workstation is running extremely slow.\n    All applications take forever to load. Low priority but annoying.\"\"\",\n    \n    \"\"\"From: sophia.williams@company.com\n    Subject: Email Account Locked\n    Sophia Williams, HR department, EMP-9102. I got locked out after entering wrong password.\n    Need access urgently for payroll processing today. CRITICAL!\"\"\"\n]\n\ndef batch_extract_tickets(ticket_list):\n    \"\"\"\n    Process multiple tickets in batch with progress tracking.\n    \n    Args:\n        ticket_list (list): List of ticket text strings\n        \n    Returns:\n        dict: Results with successes, failures, and summary\n    \"\"\"\n    results = []\n    failures = []\n    \n    # Process with progress bar\n    for i, ticket_text in enumerate(tqdm(ticket_list, desc=\"Processing tickets\")):\n        try:\n            # Extraction prompt\n            extraction_prompt = f\"\"\"\n            Extract ticket information from this email. Return ONLY valid JSON.\n            \n            {{\n              \"ticket_id\": \"TKT-{datetime.now().strftime('%Y')}-{str(i+1).zfill(4)}\",\n              \"user_name\": \"full name\",\n              \"employee_id\": \"employee ID\",\n              \"department\": \"department name\",\n              \"contact_email\": \"email address\",\n              \"issue_summary\": \"brief summary\",\n              \"urgency\": \"Low/Medium/High/Critical\"\n            }}\n            \n            Email:\n            {ticket_text}\n            \"\"\"\n            \n            # Make API call\n            response = client.responses.create(\n                model=OPENAI_MODEL,\n                input=extraction_prompt,\n                text={\"verbosity\": \"low\"}\n            )\n            \n            # Parse JSON\n            extracted_data = json.loads(response.output_text.strip())\n            \n            # Add metadata\n            extracted_data['processed_at'] = datetime.now().isoformat()\n            extracted_data['batch_index'] = i + 1\n            \n            results.append(extracted_data)\n            \n        except Exception as e:\n            failures.append({\n                \"batch_index\": i + 1,\n                \"error\": str(e),\n                \"ticket_preview\": ticket_text[:100] + \"...\"\n            })\n    \n    return {\n        \"total_processed\": len(ticket_list),\n        \"successful\": len(results),\n        \"failed\": len(failures),\n        \"success_rate\": f\"{(len(results)/len(ticket_list)*100):.1f}%\",\n        \"results\": results,\n        \"failures\": failures\n    }\n\nprint(\"üîÑ Starting batch processing of 5 tickets...\\\\n\")\n\n# Process the batch\nbatch_results = batch_extract_tickets(ticket_batch)\n\nprint(f\"\\\\n‚úÖ Batch Processing Complete!\\\\n\")\nprint(f\"üìä Summary:\")\nprint(f\"  ‚Ä¢ Total tickets: {batch_results['total_processed']}\")\nprint(f\"  ‚Ä¢ Successful: {batch_results['successful']}\")\nprint(f\"  ‚Ä¢ Failed: {batch_results['failed']}\")\nprint(f\"  ‚Ä¢ Success rate: {batch_results['success_rate']}\")\n\n# Display successful extractions\nif batch_results['results']:\n    print(f\"\\\\nüìã Successfully Extracted Tickets:\")\n    for ticket in batch_results['results']:\n        print(f\"  {ticket['ticket_id']}: {ticket['user_name']} - {ticket['issue_summary']} ({ticket['urgency']})\")\n\n# Display failures if any\nif batch_results['failures']:\n    print(f\"\\\\n‚ùå Failed Extractions:\")\n    for failure in batch_results['failures']:\n        print(f\"  Ticket {failure['batch_index']}: {failure['error']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9oz7eiazwk",
   "source": "# Save batch results to files\nprint(\"üíæ Saving batch results...\\\\n\")\n\n# Save detailed JSON results\nbatch_json_path = \"/content/batch_tickets.json\"\nwith open(batch_json_path, 'w') as f:\n    json.dump(batch_results, f, indent=2)\nprint(f\"‚úÖ Saved detailed results to: {batch_json_path}\")\n\n# Save as CSV for easy viewing\nif batch_results['results']:\n    # Prepare data for CSV (flatten structure)\n    csv_data = []\n    for ticket in batch_results['results']:\n        csv_data.append({\n            'ticket_id': ticket['ticket_id'],\n            'user_name': ticket['user_name'],\n            'employee_id': ticket['employee_id'],\n            'department': ticket['department'],\n            'contact_email': ticket['contact_email'],\n            'issue_summary': ticket['issue_summary'],\n            'urgency': ticket['urgency'],\n            'processed_at': ticket['processed_at']\n        })\n    \n    # Save to CSV\n    batch_csv_path = \"/content/batch_tickets.csv\"\n    df_batch = pd.DataFrame(csv_data)\n    df_batch.to_csv(batch_csv_path, index=False)\n    print(f\"‚úÖ Saved CSV summary to: {batch_csv_path}\")\n    \n    # Display the DataFrame\n    print(f\"\\\\nüìä Batch Results (CSV view):\")\n    display(df_batch)\nelse:\n    print(\"‚ö†Ô∏è No successful extractions to save to CSV\")\n\nprint(f\"\\\\nüéâ Batch processing complete! Processed {batch_results['total_processed']} tickets.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v2hfr3nvqm",
   "source": "---\n\n# ‚ö†Ô∏è Error Handling & Edge Cases\n\nReal-world data is messy. Let's handle common problems gracefully.\n\n---\n\n## üéØ Introduction\n\n**The reality:**\n- Users provide incomplete information\n- Data contains contradictions\n- Formats are ambiguous\n- Critical fields may be missing\n\n**Our approach:**\n- Detect issues early\n- Handle gracefully with fallbacks\n- Provide clear feedback\n- Flag for human review when needed\n\nLet's demonstrate 5 common scenarios:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qzpqmxe614b",
   "source": "## Scenario 1: Missing Critical Information\n\nHandle extremely vague tickets that lack essential details.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6upf6f9lkig",
   "source": "# Extremely vague ticket\nvague_ticket = \"\"\"\nMy computer isn't working. Please help!\n\"\"\"\n\n# Enhanced extraction prompt that handles missing info\nextraction_prompt = f\"\"\"\nExtract ticket information from this email. Handle missing information gracefully.\n\nReturn JSON:\n{{\n  \"user_name\": \"name or null\",\n  \"employee_id\": \"ID or null\",\n  \"department\": \"department or null\",\n  \"contact_email\": \"email or null\",\n  \"issue_summary\": \"summary or 'Unspecified computer issue'\",\n  \"urgency\": \"Low/Medium/High/Critical\",\n  \"confidence_score\": 0-100,\n  \"missing_fields\": [\"list\", \"of\", \"missing\", \"critical\", \"fields\"],\n  \"follow_up_questions\": [\"questions to ask user for missing info\"]\n}}\n\nEmail:\n{vague_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üß™ Scenario 1: Missing Critical Information\\\\n\")\nprint(\"Ticket text:\")\nprint(vague_ticket)\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\nincomplete_data = json.loads(response.output_text.strip())\n\nprint(\"\\\\nüìã Extracted Data:\")\nprint(json.dumps(incomplete_data, indent=2))\n\n# Save to file\nincomplete_path = \"/content/incomplete_ticket.json\"\nwith open(incomplete_path, 'w') as f:\n    json.dump(incomplete_data, f, indent=2)\n\n# Analysis\nprint(f\"\\\\nüéØ Analysis:\")\nprint(f\"  Confidence Score: {incomplete_data.get('confidence_score', 0)}/100\")\nprint(f\"  Missing Fields: {', '.join(incomplete_data.get('missing_fields', []))}\")\n\nif incomplete_data.get('confidence_score', 0) < 70:\n    print(f\"\\\\n‚ö†Ô∏è LOW CONFIDENCE - Action Required!\")\n    print(f\"\\\\n  Follow-up questions to ask user:\")\n    for i, question in enumerate(incomplete_data.get('follow_up_questions', []), 1):\n        print(f\"    {i}. {question}\")\n    print(f\"\\\\n  ‚ùå Cannot auto-process - needs human review\")\nelse:\n    print(f\"\\\\n‚úÖ Sufficient information to proceed\")\n\nprint(f\"\\\\nüíæ Saved to: {incomplete_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bo8dge1y2dh",
   "source": "## Scenario 2: Contradictory Information\n\nHandle tickets with conflicting statements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "96echgm7kb",
   "source": "# Ticket with contradictions\ncontradictory_ticket = \"\"\"\nFrom: james.anderson@company.com\nSubject: Network Issue - Urgent but can wait\n\nHi IT,\n\nI need help with a network problem. It's very urgent and critical, but it's also \nlow priority and can wait until next week. The issue is important but not really \na big deal.\n\nJames or maybe it's Jim Anderson (I go by both)\nEmployee ID: EMP-4501 or 4502, I forget which\n\"\"\"\n\nextraction_prompt = f\"\"\"\nExtract ticket information. Detect and handle contradictions.\n\nReturn JSON:\n{{\n  \"user_name\": \"best judgment on name\",\n  \"employee_id\": \"most likely ID\",\n  \"issue_summary\": \"summary\",\n  \"urgency\": \"best judgment: Low/Medium/High/Critical\",\n  \"contradiction_detected\": true or false,\n  \"contradictions_found\": [\"list of contradictions\"],\n  \"resolution_notes\": \"how you resolved each contradiction\"\n}}\n\nEmail:\n{contradictory_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üß™ Scenario 2: Contradictory Information\\\\n\")\nprint(\"Ticket text:\")\nprint(contradictory_ticket)\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\ncontradictory_data = json.loads(response.output_text.strip())\n\nprint(\"\\\\nüìã Extracted Data:\")\nprint(json.dumps(contradictory_data, indent=2))\n\n# Save to file\ncontradictory_path = \"/content/contradictory_ticket.json\"\nwith open(contradictory_path, 'w') as f:\n    json.dump(contradictory_data, f, indent=2)\n\n# Analysis\nprint(f\"\\\\nüéØ Analysis:\")\nif contradictory_data.get('contradiction_detected'):\n    print(f\"  ‚ö†Ô∏è Contradictions Detected!\")\n    print(f\"\\\\n  Issues found:\")\n    for i, contradiction in enumerate(contradictory_data.get('contradictions_found', []), 1):\n        print(f\"    {i}. {contradiction}\")\n    print(f\"\\\\n  Resolution: {contradictory_data.get('resolution_notes', 'N/A')}\")\n    print(f\"\\\\n  ‚ö†Ô∏è Recommendation: Flag for human verification\")\nelse:\n    print(f\"  ‚úÖ No contradictions detected\")\n\nprint(f\"\\\\nüíæ Saved to: {contradictory_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e57vg3hn8we",
   "source": "## Scenario 3: Ambiguous or Multiple Formats\n\nStandardize data when multiple format options exist.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "g6nhovl2e2u",
   "source": "# Ticket with ambiguous formats\nambiguous_ticket = \"\"\"\nFrom: rachel.kim@company.com OR rkim@company.com\n\nMy employee number is 5678 or EMP-5678, I'm not sure which format you need.\nPhone: 555-0123 or (555) 012-3456 or +1-555-012-3456\n\nRachel\n\"\"\"\n\nextraction_prompt = f\"\"\"\nExtract and STANDARDIZE data to consistent formats.\n\nStandard formats to use:\n- Employee ID: EMP-#### (4 digits with prefix)\n- Phone: XXX-XXXX (simple format, remove country code and formatting)\n- Email: Use primary corporate email\n\nReturn JSON:\n{{\n  \"user_name\": \"name\",\n  \"employee_id\": \"standardized to EMP-####\",\n  \"contact_email\": \"primary email chosen\",\n  \"contact_phone\": \"standardized format\",\n  \"format_notes\": \"explain standardization decisions\",\n  \"alternatives_found\": {{\n    \"employee_id_variants\": [\"list\", \"of\", \"formats\", \"found\"],\n    \"email_variants\": [\"list\"],\n    \"phone_variants\": [\"list\"]\n  }}\n}}\n\nEmail:\n{ambiguous_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üß™ Scenario 3: Ambiguous or Multiple Formats\\\\n\")\nprint(\"Ticket text:\")\nprint(ambiguous_ticket)\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\nambiguous_data = json.loads(response.output_text.strip())\n\nprint(\"\\\\nüìã Extracted & Standardized Data:\")\nprint(json.dumps(ambiguous_data, indent=2))\n\n# Save to file\nambiguous_path = \"/content/ambiguous_formats.json\"\nwith open(ambiguous_path, 'w') as f:\n    json.dump(ambiguous_data, f, indent=2)\n\n# Analysis\nprint(f\"\\\\nüéØ Standardization Results:\")\nprint(f\"  Employee ID: {ambiguous_data.get('employee_id')}\")\nprint(f\"  Email: {ambiguous_data.get('contact_email')}\")\nprint(f\"  Phone: {ambiguous_data.get('contact_phone')}\")\nprint(f\"\\\\n  Notes: {ambiguous_data.get('format_notes', 'N/A')}\")\n\nprint(f\"\\\\n‚úÖ Data standardized to consistent formats\")\nprint(f\"üíæ Saved to: {ambiguous_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8k0eh833oa8",
   "source": "## Scenario 4: Invalid Data Detected\n\nValidate extracted data and report specific issues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "83i1sgq7ops",
   "source": "# Ticket with invalid data\ninvalid_ticket = \"\"\"\nFrom: bad.email.format\nEmployee: 99\nAsset Tag: PC\nContact: jane.doe@company.com\n\nMy computer won't start. Please help!\n\"\"\"\n\n# Comprehensive validation function\ndef comprehensive_validation(data):\n    \"\"\"Validate extracted data against business rules.\"\"\"\n    validation_result = {\n        \"is_valid\": True,\n        \"errors\": [],\n        \"warnings\": []\n    }\n    \n    # Email validation\n    if 'contact_email' in data and data['contact_email']:\n        if '@' not in data['contact_email']:\n            validation_result['errors'].append(\"Invalid email: missing @\")\n            validation_result['is_valid'] = False\n    \n    # Employee ID format\n    if 'employee_id' in data and data['employee_id']:\n        if not re.match(r'^EMP-\\d{4}$', data['employee_id']):\n            validation_result['errors'].append(f\"Invalid employee ID format: {data['employee_id']} (expected: EMP-####)\")\n            validation_result['is_valid'] = False\n    \n    # Asset tag length\n    if 'asset_tag' in data and data['asset_tag']:\n        if len(data['asset_tag']) < 5:\n            validation_result['warnings'].append(f\"Asset tag too short: {data['asset_tag']} (expected: 5-20 chars)\")\n    \n    return validation_result\n\n# Extract with explicit instruction to extract AS-IS\nprint(\"üß™ Scenario 4: Invalid Data Detected\\\\n\")\nprint(\"Ticket text (with invalid data):\")\nprint(invalid_ticket)\n\nextraction_prompt = f\"\"\"\nExtract ticket data EXACTLY as written, even if formats look wrong.\nDo NOT skip or correct invalid data - extract it as-is so validation can catch issues.\n\nReturn JSON:\n{{\n  \"contact_email\": \"extract email EXACTLY as written (even if invalid)\",\n  \"employee_id\": \"extract ID EXACTLY as written (even if wrong format)\",\n  \"asset_tag\": \"extract tag EXACTLY as written (even if too short)\",\n  \"issue_summary\": \"summary\"\n}}\n\nIMPORTANT: Extract data as-is, don't validate or correct it.\n\nTicket:\n{invalid_ticket}\n\"\"\"\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\ninvalid_data = json.loads(response.output_text.strip())\n\nprint(\"\\\\nüìã Extracted Data (as-is, unvalidated):\")\nprint(json.dumps(invalid_data, indent=2))\n\n# Run validation\nvalidation = comprehensive_validation(invalid_data)\n\nprint(f\"\\\\nüîç Validation Results:\")\nprint(f\"  Valid: {validation['is_valid']}\")\n\nif validation['errors']:\n    print(f\"\\\\n  ‚ùå Errors ({len(validation['errors'])}):\")\n    for error in validation['errors']:\n        print(f\"    ‚Ä¢ {error}\")\n\nif validation['warnings']:\n    print(f\"\\\\n  ‚ö†Ô∏è Warnings ({len(validation['warnings'])}):\")\n    for warning in validation['warnings']:\n        print(f\"    ‚Ä¢ {warning}\")\n\n# Save both data and validation report\ncombined_result = {\n    \"extracted_data\": invalid_data,\n    \"validation\": validation\n}\n\ninvalid_path = \"/content/invalid_data_validation.json\"\nwith open(invalid_path, 'w') as f:\n    json.dump(combined_result, f, indent=2)\n\nprint(f\"\\\\nüíæ Saved data and validation report to: {invalid_path}\")\n\nif not validation['is_valid']:\n    print(f\"\\\\n‚õî Action Required: Data needs correction before processing\")\n    print(f\"\\\\nüí° This demonstrates why validation is critical:\")\n    print(f\"   The LLM extracted data as-is, and validation caught the problems!\")\nelse:\n    print(f\"\\\\n‚úÖ All validations passed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "56g1xsxmljj",
   "source": "## Scenario 5: Partial Success\n\nHandle vague secondhand reports where only some information can be extracted.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0hr4zy11s6fp",
   "source": "# Extremely vague secondhand report\npartial_ticket = \"\"\"\nSomething wrong in room B. Sarah mentioned it earlier.\n\"\"\"\n\nextraction_prompt = f\"\"\"\nExtract what you can from this vague report. Be honest about what's unknown.\n\nReturn JSON:\n{{\n  \"extraction_status\": \"complete/partial/minimal\",\n  \"successfully_extracted\": {{\n    \"location\": \"value or null\",\n    \"reported_by\": \"value or null\",\n    \"issue_summary\": \"value or null\"\n  }},\n  \"failed_to_extract\": [\"list\", \"of\", \"fields\", \"that\", \"couldn't\", \"be\", \"determined\"],\n  \"confidence_score\": 0-100,\n  \"assumptions_made\": [\"list any assumptions\"],\n  \"recommended_actions\": [\"what to do next\"]\n}}\n\nReport:\n{partial_ticket}\n\nReturn ONLY valid JSON.\n\"\"\"\n\nprint(\"üß™ Scenario 5: Partial Success\\\\n\")\nprint(\"Ticket text (very vague secondhand report):\")\nprint(partial_ticket)\n\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=extraction_prompt,\n    text={\"verbosity\": \"low\"}\n)\n\npartial_data = json.loads(response.output_text.strip())\n\nprint(\"\\\\nüìã Extraction Result:\")\nprint(json.dumps(partial_data, indent=2))\n\n# Save to file\npartial_path = \"/content/partial_extraction.json\"\nwith open(partial_path, 'w') as f:\n    json.dump(partial_data, f, indent=2)\n\n# Analysis\nprint(f\"\\\\nüéØ Analysis:\")\nprint(f\"  Extraction Status: {partial_data.get('extraction_status', 'unknown')}\")\nprint(f\"  Confidence: {partial_data.get('confidence_score', 0)}/100\")\n\nprint(f\"\\\\n  ‚úÖ Successfully Extracted:\")\nfor field, value in partial_data.get('successfully_extracted', {}).items():\n    if value:\n        print(f\"    ‚Ä¢ {field}: {value}\")\n\nprint(f\"\\\\n  ‚ùå Could Not Extract:\")\nfor field in partial_data.get('failed_to_extract', []):\n    print(f\"    ‚Ä¢ {field}\")\n\nprint(f\"\\\\n  üìù Recommended Actions:\")\nfor i, action in enumerate(partial_data.get('recommended_actions', []), 1):\n    print(f\"    {i}. {action}\")\n\nprint(f\"\\\\n‚ö†Ô∏è Status: Partial extraction only - human follow-up required\")\nprint(f\"üíæ Saved to: {partial_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qtt23jhtot",
   "source": "## Comprehensive Error Handler\n\nCombine all techniques into a robust extraction function.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ndq9hc51hda",
   "source": "def safe_extract_with_validation(ticket_text):\n    \"\"\"\n    Master extraction function with comprehensive error handling.\n    \n    Returns: (extracted_data, overall_status, report)\n    - overall_status: \"ready\", \"needs_review\", or \"requires_action\"\n    \"\"\"\n    try:\n        # Enhanced extraction with confidence and validation\n        extraction_prompt = f\"\"\"\n        Extract ticket data with quality indicators.\n        \n        Return JSON:\n        {{\n          \"user_name\": \"name or null\",\n          \"employee_id\": \"ID or null\",\n          \"department\": \"dept or null\",\n          \"contact_email\": \"email or null\",\n          \"issue_summary\": \"summary\",\n          \"urgency\": \"Low/Medium/High/Critical\",\n          \"confidence_score\": 0-100,\n          \"missing_critical_fields\": [\"list\"],\n          \"data_quality_notes\": \"any concerns or issues\"\n        }}\n        \n        Ticket:\n        {ticket_text}\n        \"\"\"\n        \n        response = client.responses.create(\n            model=OPENAI_MODEL,\n            input=extraction_prompt,\n            text={\"verbosity\": \"low\"}\n        )\n        \n        extracted = json.loads(response.output_text.strip())\n        \n        # Validate formats\n        validation = comprehensive_validation(extracted)\n        \n        # Check confidence\n        confidence = extracted.get('confidence_score', 0)\n        \n        # Determine status\n        if validation['is_valid'] and confidence >= 80 and not extracted.get('missing_critical_fields'):\n            status = \"ready\"\n        elif validation['is_valid'] and confidence >= 60:\n            status = \"needs_review\"\n        else:\n            status = \"requires_action\"\n        \n        # Generate report\n        report = {\n            \"extraction_status\": status,\n            \"confidence_score\": confidence,\n            \"validation_passed\": validation['is_valid'],\n            \"validation_errors\": validation.get('errors', []),\n            \"validation_warnings\": validation.get('warnings', []),\n            \"missing_fields\": extracted.get('missing_critical_fields', []),\n            \"notes\": extracted.get('data_quality_notes', '')\n        }\n        \n        return extracted, status, report\n        \n    except Exception as e:\n        return None, \"error\", {\"error\": str(e), \"message\": \"Extraction failed\"}\n\n# Test with different ticket qualities\ntest_tickets = [\n    (\"Complete ticket\", \"\"\"From: john.doe@company.com\n    I'm John Doe (EMP-1234) from Sales. My laptop won't connect to WiFi. High priority please!\"\"\"),\n    \n    (\"Vague ticket\", \"\"\"Computer broken. Help!\"\"\"),\n    \n    (\"Medium quality\", \"\"\"Sarah from HR, laptop slow, not too urgent\"\"\")\n]\n\nprint(\"üî¨ Testing Comprehensive Error Handler\\\\n\")\nprint(\"=\" * 80)\n\nfor label, ticket in test_tickets:\n    print(f\"\\\\nüìã Test: {label}\")\n    print(f\"Ticket: {ticket[:60]}...\")\n    \n    data, status, report = safe_extract_with_validation(ticket)\n    \n    print(f\"\\\\n  Status: {status.upper()}\")\n    print(f\"  Confidence: {report.get('confidence_score', 'N/A')}/100\")\n    \n    if status == \"ready\":\n        print(f\"  ‚úÖ Ready for automatic processing\")\n    elif status == \"needs_review\":\n        print(f\"  ‚ö†Ô∏è Needs human review before processing\")\n    elif status == \"requires_action\":\n        print(f\"  ‚ùå Requires user follow-up\")\n        if report.get('missing_fields'):\n            print(f\"  Missing: {', '.join(report['missing_fields'])}\")\n    \n    print(\"-\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9rv7f7ajyuv",
   "source": "---\n\n# üéØ Mini-Project: Support Ticket Intake System\n\nLet's build a complete end-to-end system that processes support requests.\n\n---\n\n## üìã System Requirements\n\nOur system will:\n1. ‚úÖ Take raw support request text\n2. ‚úÖ Extract structured information\n3. ‚úÖ Validate the extraction\n4. ‚úÖ Check confidence and completeness\n5. ‚úÖ Save to appropriate files (JSON + CSV)\n6. ‚úÖ Generate formatted ticket summary\n7. ‚úÖ Provide feedback if information is insufficient\n\nLet's build it step by step!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qds724vq3td",
   "source": "def process_support_request(request_text):\n    \"\"\"\n    Complete ticket intake system - processes support request end-to-end.\n    \n    Args:\n        request_text (str): Raw support request\n        \n    Returns:\n        dict: Processing result with status and details\n    \"\"\"\n    print(f\"üé´ Processing Support Request...\")\n    print(f\"{'='*80}\\\\n\")\n    \n    # Step 1: Extract structured information\n    extraction_prompt = f\"\"\"\n    Extract complete support ticket information.\n    \n    Return JSON:\n    {{\n      \"ticket_id\": \"TKT-{datetime.now().strftime('%Y%m%d')}-{datetime.now().strftime('%H%M%S')}\",\n      \"user_name\": \"name or null\",\n      \"employee_id\": \"ID or null\", \n      \"department\": \"dept or null\",\n      \"contact_email\": \"email or null\",\n      \"contact_phone\": \"phone or null\",\n      \"issue_summary\": \"summary\",\n      \"issue_details\": \"detailed description\",\n      \"urgency\": \"Low/Medium/High/Critical\",\n      \"confidence_score\": 0-100,\n      \"missing_info\": [\"list of missing fields\"]\n    }}\n    \n    Request:\n    {request_text}\n    \"\"\"\n    \n    try:\n        # Extract\n        response = client.responses.create(\n            model=OPENAI_MODEL,\n            input=extraction_prompt,\n            text={\"verbosity\": \"low\"}\n        )\n        \n        ticket_data = json.loads(response.output_text.strip())\n        ticket_data['processed_at'] = datetime.now().isoformat()\n        ticket_data['raw_request'] = request_text\n        \n        # Step 2: Validate\n        validation = comprehensive_validation(ticket_data)\n        \n        # Step 3: Determine status\n        confidence = ticket_data.get('confidence_score', 0)\n        missing = ticket_data.get('missing_info', [])\n        \n        if validation['is_valid'] and confidence >= 80 and len(missing) == 0:\n            status = \"Ready\"\n            action = \"Auto-process\"\n        elif validation['is_valid'] and confidence >= 60:\n            status = \"Needs Review\"\n            action = \"Human verification recommended\"\n        else:\n            status = \"Requires Action\"\n            action = \"Request additional information from user\"\n        \n        ticket_data['processing_status'] = status\n        ticket_data['recommended_action'] = action\n        ticket_data['validation_result'] = validation\n        \n        # Step 4: Save to files\n        # JSON (detailed)\n        json_path = f\"/content/processed_tickets_log.json\"\n        \n        # Load existing or create new\n        if os.path.exists(json_path):\n            with open(json_path, 'r') as f:\n                log = json.load(f)\n        else:\n            log = {\"tickets\": [], \"total_count\": 0}\n        \n        log['tickets'].append(ticket_data)\n        log['total_count'] = len(log['tickets'])\n        log['last_updated'] = datetime.now().isoformat()\n        \n        with open(json_path, 'w') as f:\n            json.dump(log, f, indent=2)\n        \n        # CSV (summary)\n        csv_path = \"/content/processed_tickets_summary.csv\"\n        csv_data = {\n            'ticket_id': ticket_data['ticket_id'],\n            'user_name': ticket_data.get('user_name', 'Unknown'),\n            'department': ticket_data.get('department', 'Unknown'),\n            'issue_summary': ticket_data['issue_summary'],\n            'urgency': ticket_data['urgency'],\n            'status': status,\n            'confidence': confidence,\n            'processed_at': ticket_data['processed_at']\n        }\n        \n        # Append to CSV\n        file_exists = os.path.exists(csv_path)\n        with open(csv_path, 'a', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=csv_data.keys())\n            if not file_exists:\n                writer.writeheader()\n            writer.writerow(csv_data)\n        \n        # Step 5: Generate summary\n        print(f\"‚úÖ TICKET PROCESSED\\\\n\")\n        print(f\"Ticket ID: {ticket_data['ticket_id']}\")\n        print(f\"Status: {status}\")\n        print(f\"Confidence: {confidence}/100\")\n        print(f\"Action: {action}\\\\n\")\n        \n        print(f\"üìã Ticket Details:\")\n        print(f\"  User: {ticket_data.get('user_name', 'Unknown')}\")\n        print(f\"  Department: {ticket_data.get('department', 'Unknown')}\")\n        print(f\"  Issue: {ticket_data['issue_summary']}\")\n        print(f\"  Urgency: {ticket_data['urgency']}\")\n        \n        if missing:\n            print(f\"\\\\n‚ö†Ô∏è Missing Information:\")\n            for item in missing:\n                print(f\"  ‚Ä¢ {item}\")\n        \n        if validation['errors']:\n            print(f\"\\\\n‚ùå Validation Errors:\")\n            for error in validation['errors']:\n                print(f\"  ‚Ä¢ {error}\")\n        \n        print(f\"\\\\nüíæ Saved to:\")\n        print(f\"  ‚Ä¢ {json_path} (detailed log)\")\n        print(f\"  ‚Ä¢ {csv_path} (summary)\")\n        \n        print(f\"\\\\n{'='*80}\")\n        \n        return {\n            \"success\": True,\n            \"ticket_id\": ticket_data['ticket_id'],\n            \"status\": status,\n            \"ticket_data\": ticket_data\n        }\n        \n    except Exception as e:\n        print(f\"‚ùå Processing failed: {e}\")\n        return {\n            \"success\": False,\n            \"error\": str(e)\n        }\n\nprint(\"‚úÖ Support Ticket Intake System Ready!\\\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "p4hdw0r6jnm",
   "source": "# Test Case 1: Complete, clear ticket\ntest1 = \"\"\"\nFrom: alex.morgan@company.com\nEmployee ID: EMP-5521\nDepartment: Engineering\n\nHi IT Support,\n\nMy Dell Latitude laptop is overheating and shutting down randomly during video calls.\nThis is high priority as I have important client demos this week.\n\nYou can reach me at ext. 3344 or this email.\n\nThanks,\nAlex Morgan\n\"\"\"\n\nresult1 = process_support_request(test1)\nprint(\"\\\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "94lefx53n4a",
   "source": "# Test Case 2: Vague ticket\ntest2 = \"\"\"\nComputer not working properly. Need help.\n\"\"\"\n\nresult2 = process_support_request(test2)\nprint(\"\\\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "pppz1mckfaf",
   "source": "# Test Case 3: Complex ticket with nested device info\ntest3 = \"\"\"\nFrom: tech.lead@company.com\nSubject: Workstation GPU Issues\n\nHey IT,\n\nI'm the tech lead for the design team (EMP-8934). Our main rendering workstation  \n(Dell Precision 7920 with NVIDIA Quadro RTX 8000, 256GB RAM) keeps crashing during\n3D rendering jobs. Critical priority - we have a project deadline tomorrow!\n\nThe system has dual Xeon processors and runs Windows 11 Pro.\n\nContact: maria.santos@company.com or 555-0199\n\nMaria Santos\n\"\"\"\n\nresult3 = process_support_request(test3)\nprint(\"\\\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hik8iwnw3c",
   "source": "# View the processed tickets summary\nprint(\"üìä PROCESSED TICKETS SUMMARY\\\\n\")\nprint(\"=\" * 80)\n\n# Load and display CSV summary\nif os.path.exists(\"/content/processed_tickets_summary.csv\"):\n    df_summary = pd.read_csv(\"/content/processed_tickets_summary.csv\")\n    print(f\"\\\\nTotal tickets processed: {len(df_summary)}\\\\n\")\n    display(df_summary)\nelse:\n    print(\"No tickets processed yet.\")\n\nprint(\"\\\\n\" + \"=\" * 80)\nprint(\"\\\\nüéâ Mini-Project Complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z64a062o1fa",
   "source": "---\n\n# ‚úÖ Best Practices & Key Takeaways\n\n## üéØ Best Practices Summary\n\n### 1. Always Validate Extracted Data\n- ‚úÖ Check required fields are present and not empty\n- ‚úÖ Validate formats (email, IDs, phone numbers)\n- ‚úÖ Use confidence scores to flag uncertain extractions\n- ‚úÖ Implement validation before using data downstream\n\n### 2. Choose the Right Model\n- ‚úÖ **gpt-5-nano** works well for most extraction tasks (cost-efficient)\n- ‚úÖ Only upgrade to premium models when quality difference matters\n- ‚úÖ Test with cheaper models first\n\n### 3. Use Appropriate Output Formats\n- ‚úÖ **CSV**: Asset lists, inventories, simple databases\n- ‚úÖ **JSON**: Support tickets, complex records, API integration\n- ‚úÖ **Pydantic**: Production systems, database integration, strict validation\n\n### 4. Handle Missing Information Gracefully\n- ‚úÖ Use `null` for missing fields instead of guessing\n- ‚úÖ Ask LLM to list missing_fields in response\n- ‚úÖ Generate follow-up questions for users\n- ‚úÖ Set confidence thresholds (e.g., < 70 = needs review)\n\n### 5. Save Extractions for Record-Keeping\n- ‚úÖ Use timestamps in filenames to prevent overwrites\n- ‚úÖ Save detailed data (JSON) AND summary data (CSV)\n- ‚úÖ Append to log files for ongoing tracking\n- ‚úÖ Include metadata (processed_at, confidence, etc.)\n\n### 6. Batch Process When Possible\n- ‚úÖ More efficient than one-by-one\n- ‚úÖ Use progress indicators (tqdm)\n- ‚úÖ Track successes and failures separately\n- ‚úÖ Generate summary reports\n\n### 7. Implement Error Handling\n- ‚úÖ Always use try/except blocks\n- ‚úÖ Handle rate limits gracefully\n- ‚úÖ Provide helpful error messages\n- ‚úÖ Log errors for debugging\n\n### 8. Structure Prompts Effectively\n- ‚úÖ Be specific about desired output format\n- ‚úÖ Provide example structure with concrete values\n- ‚úÖ Use \"Return ONLY valid JSON\" for structured output\n- ‚úÖ Set verbosity to \"low\" for cleaner JSON responses\n\n---\n\n## üìã When to Use Each Format\n\n| Format | Best For | Pros | Cons |\n|--------|----------|------|------|\n| **CSV** | Equipment lists, simple inventories, Excel reports | Easy to open in Excel, simple structure | No nesting, all strings |\n| **JSON** | Support tickets, complex data, API integration | Flexible, supports nesting, native types | More verbose |\n| **Pydantic** | Production systems, strict validation | Type safety, auto-validation, clear errors | Requires schema definition |\n\n---\n\n## üí∞ Cost Considerations\n\n**Extraction tasks are cost-effective with gpt-5-nano:**\n- ‚úÖ Input: $0.05 / 1M tokens\n- ‚úÖ Output: $0.40 / 1M tokens\n- ‚úÖ Typical ticket: ~400 tokens total = ~$0.00016 (fraction of a cent!)\n\n**Tips for managing costs:**\n- üìä Monitor token usage with `response.usage.total_tokens`\n- üéØ Use brief, focused prompts\n- üîÑ Batch process to reduce overhead\n- üìâ Set `verbosity: \"low\"` for structured output\n\n---\n\n## üöÄ Real-World Applications\n\n**1. Automated Ticket Field Population**\n- Extract user info, issue details, urgency from emails\n- Populate ticketing system fields automatically\n- Save support agents 2-3 minutes per ticket\n\n**2. Knowledge Base Creation**\n- Extract structured data from support emails\n- Build searchable knowledge base\n- Identify common issues and solutions\n\n**3. Asset Tracking & Inventory**\n- Extract equipment details from reports\n- Maintain up-to-date inventory databases\n- Track hardware across locations\n\n**4. Meeting Notes to Action Items**\n- Extract tasks, owners, deadlines from meeting notes\n- Convert to structured task lists\n- Integrate with project management tools\n\n**5. Error Log Analysis**\n- Extract error codes, sources, timestamps from logs\n- Categorize and prioritize issues\n- Generate incident reports\n\n---\n\n## üîë Key Takeaways\n\n1. **LLMs excel at extraction** - They understand context and handle variations in natural language\n\n2. **Validation is critical** - Always validate before using data in production systems\n\n3. **Confidence scoring works** - Ask LLM to rate confidence and use thresholds\n\n4. **Save everything** - File generation enables downstream processing and auditing\n\n5. **Handle edge cases** - Real data is messy; build systems that gracefully handle incomplete/invalid input\n\n6. **Start simple, iterate** - Begin with basic extraction, add validation and error handling progressively\n\n7. **Cost-effective at scale** - Processing thousands of tickets costs just a few dollars with gpt-5-nano\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "w4r5l8elbtm",
   "source": "# üìù Student Exercises\n\nPractice what you've learned with these hands-on exercises!\n\n---\n\n## Exercise 1: Network Incident Report Extraction\n\n**Scenario:** Your network team sends incident reports via email. Extract structured data from these reports.\n\n**Mock Incident Report:**\n```\nNetwork Incident Report\nDate: January 15, 2025, 14:30\nLocation: Building C, Floor 3\nAffected Systems: File Server FS-301, Database Server DB-205\nIssue: Complete network outage affecting 45 users\nDuration: 2 hours 15 minutes\nCause: Failed network switch (SW-C3-12)\nResolution: Replaced faulty switch, restored connectivity\nReported by: Network Admin - Tom Chen\n```\n\n**Your Task:**\n1. Create an extraction prompt for incident data\n2. Extract to JSON with these fields:\n   - `incident_date`\n   - `incident_time`\n   - `location`\n   - `affected_systems` (array)\n   - `user_impact` (number of users)\n   - `duration`\n   - `root_cause`\n   - `resolution`\n   - `reported_by`\n3. Save to `/content/network_incident.json`\n4. Display the results\n\n**Bonus:** Add severity classification (Low/Medium/High/Critical) based on user impact",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "g9sjkhvveue",
   "source": "---\n\n## Exercise 2: Equipment Checkout System\n\n**Scenario:** Build a system to track equipment checkouts from inventory.\n\n**Mock Checkout Requests:**\n```\nRequest 1:\n\"Hi, I'm Jessica Lee from Marketing (EMP-4401). I need to borrow a MacBook Pro \nand a wireless presenter for a client presentation on Jan 20th. I'll return \nthem by Jan 22nd. My email is jlee@company.com\"\n\nRequest 2:\n\"Tom Wilson here, IT dept, employee 5623. Need the Nikon camera and tripod \nfor documenting the new server room setup. Checkout: tomorrow, return: 3 days later\"\n\nRequest 3:\n\"Can I get a laptop? Sarah from Sales. Need it for the trade show next week.\"\n```\n\n**Your Task:**\n1. Extract checkout information from each request\n2. Create JSON with fields:\n   - `requester_name`\n   - `employee_id`\n   - `department`\n   - `contact_email`\n   - `items_requested` (array of equipment)\n   - `checkout_date`\n   - `return_date`\n   - `purpose`\n3. Save each to CSV: `/content/equipment_checkout.csv`\n4. Include columns: requester, items, checkout_date, return_date, status\n5. Mark incomplete requests with status \"pending_info\"\n\n**Bonus:** Calculate number of days for each checkout and flag if > 7 days",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1c54lhnk114",
   "source": "---\n\n## Exercise 3: Extend the Mini-Project\n\n**Scenario:** Enhance the Support Ticket Intake System with additional features.\n\n**Your Tasks:**\n\n### Part A: Email Notification Generation\nAfter extracting ticket data, generate a formatted email notification.\n\n**Requirements:**\n1. Create a function `generate_ticket_email(ticket_data)`\n2. Email should include:\n   - Subject line with ticket ID and urgency\n   - Greeting with user name\n   - Ticket summary\n   - Expected response time based on urgency\n   - Contact information for follow-up\n3. Save email text to `/content/ticket_email_{ticket_id}.txt`\n\n**Example Output:**\n```\nSubject: [TKT-20250115-001] High Priority Ticket Created\n\nHello Alex Morgan,\n\nYour support ticket has been created:\n\nTicket ID: TKT-20250115-001\nIssue: Laptop overheating during video calls\nUrgency: High\nExpected Response: Within 4 hours\n\nOur team will contact you at alex.morgan@company.com or ext. 3344\n\nThank you,\nIT Support Team\n```\n\n### Part B: Severity Scoring\nAdd automatic severity scoring based on extracted information.\n\n**Requirements:**\n1. Create scoring function that assigns points:\n   - Urgency: Critical=10, High=7, Medium=4, Low=2\n   - Missing info: -2 points per missing field\n   - Keywords in issue: \"crash\"=+3, \"data loss\"=+5, \"slow\"=+1\n2. Calculate total severity score (0-20)\n3. Add to ticket data and save\n\n### Part C: Automatic Team Assignment\nRoute tickets to appropriate teams based on issue type.\n\n**Requirements:**\n1. Use LLM to classify issue category:\n   - Hardware, Software, Network, Security, Access Management\n2. Map categories to teams:\n   - Hardware ‚Üí Desktop Support Team\n   - Software ‚Üí Application Support Team\n   - Network ‚Üí Network Operations Team\n   - Security ‚Üí Security Team\n   - Access Management ‚Üí Identity Team\n3. Add `assigned_team` field to ticket data\n4. Update CSV to include team assignment\n\n**Bonus Challenge:** Combine all three parts into one enhanced `process_support_request_v2()` function!\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "xyk4hh31jx9",
   "source": "---\n\n# üéâ Course Complete!\n\n## üèÜ Congratulations!\n\nYou've completed the **Extracting Structured Information from Unstructured Text** tutorial!\n\n### ‚úÖ What You've Mastered:\n\n**Core Skills:**\n- üìä Extracting structured data from unstructured text\n- üîÑ Choosing the right format (CSV, JSON, Pydantic)\n- ‚úÖ Validating extracted data\n- üíæ Saving results to files\n- üîÑ Batch processing with progress tracking\n- ‚ö†Ô∏è Error handling and edge cases\n\n**Advanced Techniques:**\n- üéØ Confidence scoring and quality assessment\n- üîç Handling missing, contradictory, and ambiguous data\n- üìù Format standardization\n- üõ°Ô∏è Comprehensive validation pipelines\n- üèóÔ∏è Building production-ready extraction systems\n\n**Practical Experience:**\n- üé´ Support ticket extraction\n- üì¶ Equipment inventory management\n- üêõ Error log processing\n- üîÑ Batch processing workflows\n- üéØ Complete intake system implementation\n\n### üìö Files Created Throughout This Tutorial:\n\n**Iteration 1:**\n- `/content/inventory_data.csv`\n- `/content/ticket_data.json`\n- `/content/validated_ticket.json`\n- `/content/basic_ticket.json`\n- `/content/complex_ticket_nested.json`\n\n**Iteration 2:**\n- `/content/multiple_errors.json`\n- `/content/conference_room_inventory.csv`\n- `/content/ticket_log.json`\n\n**Iteration 3:**\n- `/content/batch_tickets.json`\n- `/content/batch_tickets.csv`\n- `/content/incomplete_ticket.json`\n- `/content/contradictory_ticket.json`\n- `/content/ambiguous_formats.json`\n- `/content/invalid_data_validation.json`\n- `/content/partial_extraction.json`\n- `/content/processed_tickets_log.json`\n- `/content/processed_tickets_summary.csv`\n\n### üöÄ Next Steps:\n\n1. **Practice:** Complete the student exercises\n2. **Experiment:** Try with your own data sources\n3. **Build:** Create extraction systems for your organization\n4. **Optimize:** Fine-tune prompts for your specific use cases\n5. **Scale:** Deploy to production with proper monitoring\n\n### üí° Remember:\n\n- Start simple and iterate\n- Always validate extracted data\n- Use confidence scores wisely\n- Handle edge cases gracefully\n- Monitor costs and performance\n\n**You're now equipped to build production-ready data extraction systems using LLMs!**\n\n---\n\n## üìñ Additional Resources:\n\n- **OpenAI Documentation**: https://platform.openai.com/docs\n- **Pydantic Docs**: https://docs.pydantic.dev\n- **Pandas Guide**: https://pandas.pydata.org/docs\n\n**Happy Extracting!** üéä\n\n---",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}