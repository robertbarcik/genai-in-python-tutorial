{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ RAG: Retrieval-Augmented Generation\n",
    "\n",
    "Welcome to this hands-on tutorial on **Retrieval-Augmented Generation (RAG)**!\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What is RAG?\n",
    "\n",
    "**RAG** is a technique that combines the power of Large Language Models (LLMs) with the ability to retrieve relevant information from external knowledge sources. Think of it as giving the AI access to \"notes\" or a \"textbook\" when answering questions.\n",
    "\n",
    "### üéØ The Problem\n",
    "\n",
    "LLMs like GPT are incredibly powerful, but they have limitations:\n",
    "- They only know what they were trained on (knowledge cutoff dates)\n",
    "- They have no access to your private/proprietary data\n",
    "- They can't know about recent events or updates\n",
    "- They may \"hallucinate\" (make up information) when uncertain\n",
    "\n",
    "### ‚ú® The Solution\n",
    "\n",
    "RAG solves these problems by:\n",
    "1. **Retrieving** relevant documents from your knowledge base\n",
    "2. **Augmenting** the prompt with this retrieved information\n",
    "3. **Generating** an accurate answer based on the provided context\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Real-World Applications\n",
    "\n",
    "- **Customer Support**: Answer questions using product documentation\n",
    "- **Internal Knowledge Bases**: Help employees find company information\n",
    "- **Document Q&A**: Extract insights from reports, contracts, or research papers\n",
    "- **Code Documentation**: Search through codebases and generate explanations\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "‚úÖ **Understand the three components of RAG** (Retrieval ‚Üí Augmentation ‚Üí Generation)\n",
    "\n",
    "‚úÖ **Learn about embeddings** and how they represent meaning numerically\n",
    "\n",
    "‚úÖ **Implement semantic search** to find relevant documents\n",
    "\n",
    "‚úÖ **Build a complete RAG pipeline** from scratch\n",
    "\n",
    "‚úÖ **Understand production considerations** (vector databases, chunking, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Theory: Why RAG Exists\n",
    "\n",
    "## üö´ The Problem: LLM Limitations\n",
    "\n",
    "Before we dive into RAG, let's understand WHY we need it.\n",
    "\n",
    "### Knowledge Cutoff Dates\n",
    "LLMs are trained on data up to a specific date. They don't know about:\n",
    "- Recent events or news\n",
    "- New products or companies launched after training\n",
    "- Updated policies or procedures\n",
    "\n",
    "### No Access to Private Data\n",
    "LLMs can't access:\n",
    "- Your company's internal documents\n",
    "- Proprietary customer information\n",
    "- Personal or confidential data\n",
    "- Real-time database contents\n",
    "\n",
    "### Hallucination Risks\n",
    "When uncertain, LLMs may:\n",
    "- Generate plausible-sounding but incorrect information\n",
    "- Mix facts from different sources incorrectly\n",
    "- Fill gaps with \"reasonable\" guesses\n",
    "\n",
    "üí° **Key Insight**: LLMs can only work with what they \"remember\" from training. They can't look things up!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ The Solution: RAG\n",
    "\n",
    "RAG gives LLMs the ability to \"look things up\" before answering. Here's how:\n",
    "\n",
    "### The Three Steps of RAG:\n",
    "\n",
    "1. **üîç Retrieval**\n",
    "   - Search your knowledge base for relevant documents\n",
    "   - Find the information that best matches the user's question\n",
    "   - Like finding the right page in a textbook\n",
    "\n",
    "2. **üìù Augmentation**\n",
    "   - Add the retrieved information to the prompt as \"context\"\n",
    "   - Tell the LLM: \"Here's the relevant information, use it to answer\"\n",
    "   - Like giving someone notes before asking them a question\n",
    "\n",
    "3. **üí¨ Generation**\n",
    "   - LLM generates an answer based on the provided context\n",
    "   - Answer is grounded in real information, not guesses\n",
    "   - Like a student answering from their notes instead of memory\n",
    "\n",
    "### üí° Key Point: Giving LLMs \"Notes\"\n",
    "\n",
    "RAG is like giving the LLM access to an open-book test. Instead of relying solely on what it \"remembers\" from training, it can reference your documents to provide accurate, up-to-date answers.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- LLMs have knowledge cutoffs and can't access private/recent information\n",
    "- RAG enables LLMs to \"look things up\" in your knowledge base\n",
    "- The three steps are: Retrieval ‚Üí Augmentation ‚Üí Generation\n",
    "- RAG dramatically reduces hallucinations by grounding answers in real data\n",
    "- RAG makes LLMs practical for domain-specific and private information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Setup\n",
    "\n",
    "Let's set up our environment and prepare to build our RAG system.\n",
    "\n",
    "## üì¶ What We'll Install\n",
    "\n",
    "- **openai**: Official OpenAI Python SDK for API access\n",
    "- **pandas**: Data manipulation (we'll store documents in a DataFrame)\n",
    "- **numpy**: Numerical operations (for vector similarity calculations)\n",
    "- **matplotlib**: Optional visualization tools\n",
    "\n",
    "## üîë API Configuration\n",
    "\n",
    "You'll need an OpenAI API key. You have two options:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai pandas numpy matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI models to use\n",
    "OPENAI_LLM_MODEL = \"gpt-5-nano\"  # For text generation\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-3-small\"  # For embeddings\n",
    "\n",
    "print(f\"ü§ñ LLM Model: {OPENAI_LLM_MODEL}\")\n",
    "print(f\"üî¢ Embedding Model: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized!\")\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Baseline Demonstration: The Problem\n",
    "\n",
    "## üß™ Let's See What Happens Without RAG\n",
    "\n",
    "Before we build our RAG system, let's demonstrate the problem. We'll ask the LLM about a specific startup company that:\n",
    "- Was founded recently\n",
    "- Is not widely known\n",
    "- The model likely hasn't seen in training data\n",
    "\n",
    "**Question**: \"What does the startup company Pentera do and who invested in it?\"\n",
    "\n",
    "Let's see what the baseline LLM says (without any context provided)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Baseline query: Ask about a specific company WITHOUT providing context\nbaseline_question = \"What does the startup company Pentera do and who invested in it?\"\n\nprint(\"üîç Testing Baseline (No Context)...\")\nprint(f\"‚ùì Question: {baseline_question}\\n\")\n\ntry:\n    # Call the LLM without any context about Pentera\n    baseline_response = client.responses.create(\n        model=OPENAI_LLM_MODEL,\n        input=baseline_question\n    )\n    \n    baseline_answer = baseline_response.output_text\n    \n    print(\"‚ùå BASELINE ANSWER (no context provided):\")\n    print(f\"   {baseline_answer}\")\n    print(\"\\n\" + \"=\"*70)\n    print(\"üí° OBSERVATION:\")\n    print(\"   The model either:\")\n    print(\"   - Says it doesn't have specific information\")\n    print(\"   - Provides vague/generic information\")\n    print(\"   - Or potentially makes up (hallucinates) details\")\n    print(\"\\n   This is WHY we need RAG!\")\n    print(\"=\"*70)\n    \nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What Should You Observe?\n",
    "\n",
    "The baseline LLM likely:\n",
    "- Admits it doesn't have specific information about Pentera\n",
    "- Provides only general/vague information\n",
    "- May suggest checking their website or other sources\n",
    "- Cannot provide specific investor details\n",
    "\n",
    "üí° **This is exactly why we need RAG!** We need to give the LLM access to specific information about companies like Pentera.\n",
    "\n",
    "In the following sections, we'll build a RAG system that can answer this question accurately by retrieving relevant information from our knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Step 1: Preparing Our Documents (Theory)\n",
    "\n",
    "## üìÑ What Are \"Documents\" in RAG?\n",
    "\n",
    "In RAG, a \"document\" is any piece of text that contains information:\n",
    "- Product descriptions\n",
    "- Company profiles\n",
    "- Support articles\n",
    "- Research papers\n",
    "- Code documentation\n",
    "- Meeting notes\n",
    "\n",
    "## üèóÔ∏è Document Structure Matters\n",
    "\n",
    "Good documents are:\n",
    "- **Self-contained**: Each document has complete information about one topic\n",
    "- **Well-structured**: Clear, organized, with key information highlighted\n",
    "- **Right-sized**: Not too long (loses focus) or too short (lacks context)\n",
    "- **Consistent**: Follow the same format for similar types of information\n",
    "\n",
    "## üìä Scale\n",
    "\n",
    "- **In this tutorial**: 10 sample startup companies (learning purposes)\n",
    "- **In production**: Could be thousands or millions of documents\n",
    "\n",
    "## üéØ Our Task\n",
    "\n",
    "We'll create a small knowledge base of 10 startup companies, each with:\n",
    "- Company name\n",
    "- Industry\n",
    "- Location\n",
    "- Description (what they do)\n",
    "- Investors\n",
    "- Founded year\n",
    "\n",
    "Then we'll convert this structured data into natural language \"documents\" that are easy to search.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- Documents are the knowledge base that RAG systems search through\n",
    "- Good document structure improves retrieval quality\n",
    "- Each document should be self-contained and focused\n",
    "- In production, you might have millions of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Step 1: Preparing Our Documents (Practice)\n",
    "\n",
    "Let's create our knowledge base of startup companies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock data: 10 startup companies\n",
    "# This simulates a knowledge base you might have in a real application\n",
    "\n",
    "companies_data = [\n",
    "    {\n",
    "        \"name\": \"Pentera\",\n",
    "        \"industry\": \"Cybersecurity\",\n",
    "        \"location\": \"Tel Aviv, Israel\",\n",
    "        \"description\": \"Pentera provides automated security validation platforms that help organizations continuously test their cybersecurity defenses. Their platform simulates real-world attacks to identify vulnerabilities before hackers can exploit them.\",\n",
    "        \"investors\": [\"K1 Investment Management\", \"Insight Partners\", \"Blackstone\"],\n",
    "        \"founded\": 2015\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Wiz\",\n",
    "        \"industry\": \"Cloud Security\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"Wiz is a cloud security platform that helps organizations identify and remove critical risks across their cloud infrastructure. They provide comprehensive visibility and threat detection for AWS, Azure, and Google Cloud.\",\n",
    "        \"investors\": [\"Sequoia Capital\", \"Greenoaks\", \"Salesforce Ventures\", \"Cyberstarts\"],\n",
    "        \"founded\": 2020\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ramp\",\n",
    "        \"industry\": \"FinTech\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"Ramp is a corporate card and spend management platform that helps companies save time and money. Their platform automates expense tracking, provides real-time insights, and identifies cost-saving opportunities.\",\n",
    "        \"investors\": [\"Founders Fund\", \"Stripe\", \"Goldman Sachs\", \"Thrive Capital\"],\n",
    "        \"founded\": 2019\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Notion\",\n",
    "        \"industry\": \"Productivity Software\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Notion is an all-in-one workspace that combines notes, tasks, wikis, and databases. Teams use Notion to collaborate, organize knowledge, and manage projects in one unified platform.\",\n",
    "        \"investors\": [\"Coatue\", \"Sequoia Capital\", \"Index Ventures\"],\n",
    "        \"founded\": 2016\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Anduril Industries\",\n",
    "        \"industry\": \"Defense Technology\",\n",
    "        \"location\": \"Costa Mesa, USA\",\n",
    "        \"description\": \"Anduril Industries builds advanced defense technology products including autonomous systems, sensors, and AI-powered solutions. Their technology is used for border security, base security, and military applications.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"Founders Fund\", \"8VC\", \"Valor Equity Partners\"],\n",
    "        \"founded\": 2017\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Databricks\",\n",
    "        \"industry\": \"Data Analytics\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Databricks provides a unified analytics platform built on Apache Spark. Their lakehouse platform combines data warehousing and data lakes, enabling companies to build data, analytics, and AI solutions at scale.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"NEA\", \"Coatue\", \"Tiger Global\"],\n",
    "        \"founded\": 2013\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Figma\",\n",
    "        \"industry\": \"Design Software\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Figma is a collaborative interface design tool that runs in the browser. Designers use Figma to create, prototype, and collaborate on user interfaces for web and mobile applications in real-time.\",\n",
    "        \"investors\": [\"Sequoia Capital\", \"Greylock Partners\", \"Kleiner Perkins\", \"Index Ventures\"],\n",
    "        \"founded\": 2012\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Plaid\",\n",
    "        \"industry\": \"FinTech Infrastructure\",\n",
    "        \"location\": \"San Francisco, USA\",\n",
    "        \"description\": \"Plaid provides financial services APIs that enable applications to connect with users' bank accounts. Their platform powers thousands of fintech apps including Venmo, Robinhood, and Chime.\",\n",
    "        \"investors\": [\"Andreessen Horowitz\", \"NEA\", \"Index Ventures\", \"Goldman Sachs\"],\n",
    "        \"founded\": 2013\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"UiPath\",\n",
    "        \"industry\": \"Robotic Process Automation\",\n",
    "        \"location\": \"New York, USA\",\n",
    "        \"description\": \"UiPath is a leading RPA platform that helps organizations automate repetitive business processes. Their software robots can handle tasks like data entry, document processing, and system integration.\",\n",
    "        \"investors\": [\"Accel\", \"CapitalG\", \"Sequoia Capital\", \"Tiger Global\"],\n",
    "        \"founded\": 2005\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snyk\",\n",
    "        \"industry\": \"Developer Security\",\n",
    "        \"location\": \"Boston, USA\",\n",
    "        \"description\": \"Snyk is a developer security platform that helps teams find and fix vulnerabilities in code, dependencies, containers, and infrastructure as code. Their tools integrate directly into developer workflows.\",\n",
    "        \"investors\": [\"Accel\", \"Coatue\", \"Tiger Global\", \"Boldstart Ventures\"],\n",
    "        \"founded\": 2015\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created data for {len(companies_data)} startup companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier handling\n",
    "# DataFrames are great for structured data manipulation\n",
    "\n",
    "df = pd.DataFrame(companies_data)\n",
    "\n",
    "print(f\"‚úÖ Created database of {len(df)} companies\")\n",
    "print(\"\\nüìä First few companies:\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_text(company: dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert a company dictionary into a readable text document.\n",
    "    This is what we'll create embeddings for and search through.\n",
    "    \n",
    "    Args:\n",
    "        company: Dictionary containing company information\n",
    "        \n",
    "    Returns:\n",
    "        A formatted text document describing the company\n",
    "    \"\"\"\n",
    "    # Join the list of investors into a readable string\n",
    "    investors_str = \", \".join(company[\"investors\"])\n",
    "    \n",
    "    # Create a natural language document\n",
    "    # Note: This structure makes it easy for semantic search to find relevant info\n",
    "    text = f\"\"\"{company['name']} is a {company['industry']} company headquartered in {company['location']}. {company['description']} The company was founded in {company['founded']}. Key investors include: {investors_str}.\"\"\"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply the function to all companies to create searchable documents\n",
    "df['document'] = df.apply(lambda row: create_document_text(row.to_dict()), axis=1)\n",
    "\n",
    "print(\"‚úÖ Created searchable documents for all companies\")\n",
    "print(\"\\nüìÑ Example document:\")\n",
    "print(\"=\"*70)\n",
    "print(df['document'].iloc[0])\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° What We Just Did\n",
    "\n",
    "We converted structured data (dictionaries) into natural language documents. This is important because:\n",
    "\n",
    "1. **Semantic Search Works on Text**: Embeddings understand natural language, not structured data\n",
    "2. **Context Matters**: Full sentences provide better context than isolated fields\n",
    "3. **LLM-Friendly**: These documents can be directly used as context in prompts\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- We created 10 startup company profiles as our knowledge base\n",
    "- Each company has structured information (name, industry, investors, etc.)\n",
    "- We converted this structured data into natural language documents\n",
    "- These documents will be embedded and searched in the next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Step 2: Creating Embeddings (Theory)\n",
    "\n",
    "## üî¢ What Are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations that capture the *meaning* of text. Think of them as coordinates in a \"meaning space.\"\n",
    "\n",
    "### üìç The Map Analogy\n",
    "\n",
    "Imagine a map where:\n",
    "- Each word or sentence is a point\n",
    "- Similar meanings are close together\n",
    "- Different meanings are far apart\n",
    "\n",
    "For example:\n",
    "- \"dog\" and \"puppy\" would be very close\n",
    "- \"dog\" and \"bicycle\" would be far apart\n",
    "- \"king\" - \"man\" + \"woman\" ‚âà \"queen\" (famous example!)\n",
    "\n",
    "### üéØ Key Properties\n",
    "\n",
    "1. **High-Dimensional**: Typically 1536 dimensions (text-embedding-3-small)\n",
    "2. **Semantic Similarity Preserved**: Similar meanings ‚Üí similar vectors\n",
    "3. **Mathematically Comparable**: Can calculate distance/similarity between vectors\n",
    "\n",
    "### üí° Why This Matters\n",
    "\n",
    "Embeddings let us:\n",
    "- Search by *meaning*, not just keywords\n",
    "- Find \"cybersecurity startup\" even when the text says \"security company\"\n",
    "- Match \"who invested?\" with text about \"key investors\"\n",
    "\n",
    "---\n",
    "\n",
    "## üîç How Semantic Search Works\n",
    "\n",
    "Here's the complete process:\n",
    "\n",
    "1. **Embed All Documents** (one-time setup)\n",
    "   - Convert each document to an embedding vector\n",
    "   - Store these vectors (in production: use a vector database)\n",
    "\n",
    "2. **Embed the Query** (at search time)\n",
    "   - Convert user's question to an embedding vector\n",
    "   - Use the same embedding model!\n",
    "\n",
    "3. **Calculate Similarity**\n",
    "   - Compare query embedding with all document embeddings\n",
    "   - Use cosine similarity or dot product\n",
    "\n",
    "4. **Retrieve Top Matches**\n",
    "   - Rank documents by similarity score\n",
    "   - Return the most relevant documents\n",
    "\n",
    "### üí° Key Insight: Semantic vs Keyword Search\n",
    "\n",
    "**Keyword Search** (traditional):\n",
    "- Finds exact word matches\n",
    "- \"car\" won't match \"automobile\"\n",
    "- Order and context don't matter much\n",
    "\n",
    "**Semantic Search** (embeddings):\n",
    "- Understands meaning\n",
    "- \"car\" and \"automobile\" are similar\n",
    "- \"bank\" (financial) ‚â† \"bank\" (river)\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Cost Consideration\n",
    "\n",
    "**OpenAI Embeddings Pricing**:\n",
    "- text-embedding-3-small: ~$0.02 per 1M tokens\n",
    "- For our 10 documents (~2000 tokens): Less than $0.001\n",
    "- Very cost-effective!\n",
    "\n",
    "**Why text-embedding-3-small?**\n",
    "- Good quality for most use cases\n",
    "- 6x cheaper than text-embedding-3-large\n",
    "- Fast processing\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- Embeddings convert text into numerical vectors that capture meaning\n",
    "- Similar meanings produce similar vectors (high similarity score)\n",
    "- Semantic search finds documents by meaning, not just keyword matching\n",
    "- Embeddings enable the \"Retrieval\" step in RAG\n",
    "- Very cost-effective compared to LLM calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Step 2: Creating Embeddings (Practice)\n",
    "\n",
    "Let's generate embeddings for all our company documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate an embedding vector for the given text using OpenAI's API.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to embed\n",
    "        \n",
    "    Returns:\n",
    "        A list of floats representing the embedding vector (1536 dimensions)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call OpenAI's embeddings API\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=OPENAI_EMBEDDING_MODEL\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vector from the response\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the function with a sample\n",
    "sample_text = \"This is a test sentence about cybersecurity\"\n",
    "sample_embedding = get_embedding(sample_text)\n",
    "\n",
    "if sample_embedding:\n",
    "    print(f\"‚úÖ Generated embedding with {len(sample_embedding)} dimensions\")\n",
    "    print(f\"\\nüìä First 10 values: {sample_embedding[:10]}\")\n",
    "    print(f\"\\nüí° Each document will become a vector like this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all company documents\n",
    "# This is the step that converts our text into searchable vectors\n",
    "\n",
    "print(\"üîÑ Generating embeddings for all documents...\")\n",
    "print(\"   (This may take a few seconds)\\n\")\n",
    "\n",
    "# Apply the get_embedding function to each document\n",
    "df['embedding'] = df['document'].apply(get_embedding)\n",
    "\n",
    "# Check for any failures\n",
    "failed_count = df['embedding'].isnull().sum()\n",
    "\n",
    "if failed_count > 0:\n",
    "    print(f\"‚ö†Ô∏è Warning: {failed_count} embeddings failed to generate\")\n",
    "else:\n",
    "    print(f\"‚úÖ Successfully generated embeddings for all {len(df)} documents!\")\n",
    "\n",
    "# Show some stats about the embeddings\n",
    "print(f\"\\nüìä Embedding Statistics:\")\n",
    "print(f\"   Dimensions: {len(df['embedding'].iloc[0])}\")\n",
    "print(f\"   Total embeddings: {len(df)}\")\n",
    "print(f\"\\nüìù First embedding (first 10 values):\")\n",
    "print(f\"   {df['embedding'].iloc[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° What We Just Did\n",
    "\n",
    "We converted all 10 company documents into numerical vectors (embeddings). Each document is now:\n",
    "- A 1536-dimensional vector\n",
    "- Representing the semantic meaning of the text\n",
    "- Ready to be compared with query embeddings\n",
    "\n",
    "## üóÑÔ∏è Production Note: Vector Databases\n",
    "\n",
    "In this tutorial, we're storing embeddings in a Pandas DataFrame (in memory). This works fine for learning, but in production:\n",
    "\n",
    "- **Don't do this**: Store millions of embeddings in memory\n",
    "- **Do this instead**: Use a vector database (Pinecone, Weaviate, Chroma, Qdrant)\n",
    "- **Why?**: Vector databases are optimized for fast similarity search at scale\n",
    "\n",
    "We'll cover vector databases in later notebooks!\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- We successfully embedded all 10 company documents\n",
    "- Each embedding is a 1536-dimensional vector\n",
    "- These embeddings capture the semantic meaning of each company's profile\n",
    "- We're now ready to implement semantic search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Step 3: Semantic Search (Theory)\n",
    "\n",
    "## üìè Measuring Similarity\n",
    "\n",
    "Now that we have embeddings for all documents, we need to measure how similar they are to a query.\n",
    "\n",
    "### üßÆ Cosine Similarity\n",
    "\n",
    "The most common way to measure vector similarity is **cosine similarity**:\n",
    "\n",
    "- Measures the angle between two vectors\n",
    "- Returns a value between -1 and 1\n",
    "- 1 = identical direction (very similar)\n",
    "- 0 = perpendicular (unrelated)\n",
    "- -1 = opposite direction (very different)\n",
    "\n",
    "### üí° Simple Explanation\n",
    "\n",
    "Imagine two arrows in space:\n",
    "- If they point in the same direction ‚Üí high similarity\n",
    "- If they point in different directions ‚Üí low similarity\n",
    "\n",
    "**Why this works**: Documents with similar meanings have embeddings that \"point\" in similar directions in the high-dimensional space.\n",
    "\n",
    "### üìê The Formula (Optional)\n",
    "\n",
    "```\n",
    "cosine_similarity(A, B) = (A ¬∑ B) / (||A|| * ||B||)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- A ¬∑ B = dot product\n",
    "- ||A|| = magnitude of vector A\n",
    "\n",
    "Don't worry if the math seems complex - the key insight is: **similar meanings ‚Üí high scores**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç The Retrieval Process\n",
    "\n",
    "Here's how we find relevant documents:\n",
    "\n",
    "### Step-by-Step:\n",
    "\n",
    "1. **User asks a question**: \"What does Pentera do?\"\n",
    "\n",
    "2. **Embed the question**: Convert it to a vector using the same embedding model\n",
    "\n",
    "3. **Calculate similarity**: Compare question vector with all document vectors\n",
    "   ```\n",
    "   Question: [0.2, 0.5, 0.1, ...]\n",
    "   \n",
    "   Doc 1 (Pentera): [0.3, 0.4, 0.2, ...] ‚Üí similarity: 0.92 ‚úÖ\n",
    "   Doc 2 (Wiz):     [0.3, 0.4, 0.1, ...] ‚Üí similarity: 0.87\n",
    "   Doc 3 (Ramp):    [0.1, 0.2, 0.8, ...] ‚Üí similarity: 0.45\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "4. **Rank by score**: Sort documents by similarity (highest first)\n",
    "\n",
    "5. **Return top-k**: Get the most relevant documents (typically top 1-5)\n",
    "\n",
    "### üí° Key Point: This Is \"Retrieval\" in RAG\n",
    "\n",
    "This semantic search process is the **\"Retrieval\"** component of RAG. We're retrieving the most relevant documents to provide as context to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- Cosine similarity measures how \"close\" two vectors are in meaning\n",
    "- Higher similarity score = more relevant document\n",
    "- The retrieval process: embed query ‚Üí calculate similarities ‚Üí rank ‚Üí return top-k\n",
    "- This is much more powerful than keyword search\n",
    "- Semantic search is the foundation of the RAG \"Retrieval\" step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Step 3: Semantic Search (Practice)\n",
    "\n",
    "Let's implement semantic search to find relevant documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: list, vec2: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    Returns a value between -1 and 1, where 1 means identical direction.\n",
    "    \n",
    "    Args:\n",
    "        vec1: First embedding vector\n",
    "        vec2: Second embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Similarity score (higher = more similar)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for mathematical operations\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # Calculate dot product (how much vectors point in same direction)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    \n",
    "    # Calculate magnitudes (length of each vector)\n",
    "    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    \n",
    "    # Cosine similarity = dot product / product of magnitudes\n",
    "    return dot_product / norm_product\n",
    "\n",
    "# Test with two sample embeddings\n",
    "test_vec1 = get_embedding(\"cybersecurity startup\")\n",
    "test_vec2 = get_embedding(\"security company\")\n",
    "test_vec3 = get_embedding(\"restaurant food delivery\")\n",
    "\n",
    "sim_similar = cosine_similarity(test_vec1, test_vec2)\n",
    "sim_different = cosine_similarity(test_vec1, test_vec3)\n",
    "\n",
    "print(\"üß™ Testing Cosine Similarity:\\n\")\n",
    "print(f\"   'cybersecurity startup' vs 'security company': {sim_similar:.4f} ‚úÖ\")\n",
    "print(f\"   'cybersecurity startup' vs 'restaurant food': {sim_different:.4f}\")\n",
    "print(f\"\\nüí° Notice: Similar concepts have higher scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_relevant_documents(query: str, documents_df: pd.DataFrame, top_k: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the top_k most relevant documents for a given query.\n",
    "    This is the core semantic search function.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query (user's question)\n",
    "        documents_df: DataFrame with documents and their embeddings\n",
    "        top_k: Number of top documents to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with top_k most relevant documents, sorted by similarity\n",
    "    \"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    \n",
    "    # Step 1: Generate embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        print(\"‚ùå Failed to generate query embedding\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Calculate similarity with all documents\n",
    "    # For each document embedding, calculate cosine similarity with query\n",
    "    documents_df['similarity'] = documents_df['embedding'].apply(\n",
    "        lambda doc_embedding: cosine_similarity(doc_embedding, query_embedding)\n",
    "    )\n",
    "    \n",
    "    # Step 3: Get top_k results (highest similarity scores)\n",
    "    results = documents_df.nlargest(top_k, 'similarity')\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(results)} relevant document(s)\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Semantic search function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search with our original question\n",
    "question = \"What does the startup company Pentera do and who invested in it?\"\n",
    "\n",
    "print(\"üöÄ Testing Semantic Search\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find most relevant document\n",
    "relevant_docs = find_most_relevant_documents(question, df, top_k=1)\n",
    "\n",
    "# Display results\n",
    "if relevant_docs is not None and len(relevant_docs) > 0:\n",
    "    print(\"üìÑ MOST RELEVANT DOCUMENT:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Company: {relevant_docs.iloc[0]['name']}\")\n",
    "    print(f\"Industry: {relevant_docs.iloc[0]['industry']}\")\n",
    "    print(f\"Similarity Score: {relevant_docs.iloc[0]['similarity']:.4f}\")\n",
    "    print(f\"\\nDocument Text:\")\n",
    "    print(\"-\"*70)\n",
    "    print(relevant_docs.iloc[0]['document'])\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüí° SUCCESS: We found the right document!\")\n",
    "    print(\"   This is the 'Retrieval' part of RAG working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Semantic Search Is Working!\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "1. We asked: \"What does Pentera do and who invested in it?\"\n",
    "2. The system converted our question to an embedding\n",
    "3. It compared our question with all 10 company documents\n",
    "4. It found that the Pentera document was most similar\n",
    "5. It returned that document with a high similarity score\n",
    "\n",
    "### üìä Understanding Similarity Scores\n",
    "\n",
    "- **0.9 - 1.0**: Extremely relevant (nearly identical meaning)\n",
    "- **0.7 - 0.9**: Very relevant (strong semantic match)\n",
    "- **0.5 - 0.7**: Somewhat relevant (partial match)\n",
    "- **< 0.5**: Not very relevant (weak or no match)\n",
    "\n",
    "### üí° Why This Is Powerful\n",
    "\n",
    "Notice that:\n",
    "- We asked about \"what does Pentera do\"\n",
    "- The document says \"provides automated security validation\"\n",
    "- No exact keyword match for \"do\", but semantic search understood the intent!\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- Semantic search successfully found the most relevant document\n",
    "- The similarity score indicates confidence in the match\n",
    "- No keyword matching needed - search understands meaning\n",
    "- This is the \"Retrieval\" component of RAG in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Step 4: Augmented Generation (Theory)\n",
    "\n",
    "## üìù Prompt Engineering for RAG\n",
    "\n",
    "Now we have the relevant document. How do we use it? We need to carefully structure our prompt to:\n",
    "\n",
    "### 1. Provide Context Explicitly\n",
    "```\n",
    "Context:\n",
    "Pentera is a cybersecurity company...\n",
    "```\n",
    "\n",
    "### 2. Give Clear Instructions\n",
    "```\n",
    "Answer the question using ONLY the context provided.\n",
    "```\n",
    "\n",
    "### 3. Tell Model to Stay Grounded\n",
    "```\n",
    "If you cannot answer from the context, say so.\n",
    "```\n",
    "\n",
    "### 4. Ask the Question\n",
    "```\n",
    "Question: What does Pentera do?\n",
    "```\n",
    "\n",
    "## üéØ The RAG Prompt Structure\n",
    "\n",
    "A typical RAG prompt looks like:\n",
    "\n",
    "```\n",
    "Answer the question below using ONLY the context provided.\n",
    "If you cannot answer from the context, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "[Retrieved document(s) here]\n",
    "\n",
    "Question: [User's question here]\n",
    "\n",
    "Answer:\n",
    "```\n",
    "\n",
    "### üí° Key Insight: \"Augmenting\" the Prompt\n",
    "\n",
    "This is the **\"Augmentation\"** in RAG. We're augmenting (enriching) the prompt with retrieved information. The LLM now has:\n",
    "- The user's question\n",
    "- Relevant context to answer it\n",
    "- Clear instructions on how to use the context\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why This Works\n",
    "\n",
    "### Without RAG (Baseline):\n",
    "```\n",
    "LLM receives: \"What does Pentera do?\"\n",
    "LLM thinks: \"I don't have specific information about this company\"\n",
    "LLM responds: \"I don't have current information...\"\n",
    "```\n",
    "\n",
    "### With RAG:\n",
    "```\n",
    "LLM receives: \"Here's info about Pentera: [full context]. Now answer: What does Pentera do?\"\n",
    "LLM thinks: \"I can see exactly what Pentera does in the context\"\n",
    "LLM responds: \"Pentera provides automated security validation platforms...\"\n",
    "```\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. **Reduces Hallucination**: Model has facts to work with\n",
    "2. **Provides Source Attribution**: We know where the answer came from\n",
    "3. **Up-to-date Information**: Documents can be updated without retraining\n",
    "4. **Domain-Specific**: Works with private/proprietary information\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- RAG prompts provide context explicitly before asking the question\n",
    "- Clear instructions tell the model to use only the provided context\n",
    "- This is the \"Augmentation\" step - enriching the prompt with retrieved info\n",
    "- Augmented prompts dramatically reduce hallucination\n",
    "- The model can now answer accurately about specific, private, or recent information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# 7Ô∏è‚É£ Step 4: Augmented Generation (Practice)\n\nLet's implement the generation step with retrieved context!\n\n## üìù About the Prompt Structure\n\nThe prompt we'll construct has three key parts:\n\n1. **Instructions**: \"Answer using ONLY the context provided\"\n2. **Context**: The retrieved document(s) with relevant information\n3. **Question**: The user's original query\n\nThis structure ensures the LLM stays grounded in the retrieved facts and doesn't hallucinate information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_answer_with_rag(query: str, context: str) -> str:\n    \"\"\"\n    Generate an answer using the retrieved context.\n    This is the 'Generation' step in RAG.\n    \n    Args:\n        query: The user's question\n        context: The retrieved document(s) to use as context\n        \n    Returns:\n        The generated answer based on the context\n    \"\"\"\n    # Construct the prompt with context and instructions\n    # This is the \"Augmentation\" - we're adding retrieved context to guide the LLM\n    prompt = f\"\"\"Answer the question below using ONLY the context provided. \nIf you cannot answer the question from the context, say \"I don't have enough information in the provided context to answer that question.\"\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n    \n    try:\n        # Call the LLM with the augmented prompt using the Responses API\n        response = client.responses.create(\n            model=OPENAI_LLM_MODEL,\n            input=prompt\n        )\n        \n        return response.output_text\n    \n    except Exception as e:\n        return f\"‚ùå Error generating answer: {e}\"\n\nprint(\"‚úÖ RAG answer generation function created!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query: str, documents_df: pd.DataFrame, top_k: int = 1) -> dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieval + Augmented Generation.\n",
    "    \n",
    "    This function combines all steps:\n",
    "    1. Retrieve relevant documents (semantic search)\n",
    "    2. Extract context from retrieved documents\n",
    "    3. Generate answer using context\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        documents_df: DataFrame with documents and embeddings\n",
    "        top_k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with query, retrieved docs, and answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    relevant_docs = find_most_relevant_documents(query, documents_df, top_k)\n",
    "    \n",
    "    if relevant_docs is None or len(relevant_docs) == 0:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": None,\n",
    "            \"answer\": \"Failed to retrieve documents\"\n",
    "        }\n",
    "    \n",
    "    # Step 2: Extract context from retrieved documents\n",
    "    # Join multiple documents with double newlines for clarity\n",
    "    context = \"\\n\\n\".join(relevant_docs['document'].tolist())\n",
    "    \n",
    "    # Step 3: Generate answer with context\n",
    "    answer = generate_answer_with_rag(query, context)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": relevant_docs,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Complete RAG pipeline function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete RAG pipeline\n",
    "question = \"What does the startup company Pentera do and who invested in it?\"\n",
    "\n",
    "print(\"üöÄ Running Complete RAG Pipeline...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = rag_pipeline(question, df, top_k=1)\n",
    "\n",
    "print(f\"\\n‚ùì QUESTION:\")\n",
    "print(f\"   {result['query']}\\n\")\n",
    "\n",
    "print(f\"üìÑ RETRIEVED DOCUMENT:\")\n",
    "print(f\"   Company: {result['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"   Similarity: {result['retrieved_docs'].iloc[0]['similarity']:.4f}\\n\")\n",
    "\n",
    "print(f\"‚úÖ RAG ANSWER:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüéâ SUCCESS! RAG pipeline is working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Complete RAG Pipeline Is Working!\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "We successfully implemented the complete RAG pipeline:\n",
    "\n",
    "1. **üîç Retrieval**: Found the most relevant document (Pentera profile)\n",
    "2. **üìù Augmentation**: Added that document as context to our prompt\n",
    "3. **üí¨ Generation**: LLM generated an accurate answer using the context\n",
    "\n",
    "### üí° Key Observations\n",
    "\n",
    "- The answer is **specific and accurate** (mentions Pentera's exact services)\n",
    "- It includes **investor information** (K1, Insight Partners, Blackstone)\n",
    "- The LLM **didn't make anything up** - it used only the provided context\n",
    "- We have **source attribution** - we know exactly where the answer came from\n",
    "\n",
    "Compare this to the baseline answer we saw earlier - the improvement is dramatic!\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- The complete RAG pipeline combines Retrieval ‚Üí Augmentation ‚Üí Generation\n",
    "- Retrieved context enables accurate, specific answers\n",
    "- RAG works even for niche information the model wasn't trained on\n",
    "- We can now answer questions about private/proprietary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Comparison: Before vs After RAG\n",
    "\n",
    "Let's do a direct side-by-side comparison to see the power of RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_baseline_vs_rag(query: str, documents_df: pd.DataFrame):\n    \"\"\"\n    Compare baseline LLM response vs RAG-enhanced response.\n    This demonstrates the dramatic improvement RAG provides.\n    \n    Args:\n        query: The question to test\n        documents_df: DataFrame with documents and embeddings\n    \"\"\"\n    print(\"üî¨ COMPARISON: Baseline vs RAG\")\n    print(\"=\"*80)\n    print(f\"\\n‚ùì QUESTION: {query}\\n\")\n    \n    # Get baseline answer (no context)\n    print(\"‚è≥ Getting baseline answer (no context)...\")\n    try:\n        baseline_response = client.responses.create(\n            model=OPENAI_LLM_MODEL,\n            input=query\n        )\n        baseline_answer = baseline_response.output_text\n    except Exception as e:\n        baseline_answer = f\"Error: {e}\"\n    \n    # Get RAG answer (with context)\n    print(\"‚è≥ Getting RAG answer (with retrieved context)...\\n\")\n    rag_result = rag_pipeline(query, documents_df, top_k=1)\n    rag_answer = rag_result['answer']\n    \n    # Display comparison\n    print(\"=\"*80)\n    print(\"\\n‚ùå BASELINE ANSWER (no context):\")\n    print(\"-\"*80)\n    print(baseline_answer)\n    print(\"-\"*80)\n    \n    print(\"\\n‚úÖ RAG ANSWER (with retrieved context):\")\n    print(\"-\"*80)\n    print(rag_answer)\n    print(\"-\"*80)\n    \n    print(\"\\nüí° NOTICE THE DIFFERENCE:\")\n    print(\"   - Baseline: Vague, admits lack of knowledge, or provides generic info\")\n    print(\"   - RAG: Specific, accurate answer with concrete details\")\n    print(\"   - RAG includes exact investor names, company details, etc.\")\n    print(\"\\nüéâ This is the power of RAG!\")\n    print(\"=\"*80)\n\n# Run the comparison\ncompare_baseline_vs_rag(question, df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Observations\n",
    "\n",
    "### Baseline (Without RAG):\n",
    "- ‚ùå Admits it doesn't have specific information\n",
    "- ‚ùå Can't provide investor details\n",
    "- ‚ùå May provide generic or outdated information\n",
    "- ‚ùå Suggests checking external sources\n",
    "\n",
    "### RAG (With Retrieved Context):\n",
    "- ‚úÖ Provides specific, accurate information\n",
    "- ‚úÖ Lists exact investors by name\n",
    "- ‚úÖ Describes what the company actually does\n",
    "- ‚úÖ Grounded in the retrieved document\n",
    "\n",
    "### üí° The Transformation\n",
    "\n",
    "RAG transforms the LLM from:\n",
    "- \"I don't know\" ‚Üí \"Here's exactly what you need to know\"\n",
    "- Generic responses ‚Üí Specific, accurate answers\n",
    "- Uncertainty ‚Üí Confidence (backed by retrieved data)\n",
    "\n",
    "This is why RAG is so powerful for real-world applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Optional Experiment: Top-K Retrieval\n",
    "\n",
    "## üß™ What If We Retrieve Multiple Documents?\n",
    "\n",
    "So far, we've been retrieving just the top 1 most relevant document. But what if we retrieve multiple documents?\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "**More Documents (higher top_k):**\n",
    "- ‚úÖ More complete information\n",
    "- ‚úÖ Better coverage if information is split across documents\n",
    "- ‚ùå More tokens = higher cost\n",
    "- ‚ùå Irrelevant context can confuse the model\n",
    "- ‚ùå Longer processing time\n",
    "\n",
    "**Fewer Documents (lower top_k):**\n",
    "- ‚úÖ Lower cost\n",
    "- ‚úÖ Faster\n",
    "- ‚úÖ More focused context\n",
    "- ‚ùå Might miss relevant information\n",
    "\n",
    "Let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a question that might benefit from multiple documents\n",
    "experiment_question = \"Which companies are in the cybersecurity industry?\"\n",
    "\n",
    "print(\"üß™ EXPERIMENT: Comparing top-1 vs top-3 retrieval\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ùì Question: {experiment_question}\\n\")\n",
    "\n",
    "# Test with top-1\n",
    "print(\"üìä Test 1: Retrieving top-1 document\")\n",
    "print(\"-\"*70)\n",
    "result_top1 = rag_pipeline(experiment_question, df, top_k=1)\n",
    "\n",
    "print(f\"Retrieved: {result_top1['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"\\nAnswer: {result_top1['answer']}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Test with top-3\n",
    "print(\"\\nüìä Test 2: Retrieving top-3 documents\")\n",
    "print(\"-\"*70)\n",
    "result_top3 = rag_pipeline(experiment_question, df, top_k=3)\n",
    "\n",
    "print(f\"Retrieved:\")\n",
    "for idx, row in result_top3['retrieved_docs'].iterrows():\n",
    "    print(f\"  {idx+1}. {row['name']} (similarity: {row['similarity']:.4f})\")\n",
    "\n",
    "print(f\"\\nAnswer: {result_top3['answer']}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   - Top-1: May only mention one company\")\n",
    "print(\"   - Top-3: Can mention multiple companies if they're all in the context\")\n",
    "print(\"   - Trade-off: More complete vs more costly\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Analysis\n",
    "\n",
    "### When to Use top_k = 1:\n",
    "- Questions about a specific entity (\"Tell me about Pentera\")\n",
    "- When you need focused, specific information\n",
    "- Cost/speed is a priority\n",
    "- Your documents are comprehensive (all info in one place)\n",
    "\n",
    "### When to Use top_k > 1:\n",
    "- Comparative questions (\"Which companies do X?\")\n",
    "- Information might be split across documents\n",
    "- Need comprehensive coverage\n",
    "- Accuracy is more important than cost\n",
    "\n",
    "### üí° Key Insight\n",
    "\n",
    "**top_k is a hyperparameter you tune based on your use case!**\n",
    "\n",
    "In production, you might:\n",
    "- Start with top_k = 3-5\n",
    "- Test with your specific questions\n",
    "- Measure quality vs cost\n",
    "- Adjust based on results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîü Production Considerations\n",
    "\n",
    "## üì¶ Vector Databases in Production\n",
    "\n",
    "In this notebook, we stored embeddings in a Pandas DataFrame. This works great for learning with 10 documents, but NOT for production:\n",
    "\n",
    "### ‚ùå Problems with Our Approach:\n",
    "\n",
    "1. **Slow**: Must compare query to ALL documents every time\n",
    "   - 10 documents: Fast\n",
    "   - 1 million documents: Extremely slow\n",
    "\n",
    "2. **Limited Scale**: Can't handle millions of documents\n",
    "   - Everything stored in memory\n",
    "   - No optimization for large-scale search\n",
    "\n",
    "3. **No Persistence**: Data lost when notebook closes\n",
    "   - Must regenerate embeddings every time\n",
    "   - No way to update documents incrementally\n",
    "\n",
    "4. **No Advanced Features**:\n",
    "   - Can't filter by metadata (\"cybersecurity companies only\")\n",
    "   - No hybrid search (semantic + keyword)\n",
    "   - No approximate nearest neighbor (ANN) algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Production Solution: Vector Databases\n",
    "\n",
    "Vector databases are specialized systems optimized for similarity search:\n",
    "\n",
    "### Popular Vector Databases:\n",
    "\n",
    "- **Pinecone**: Fully managed, cloud-based, easy to use\n",
    "- **Weaviate**: Open-source, supports hybrid search\n",
    "- **Chroma**: Simple, lightweight, great for prototyping\n",
    "- **Qdrant**: Fast, Rust-based, good for production\n",
    "- **Milvus**: Open-source, scalable, enterprise-ready\n",
    "\n",
    "### üöÄ What Vector Databases Provide:\n",
    "\n",
    "1. **Fast Search**: \n",
    "   - Approximate Nearest Neighbor (ANN) algorithms\n",
    "   - HNSW, IVF, etc.\n",
    "   - Search millions of vectors in milliseconds\n",
    "\n",
    "2. **Scalability**: \n",
    "   - Handle billions of vectors\n",
    "   - Distributed storage\n",
    "   - Horizontal scaling\n",
    "\n",
    "3. **Persistence**: \n",
    "   - Store embeddings permanently\n",
    "   - Add/update/delete documents\n",
    "   - No need to regenerate\n",
    "\n",
    "4. **Advanced Features**:\n",
    "   - Metadata filtering\n",
    "   - Hybrid search (semantic + keyword)\n",
    "   - Multi-vector search\n",
    "   - Analytics and monitoring\n",
    "\n",
    "### üí° Coming Soon!\n",
    "\n",
    "In the next notebook, we'll learn to use **Chroma** for real-world RAG with:\n",
    "- Persistent vector storage\n",
    "- Fast similarity search\n",
    "- Metadata filtering\n",
    "- And more!\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Other Production Considerations\n",
    "\n",
    "### 1. Document Chunking\n",
    "\n",
    "**Problem**: Long documents don't fit in context windows\n",
    "\n",
    "**Solution**: Break documents into smaller chunks\n",
    "- Chunk size: 200-500 words typical\n",
    "- Overlap: 10-20% to maintain context\n",
    "- Methods: Sentence-based, semantic chunking, fixed-size\n",
    "\n",
    "### 2. Metadata Filtering\n",
    "\n",
    "**Example**: \"Find cybersecurity companies in the USA\"\n",
    "- Pre-filter by country = USA\n",
    "- Then semantic search within filtered set\n",
    "- Faster and more accurate\n",
    "\n",
    "### 3. Hybrid Search\n",
    "\n",
    "Combine semantic + keyword search:\n",
    "- Semantic: Understands meaning\n",
    "- Keyword: Exact matches (names, IDs, codes)\n",
    "- Best of both worlds!\n",
    "\n",
    "### 4. Re-ranking\n",
    "\n",
    "Two-stage retrieval:\n",
    "1. Fast retrieval: Get top 50 documents (fast, approximate)\n",
    "2. Re-ranking: Use more sophisticated model on top 50\n",
    "3. Return top 5 after re-ranking\n",
    "\n",
    "### 5. Evaluation\n",
    "\n",
    "How do you know if your RAG system is working well?\n",
    "\n",
    "**Retrieval Metrics**:\n",
    "- Precision@k: How many retrieved docs are relevant?\n",
    "- Recall@k: Did we retrieve all relevant docs?\n",
    "- MRR (Mean Reciprocal Rank): Where do relevant docs appear?\n",
    "\n",
    "**Generation Metrics**:\n",
    "- Faithfulness: Is answer grounded in context?\n",
    "- Relevance: Does answer address the question?\n",
    "- Human evaluation: Still the gold standard!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "- In-memory storage works for learning but not production\n",
    "- Vector databases are essential for real-world RAG systems\n",
    "- Document chunking is crucial for long documents\n",
    "- Hybrid search combines semantic understanding with keyword precision\n",
    "- Evaluation is critical to ensure quality\n",
    "- RAG is a system - each component can be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# 1Ô∏è‚É£1Ô∏è‚É£ Best Practices & Common Mistakes\n\n## üí° Best Practices for RAG\n\n### 1. Document Quality\n‚úÖ **Do**:\n- Clean, well-structured documents\n- Remove irrelevant content (headers, footers, boilerplate)\n- Consistent formatting\n- Clear, descriptive text\n\n‚ùå **Don't**:\n- Include lots of HTML tags, formatting codes\n- Have inconsistent document structures\n- Mix multiple topics in one document\n\n### 2. Prompt Engineering\n‚úÖ **Do**:\n- Give clear instructions (\"Answer using ONLY the context\")\n- Define behavior for missing information\n- Use system messages effectively\n- Note: gpt-5-nano only supports default temperature (1)\n\n‚ùå **Don't**:\n- Use vague instructions\n- Let model make things up when unsure\n- Specify unsupported parameters like custom temperature values\n\n### 3. Error Handling\n‚úÖ **Do**:\n- Handle API failures gracefully\n- Check for None/empty results\n- Validate embedding generation\n- Log errors for debugging\n\n‚ùå **Don't**:\n- Assume APIs always succeed\n- Return raw error messages to users\n- Skip validation checks\n\n### 4. Context Boundaries\n‚úÖ **Do**:\n- Tell model what to do if context is insufficient\n- Monitor token usage\n- Handle context window limits\n\n‚ùå **Don't**:\n- Exceed context limits silently\n- Force model to answer without enough info\n\n### 5. Validation\n‚úÖ **Do**:\n- Check similarity scores (low score = poor match)\n- Set thresholds (e.g., reject if similarity < 0.7)\n- Test with various question types\n- Monitor quality over time\n\n‚ùå **Don't**:\n- Return results with very low similarity scores\n- Assume all retrievals are good\n- Skip testing edge cases\n\n### 6. Cost Awareness\n‚úÖ **Do**:\n- Cache embeddings (don't regenerate)\n- Use appropriate models (text-embedding-3-small)\n- Monitor API usage\n- Batch embedding generation when possible\n\n‚ùå **Don't**:\n- Regenerate embeddings unnecessarily\n- Use expensive models when cheaper ones work\n- Ignore token costs\n\n---\n\n## ‚ö†Ô∏è Common Mistakes to Avoid\n\n### 1. Vague Questions\n‚ùå **Bad**: \"What is this about?\"\n- Too general, hard to find relevant documents\n- Unclear what information is needed\n\n‚úÖ **Good**: \"What does Pentera do and who are their main investors?\"\n- Specific, clear information request\n- Easy to retrieve relevant documents\n\n### 2. Not Handling No Results\n‚ùå **Bad**: Assume there's always a good match\n```python\nanswer = rag_pipeline(query, df)\nreturn answer  # What if similarity is 0.2?\n```\n\n‚úÖ **Good**: Check similarity and handle low scores\n```python\nif result['similarity'] < 0.7:\n    return \"I don't have relevant information to answer that.\"\n```\n\n### 3. Ignoring Context Limits\n‚ùå **Bad**: Stuff 50 documents into context\n- May exceed token limits\n- Confuses the model\n- Expensive\n\n‚úÖ **Good**: Use appropriate top_k (1-5 typically)\n- Focused, relevant context\n- Within token limits\n- Cost-effective\n\n### 4. Poor Document Structure\n‚ùå **Bad**: \"Everything in one huge document\"\n- Hard to retrieve specific information\n- May exceed context limits\n- Poor retrieval quality\n\n‚úÖ **Good**: One focused document per topic/entity\n- Easy to retrieve relevant info\n- Better semantic search\n- Fits in context window\n\n### 5. No Fallback Strategy\n‚ùå **Bad**: Crash when API fails\n```python\nembedding = get_embedding(text)  # What if this fails?\n```\n\n‚úÖ **Good**: Handle failures gracefully\n```python\nembedding = get_embedding(text)\nif embedding is None:\n    return \"Service temporarily unavailable\"\n```\n\n### 6. Assuming Perfect Retrieval\n‚ùå **Bad**: Trust all retrieved documents blindly\n\n‚úÖ **Good**: \n- Verify similarity scores\n- Test with diverse questions\n- Have humans review outputs\n- Iterate and improve\n\n---\n\n## üéØ Key Takeaways\n\n- Document quality and structure directly impact retrieval quality\n- Always validate similarity scores before using retrieved docs\n- Error handling is crucial for production systems\n- Clear prompts and instructions reduce hallucination\n- Monitor costs and optimize model choices\n- Test thoroughly with diverse questions and edge cases\n- Remember: gpt-5-nano has model-specific constraints (e.g., default temperature only)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Try It Yourself! üéÆ\n",
    "\n",
    "Now it's your turn to experiment! Use the playground below to:\n",
    "- Try different questions\n",
    "- Experiment with top_k values\n",
    "- Test questions that shouldn't be answerable from the data\n",
    "- See how the system handles edge cases\n",
    "\n",
    "## üí° Suggested Experiments:\n",
    "\n",
    "1. **Specific company questions**:\n",
    "   - \"What does Wiz do?\"\n",
    "   - \"Who invested in Notion?\"\n",
    "   - \"When was Databricks founded?\"\n",
    "\n",
    "2. **Comparative questions**:\n",
    "   - \"Which companies are in the cybersecurity industry?\"\n",
    "   - \"What fintech companies are in the database?\"\n",
    "   - \"Compare Pentera and Wiz\"\n",
    "\n",
    "3. **Questions that SHOULD fail** (not in our data):\n",
    "   - \"What does Apple do?\"\n",
    "   - \"Tell me about restaurants in New York\"\n",
    "   - \"What's the weather today?\"\n",
    "\n",
    "4. **Different top_k values**:\n",
    "   - Try top_k=1, top_k=3, top_k=5\n",
    "   - See how answers change\n",
    "\n",
    "Have fun experimenting! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéÆ PLAYGROUND: Try your own questions!\n",
    "\n",
    "# Modify these variables:\n",
    "your_question = \"Which companies are in the cybersecurity industry?\"  # ‚Üê Change this!\n",
    "your_top_k = 2  # ‚Üê Try different values (1, 2, 3, 5)\n",
    "\n",
    "# Run the RAG pipeline\n",
    "result = rag_pipeline(your_question, df, top_k=your_top_k)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(f\"‚ùì YOUR QUESTION:\")\n",
    "print(f\"   {result['query']}\\n\")\n",
    "\n",
    "print(f\"üìÑ RETRIEVED DOCUMENTS (top-{your_top_k}):\")\n",
    "for idx, row in result['retrieved_docs'].iterrows():\n",
    "    print(f\"   {idx+1}. {row['name']} (similarity: {row['similarity']:.4f})\")\n",
    "\n",
    "print(f\"\\n‚úÖ ANSWER:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ ADVANCED: Test a question that SHOULDN'T be answerable\n",
    "# This tests how well the model handles missing information\n",
    "\n",
    "off_topic_question = \"What does Apple Inc do and who is their CEO?\"\n",
    "\n",
    "print(\"üß™ Testing off-topic question (not in our database):\\n\")\n",
    "result = rag_pipeline(off_topic_question, df, top_k=1)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"‚ùì Question: {off_topic_question}\\n\")\n",
    "print(f\"üìÑ Top Retrieved Document: {result['retrieved_docs'].iloc[0]['name']}\")\n",
    "print(f\"   Similarity Score: {result['retrieved_docs'].iloc[0]['similarity']:.4f}\")\n",
    "print(f\"\\n‚úÖ Answer:\")\n",
    "print(\"-\"*70)\n",
    "print(result['answer'])\n",
    "print(\"-\"*70)\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   - Low similarity score indicates poor match\")\n",
    "print(\"   - The answer should indicate insufficient information\")\n",
    "print(\"   - In production, you might reject queries with similarity < 0.7\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Summary & Next Steps\n",
    "\n",
    "## üéì What You Learned Today\n",
    "\n",
    "Congratulations! You've successfully learned and implemented RAG from scratch. Here's what you accomplished:\n",
    "\n",
    "### ‚úÖ Core Concepts:\n",
    "- **The three components of RAG**: Retrieval ‚Üí Augmentation ‚Üí Generation\n",
    "- **Why RAG exists**: Solving LLM limitations (knowledge cutoffs, no private data access)\n",
    "- **How embeddings work**: Numerical representations that capture meaning\n",
    "- **Semantic search**: Finding documents by meaning, not just keywords\n",
    "- **Prompt augmentation**: Enriching prompts with retrieved context\n",
    "\n",
    "### ‚úÖ Technical Skills:\n",
    "- Created a knowledge base of documents\n",
    "- Generated embeddings using OpenAI's API\n",
    "- Implemented cosine similarity for vector comparison\n",
    "- Built a semantic search function\n",
    "- Created a complete RAG pipeline\n",
    "- Compared baseline vs RAG performance\n",
    "- Experimented with different retrieval strategies (top-k)\n",
    "\n",
    "### ‚úÖ Production Awareness:\n",
    "- Understood when to use vector databases\n",
    "- Learned about chunking, hybrid search, and re-ranking\n",
    "- Discovered best practices and common mistakes\n",
    "- Learned how to evaluate RAG systems\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps in the Course\n",
    "\n",
    "### Immediate Next Steps:\n",
    "\n",
    "**Next Notebook**: Vector Databases with Chroma\n",
    "- Persistent storage for embeddings\n",
    "- Faster search with optimized algorithms\n",
    "- Metadata filtering\n",
    "- Production-ready RAG\n",
    "\n",
    "### Future Topics:\n",
    "\n",
    "1. **Advanced Chunking Strategies**\n",
    "   - Semantic chunking\n",
    "   - Overlap strategies\n",
    "   - Document hierarchies\n",
    "\n",
    "2. **Hybrid Search**\n",
    "   - Combining semantic + keyword search\n",
    "   - BM25 algorithm\n",
    "   - Fusion strategies\n",
    "\n",
    "3. **RAG Evaluation**\n",
    "   - Retrieval metrics (Precision, Recall, MRR)\n",
    "   - Generation metrics (Faithfulness, Relevance)\n",
    "   - Building test sets\n",
    "\n",
    "4. **Multi-Document Reasoning**\n",
    "   - Synthesizing information across documents\n",
    "   - Citation and source attribution\n",
    "   - Handling conflicting information\n",
    "\n",
    "5. **Production Optimization**\n",
    "   - Caching strategies\n",
    "   - Batch processing\n",
    "   - Cost optimization\n",
    "   - Monitoring and logging\n",
    "\n",
    "---\n",
    "\n",
    "## üí™ Practice Exercises\n",
    "\n",
    "To reinforce your learning, try these exercises:\n",
    "\n",
    "### Exercise 1: Different Domain\n",
    "Create a RAG system for a different domain:\n",
    "- Product catalog (electronics, clothing, etc.)\n",
    "- Movie database (titles, actors, plots)\n",
    "- Restaurant information (cuisine, location, reviews)\n",
    "\n",
    "### Exercise 2: Optimization Challenge\n",
    "Improve the system:\n",
    "- Add similarity score thresholds\n",
    "- Implement error messages for low-confidence results\n",
    "- Create a function to track costs\n",
    "- Add logging for debugging\n",
    "\n",
    "### Exercise 3: Evaluation\n",
    "Test systematically:\n",
    "- Create 10 test questions with expected answers\n",
    "- Run RAG pipeline on all questions\n",
    "- Compare answers to expectations\n",
    "- Calculate accuracy\n",
    "\n",
    "### Exercise 4: Edge Cases\n",
    "Test the limits:\n",
    "- Questions with no relevant documents\n",
    "- Ambiguous questions\n",
    "- Questions requiring multiple documents\n",
    "- Very long vs very short questions\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Key Insights\n",
    "\n",
    "### Remember:\n",
    "\n",
    "1. **RAG is a Pattern, Not a Library**: You can implement it with any LLM and embedding model\n",
    "\n",
    "2. **Quality Matters**: Document quality directly impacts RAG performance\n",
    "\n",
    "3. **It's a System**: Each component (retrieval, augmentation, generation) can be optimized independently\n",
    "\n",
    "4. **Start Simple**: Begin with basic RAG, then add complexity (re-ranking, hybrid search, etc.)\n",
    "\n",
    "5. **Measure Everything**: You can't improve what you don't measure\n",
    "\n",
    "6. **Context is King**: Better retrieval = better answers\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You now understand the fundamentals of RAG and can build your own RAG systems! This is a crucial skill for working with LLMs in real-world applications.\n",
    "\n",
    "RAG enables:\n",
    "- ‚úÖ Private/proprietary knowledge access\n",
    "- ‚úÖ Up-to-date information\n",
    "- ‚úÖ Reduced hallucination\n",
    "- ‚úÖ Source attribution\n",
    "- ‚úÖ Domain-specific accuracy\n",
    "\n",
    "Keep practicing, keep experimenting, and keep building! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ Additional Resources:\n",
    "\n",
    "- **OpenAI Embeddings Guide**: https://platform.openai.com/docs/guides/embeddings\n",
    "- **Vector Database Comparison**: Research different vector DBs for your use case\n",
    "- **RAG Papers**: Look up \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\n",
    "- **Community**: Join AI/ML communities to share learnings and get help\n",
    "\n",
    "---\n",
    "\n",
    "**Happy building! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}