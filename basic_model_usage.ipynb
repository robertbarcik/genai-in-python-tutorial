{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Basic OpenAI API Usage for IT Support Professionals\n",
    "\n",
    "Welcome! This notebook will teach you how to use the OpenAI API for practical IT support tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Content Moderation** - Check user messages for inappropriate content\n",
    "2. **Text Generation** - Draft responses, summarize documents, assist with troubleshooting\n",
    "3. **Audio Features** - Convert text to speech and transcribe audio\n",
    "4. **Image Generation** - Create simple visuals for documentation\n",
    "5. **Vision Analysis** - Analyze error screenshots and hardware images\n",
    "6. **Cost Management** - Use the most cost-effective models for each task\n",
    "\n",
    "---\n",
    "\n",
    "## üåê What is an API?\n",
    "\n",
    "An **API (Application Programming Interface)** is a way for different software programs to communicate with each other. Think of it like a menu at a restaurant:\n",
    "\n",
    "- You (your code) are the customer\n",
    "- The restaurant kitchen (OpenAI servers) has various capabilities\n",
    "- The menu (API documentation) tells you what you can order\n",
    "- You place an order (API request) and receive your food (API response)\n",
    "\n",
    "### üéØ What is an API Endpoint?\n",
    "\n",
    "An **endpoint** is a specific function or service the API provides. For example:\n",
    "- `/v1/chat/completions` - Text generation endpoint\n",
    "- `/v1/moderations` - Content moderation endpoint\n",
    "- `/v1/images/generations` - Image generation endpoint\n",
    "\n",
    "Each endpoint has a specific purpose and accepts different parameters.\n",
    "\n",
    "### üì¨ Request/Response Structure\n",
    "\n",
    "API communication follows a simple pattern:\n",
    "\n",
    "1. **Request**: You send data to the API\n",
    "   - Headers (authentication, content type)\n",
    "   - Parameters (instructions, settings)\n",
    "   - Body (the actual data to process)\n",
    "\n",
    "2. **Response**: The API sends back results\n",
    "   - Status code (200 = success, 429 = rate limit, etc.)\n",
    "   - Data (the result you requested)\n",
    "   - Metadata (usage stats, IDs, etc.)\n",
    "\n",
    "### ‚ö†Ô∏è Important Concepts\n",
    "\n",
    "**Rate Limits**: APIs limit how many requests you can make per minute/day to prevent abuse. If you exceed these limits, you'll receive an error.\n",
    "\n",
    "**Costs**: Most API calls cost money based on usage:\n",
    "- Text models charge per \"token\" (roughly 4 characters)\n",
    "- Image models charge per image generated\n",
    "- Audio models charge per character (TTS) or per minute (transcription)\n",
    "\n",
    "**Security Best Practice**: ‚ö†Ô∏è **NEVER hardcode API keys in production code!** Always use environment variables, secrets management, or secure configuration systems.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Setup\n",
    "\n",
    "First, we need to configure your OpenAI API key and install required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies\n",
    "\n",
    "We'll install three libraries:\n",
    "- **openai**: Official OpenAI Python client for API access\n",
    "- **pillow**: Image processing library for displaying and manipulating images\n",
    "- **requests**: HTTP library for downloading files from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "# Options: \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-3.5-turbo\", \"gpt-5-nano\", etc.\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize OpenAI Client\n",
    "\n",
    "Now let's create a client instance to interact with the OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Content Moderation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "The moderation API detects potentially harmful or inappropriate content in text. It checks for:\n",
    "- Hate speech\n",
    "- Harassment and bullying\n",
    "- Violence and graphic content\n",
    "- Self-harm content\n",
    "- Sexual content\n",
    "- And more...\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "Use moderation **before** processing user-submitted content:\n",
    "- Support ticket messages\n",
    "- Chat messages\n",
    "- Email content\n",
    "- User feedback forms\n",
    "\n",
    "## üí° Why It Matters\n",
    "\n",
    "1. **Protects your API account** - OpenAI can suspend accounts that process harmful content\n",
    "2. **Ensures safe interactions** - Filters out inappropriate content before your team sees it\n",
    "3. **FREE to use** - The moderation API is completely free for OpenAI users!\n",
    "\n",
    "## üîë Key Parameters\n",
    "\n",
    "- **input**: The text to check (string or array of strings)\n",
    "\n",
    "**Response contains:**\n",
    "- **flagged**: Boolean indicating if content violates policies\n",
    "- **categories**: Dictionary of specific violation types (hate, violence, etc.)\n",
    "- **category_scores**: Confidence scores (0-1) for each category\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Practical Example: Checking Customer Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Frustrated but appropriate customer message\n",
    "customer_message_1 = \"\"\"This is absolutely ridiculous! Your service is terrible \n",
    "and I'm furious about this issue with my account!\"\"\"\n",
    "\n",
    "# Check the message using moderation API\n",
    "response = client.moderations.create(input=customer_message_1)\n",
    "\n",
    "result = response.results[0]\n",
    "\n",
    "print(\"üîç Moderation Check Results:\")\n",
    "print(f\"Is Flagged: {result.flagged}\")\n",
    "print(f\"\\nCategory Flags:\")\n",
    "for category, flagged in result.categories:\n",
    "    if flagged:\n",
    "        print(f\"  ‚ö†Ô∏è {category}: {flagged}\")\n",
    "\n",
    "if not result.flagged:\n",
    "    print(\"\\n‚úÖ Message is safe to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Inappropriate message (DO NOT USE IN PRODUCTION)\n",
    "# This example demonstrates what gets flagged\n",
    "inappropriate_message = \"\"\"I hate your company and all your stupid employees. \n",
    "You're all idiots and deserve to lose your jobs.\"\"\"\n",
    "\n",
    "response = client.moderations.create(input=inappropriate_message)\n",
    "result = response.results[0]\n",
    "\n",
    "print(\"üîç Moderation Check Results:\")\n",
    "print(f\"Is Flagged: {result.flagged}\")\n",
    "print(f\"\\nCategory Flags:\")\n",
    "for category, flagged in result.categories:\n",
    "    if flagged:\n",
    "        print(f\"  ‚ö†Ô∏è {category}: {flagged}\")\n",
    "\n",
    "if result.flagged:\n",
    "    print(\"\\nüö´ Message contains inappropriate content - DO NOT process with API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Reusable Function\n",
    "\n",
    "Let's create a simple function you can use in your projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_safety(text):\n",
    "    \"\"\"\n",
    "    Check if text content is safe to process.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'is_safe' boolean and 'flagged_categories' list\n",
    "    \"\"\"\n",
    "    response = client.moderations.create(input=text)\n",
    "    result = response.results[0]\n",
    "    \n",
    "    # Get all flagged categories\n",
    "    flagged_categories = [category for category, flagged in result.categories if flagged]\n",
    "    \n",
    "    return {\n",
    "        \"is_safe\": not result.flagged,\n",
    "        \"flagged_categories\": flagged_categories\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_message = \"My printer won't work and I need help urgently!\"\n",
    "safety_check = check_content_safety(test_message)\n",
    "\n",
    "print(f\"Is safe: {safety_check['is_safe']}\")\n",
    "if safety_check['flagged_categories']:\n",
    "    print(f\"Flagged for: {', '.join(safety_check['flagged_categories'])}\")\n",
    "else:\n",
    "    print(\"No violations detected ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Not checking all category flags individually** - Just checking `flagged` isn't enough; sometimes you need to know *what* was flagged\n",
    "\n",
    "2. **Forgetting to check moderation before sending to main API** - Always moderate user content first to protect your account\n",
    "\n",
    "3. **Over-relying on moderation** - The API is very good but not 100% perfect. Consider it as a helpful filter, not absolute protection\n",
    "\n",
    "4. **Not handling edge cases** - Very short messages or special characters might behave unexpectedly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Text Generation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "Text generation models create human-like text responses using advanced language models. They can:\n",
    "- Answer questions\n",
    "- Draft responses\n",
    "- Summarize content\n",
    "- Provide troubleshooting guidance\n",
    "- Explain technical concepts\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "- **Drafting support ticket responses** - Save time with AI-generated first drafts\n",
    "- **Summarizing documentation** - Condense long technical docs into key points\n",
    "- **Troubleshooting assistance** - Generate step-by-step diagnostic procedures\n",
    "- **Knowledge base creation** - Turn technical info into user-friendly content\n",
    "\n",
    "## üí∞ Cost Implications\n",
    "\n",
    "Text generation is charged **per token**. \n",
    "\n",
    "**What's a token?** Roughly 4 characters or 0.75 words. For example:\n",
    "- \"Hello\" = 1 token\n",
    "- \"Hello, how are you?\" = 5 tokens\n",
    "- 100 words ‚âà 133 tokens\n",
    "\n",
    "You're charged for **both input and output tokens**.\n",
    "\n",
    "## üìè Context Window\n",
    "\n",
    "Models have limits on how much text they can process at once (input + output). For example:\n",
    "- `gpt-5-nano`: 128K tokens (~96,000 words)\n",
    "- `gpt-3.5-turbo`: 16K tokens (~12,000 words)\n",
    "\n",
    "## üîë Key Parameters for gpt-5-nano (Responses API)\n",
    "\n",
    "**Important**: gpt-5-nano uses the **Responses API**, not the Chat Completions API!\n",
    "\n",
    "- **model**: \"gpt-5-nano\" (one of the cheapest models)\n",
    "- **input**: Your prompt or question (string)\n",
    "- **text**: Dictionary with optional settings:\n",
    "  - **verbosity**: \"low\", \"medium\", or \"high\" - controls response length and detail\n",
    "    - \"low\": Brief, concise responses\n",
    "    - \"medium\": Balanced responses (default)\n",
    "    - \"high\": Detailed, comprehensive responses\n",
    "\n",
    "**Response structure**:\n",
    "- Use `response.output_text` to get the generated text\n",
    "- Use `response.usage.total_tokens` for token count\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Your prompt here\",\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "output = response.output_text\n",
    "```\n",
    "\n",
    "**Important notes about gpt-5-nano**:\n",
    "- Uses `client.responses.create()` instead of `client.chat.completions.create()`\n",
    "- Access output with `response.output_text` instead of `response.choices[0].message.content`\n",
    "- System prompts should be included directly in the input string\n",
    "- Pricing: $0.05/1M input tokens, $0.40/1M output tokens (cheaper than gpt-4o!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 1: Drafting a Support Ticket Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: User reports printer not working\n",
    "user_ticket = \"\"\"My printer won't print anything. I tried turning it off and on \n",
    "but nothing works. I need to print reports for a meeting in 30 minutes.\"\"\"\n",
    "\n",
    "# System prompt defines the assistant's role and behavior\n",
    "system_prompt = \"\"\"You are an IT support assistant. Respond professionally, \n",
    "empathetically, and provide clear troubleshooting steps. Keep responses concise but thorough.\"\"\"\n",
    "\n",
    "# Combine system prompt with user message\n",
    "full_input = f\"{system_prompt}\\n\\nUser message: {user_ticket}\"\n",
    "\n",
    "# Make the API call using Responses API\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "draft_response = response.output_text\n",
    "\n",
    "print(\"üìù Draft Response:\")\n",
    "print(draft_response)\n",
    "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 2: Summarizing Technical Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Summarize a long technical document\n",
    "long_technical_doc = \"\"\"\n",
    "VPN Configuration Guide:\n",
    "\n",
    "To establish a secure VPN connection, users must first ensure they have the latest \n",
    "VPN client installed (version 8.2 or higher). The installation package can be \n",
    "downloaded from the company portal under Software > Security Tools.\n",
    "\n",
    "Once installed, launch the VPN client and enter your corporate credentials. \n",
    "The username should be in the format: firstname.lastname@company.com. \n",
    "Use your standard Windows password.\n",
    "\n",
    "After successful authentication, select the appropriate VPN gateway from the dropdown:\n",
    "- US-EAST-01: For users in North America\n",
    "- EU-WEST-01: For users in Europe\n",
    "- ASIA-PAC-01: For users in Asia Pacific\n",
    "\n",
    "Click Connect and wait for the status indicator to turn green. This typically takes \n",
    "15-30 seconds. Once connected, you'll have access to internal resources including \n",
    "file shares, internal websites, and database servers.\n",
    "\n",
    "If you experience connection issues, first verify your internet connection is stable. \n",
    "Then check if your antivirus software is blocking the VPN client. Common antivirus \n",
    "programs that may interfere include McAfee and Norton. Add an exception for the \n",
    "VPN client executable if necessary.\n",
    "\n",
    "For persistent issues, contact IT support with your error message and the VPN.log \n",
    "file located in C:\\\\Program Files\\\\CompanyVPN\\\\logs\\\\.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"Summarize this technical documentation in 3-4 bullet points for end users.\"\n",
    "\n",
    "# Combine system prompt with document\n",
    "full_input = f\"{system_prompt}\\n\\n{long_technical_doc}\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"medium\"}  # Using medium for concise summary\n",
    ")\n",
    "\n",
    "summary = response.output_text\n",
    "\n",
    "print(\"üìã Summary:\")\n",
    "print(summary)\n",
    "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")\n",
    "print(f\"   Input: {response.usage.input_tokens}, Output: {response.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 3: Troubleshooting Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: User describes an error\n",
    "user_description = \"I keep getting 'Access Denied' when trying to open the shared drive.\"\n",
    "\n",
    "system_prompt = \"\"\"You are a troubleshooting assistant. Provide step-by-step diagnostic steps. \n",
    "Number each step clearly. Focus on the most common causes first.\"\"\"\n",
    "\n",
    "# Combine system prompt with user description\n",
    "full_input = f\"{system_prompt}\\n\\nUser issue: {user_description}\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "troubleshooting_steps = response.output_text\n",
    "\n",
    "print(\"üîß Troubleshooting Steps:\")\n",
    "print(troubleshooting_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Error Handling\n",
    "\n",
    "Always wrap API calls in try/except blocks to handle potential errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_safely(user_message, system_message=\"You are a helpful IT assistant.\"):\n",
    "    \"\"\"\n",
    "    Generate a response with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_message (str): The user's input\n",
    "        system_message (str): System prompt for the assistant\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'success' boolean, 'response' text, and optional 'error' message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Combine system prompt with user message\n",
    "        full_input = f\"{system_message}\\n\\nUser: {user_message}\"\n",
    "        \n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=full_input,\n",
    "            text={\"verbosity\": \"high\"}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": response.output_text,\n",
    "            \"tokens_used\": response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"response\": None\n",
    "        }\n",
    "\n",
    "# Test the function\n",
    "result = generate_response_safely(\"How do I reset my password?\")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"‚úÖ Response generated successfully\")\n",
    "    print(result[\"response\"])\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Vague prompts leading to generic responses**\n",
    "   - ‚ùå Bad: \"Help with printer\"\n",
    "   - ‚úÖ Good: \"Provide step-by-step troubleshooting for a printer that won't print\"\n",
    "\n",
    "2. **Not setting appropriate max_completion_tokens**\n",
    "   - Set limits to control costs and prevent overly long responses\n",
    "   - 100 tokens ‚âà 75 words\n",
    "\n",
    "3. **Forgetting to handle API errors**\n",
    "   - Always use try/except blocks\n",
    "   - Common errors: rate limits, invalid API key, network issues\n",
    "\n",
    "4. **Not using system prompts effectively**\n",
    "   - System prompts set the behavior and tone\n",
    "   - Be specific about desired output format and style\n",
    "\n",
    "5. **Using unsupported parameters**\n",
    "   - `gpt-5-nano` only supports the default temperature (1)\n",
    "   - Don't specify temperature parameter or you'll get an error\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Audio Features (Text-to-Speech & Transcription)\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "OpenAI provides two audio capabilities:\n",
    "1. **Text-to-Speech (TTS)**: Convert text into natural-sounding speech\n",
    "2. **Transcription**: Convert audio recordings into text\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "**Text-to-Speech:**\n",
    "- Creating audio guides for common procedures\n",
    "- Accessibility features for visually impaired users\n",
    "- Automated phone system messages\n",
    "- Training materials\n",
    "\n",
    "**Transcription:**\n",
    "- Converting support call recordings to text\n",
    "- Creating searchable records of meetings\n",
    "- Documenting verbal troubleshooting sessions\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "**TTS Models:**\n",
    "- `tts-1`: Faster, cheaper, good quality\n",
    "- `tts-1-hd`: Higher quality, more expensive\n",
    "\n",
    "**Transcription Model:**\n",
    "- `whisper-1`: OpenAI's speech recognition model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîä Part A: Text-to-Speech\n",
    "\n",
    "### üîë Key Parameters\n",
    "\n",
    "- **model**: `\"tts-1\"` (cheaper, faster) or `\"tts-1-hd\"` (higher quality)\n",
    "- **voice**: Choose from 6 voices:\n",
    "  - `alloy`: Neutral, balanced\n",
    "  - `echo`: Male, clear\n",
    "  - `fable`: Male, expressive\n",
    "  - `onyx`: Male, deep\n",
    "  - `nova`: Female, energetic\n",
    "  - `shimmer`: Female, soft\n",
    "- **input**: Text to convert (max 4096 characters)\n",
    "- **speed**: 0.25 to 4.0 (default 1.0)\n",
    "\n",
    "### üíª Practical Example: Create Password Reset Audio Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text for our audio guide\n",
    "password_reset_guide = \"\"\"\n",
    "Welcome to the password reset guide. Here are the steps:\n",
    "\n",
    "First, go to the login page and click 'Forgot Password'.\n",
    "\n",
    "Second, enter your work email address.\n",
    "\n",
    "Third, check your email for a reset link. This may take a few minutes.\n",
    "\n",
    "Finally, click the link and create a new password.\n",
    "\n",
    "Remember: your password must be at least 8 characters with numbers and symbols.\n",
    "\n",
    "If you need further assistance, contact the IT help desk. Thank you.\n",
    "\"\"\"\n",
    "\n",
    "# Generate speech\n",
    "print(\"üéôÔ∏è Generating audio...\")\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",  # Using cheaper model\n",
    "    voice=\"nova\",   # Female, energetic voice\n",
    "    input=password_reset_guide,\n",
    "    speed=1.0\n",
    ")\n",
    "\n",
    "# Save the audio file\n",
    "audio_file_path = \"/content/password_reset_guide.mp3\"\n",
    "response.stream_to_file(audio_file_path)\n",
    "\n",
    "print(f\"‚úÖ Audio saved to: {audio_file_path}\")\n",
    "print(\"\\n‚ñ∂Ô∏è You can play it using the file browser on the left sidebar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audio player in notebook\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(audio_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé§ Part B: Transcription (Speech-to-Text)\n",
    "\n",
    "### üîë Key Parameters\n",
    "\n",
    "- **model**: `\"whisper-1\"` (OpenAI's speech recognition model)\n",
    "- **file**: Audio file to transcribe (max 25MB)\n",
    "- **language**: Optional ISO-639-1 code (e.g., \"en\" for English) for better accuracy\n",
    "- **response_format**: \"json\", \"text\", \"srt\", \"vtt\", or \"verbose_json\"\n",
    "\n",
    "**Supported formats**: mp3, mp4, mpeg, mpga, m4a, wav, webm\n",
    "\n",
    "### üíª Practical Example: Transcribe the Audio We Just Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the password reset guide we just created\n",
    "print(\"üé§ Transcribing audio...\")\n",
    "\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        language=\"en\"  # Specify English for better accuracy\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Transcription complete!\\n\")\n",
    "print(\"üìù Transcribed Text:\")\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Compare Original vs Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Original Text ‚Üí Audio ‚Üí Transcription\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìÑ ORIGINAL TEXT:\")\n",
    "print(password_reset_guide)\n",
    "\n",
    "print(\"\\nüéôÔ∏è TRANSCRIBED TEXT:\")\n",
    "print(transcript.text)\n",
    "\n",
    "print(\"\\n‚ú® Notice how accurate the transcription is!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### Text-to-Speech:\n",
    "1. **Text too long** - Maximum 4096 characters per request. Split longer text into chunks.\n",
    "2. **Wrong speed settings** - Speed too fast (>1.5) can reduce clarity\n",
    "3. **Not choosing appropriate voice** - Test different voices for your use case\n",
    "\n",
    "### Transcription:\n",
    "1. **Audio files too large** - Whisper-1 has a 25MB limit. Compress large files first.\n",
    "2. **Wrong audio format** - Ensure your file is in a supported format (mp3, wav, etc.)\n",
    "3. **Not specifying language** - Adding the language parameter improves accuracy\n",
    "4. **Poor audio quality** - Background noise and low volume reduce transcription accuracy\n",
    "5. **Very long files** - Consider splitting files longer than 30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Image Generation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "The image generation API creates images from text descriptions using DALL-E models. You provide a text prompt, and it generates an original image.\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "- **Quick mockups** - Visualize concepts before creating proper designs\n",
    "- **Documentation visuals** - Create diagrams and illustrations for guides\n",
    "- **Training materials** - Generate images for presentations and tutorials\n",
    "- **Placeholder images** - Quick visuals during development\n",
    "\n",
    "## ‚ö†Ô∏è Important Limitation\n",
    "\n",
    "**AI-generated technical diagrams are NOT always accurate!** They're best for:\n",
    "- Conceptual illustrations\n",
    "- General visual aids\n",
    "- Quick drafts\n",
    "\n",
    "For production use, always have technical diagrams reviewed by experts.\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "- `dall-e-2`: Cheaper, good quality, faster\n",
    "- `dall-e-3`: Higher quality, more expensive, better prompt following\n",
    "\n",
    "**We'll use DALL-E 2 for cost efficiency.**\n",
    "\n",
    "## üîë Key Parameters\n",
    "\n",
    "- **model**: `\"dall-e-2\"` or `\"dall-e-3\"`\n",
    "- **prompt**: Description of the image (max 1000 chars for DALL-E 2)\n",
    "- **size**: \n",
    "  - DALL-E 2: `\"256x256\"`, `\"512x512\"`, or `\"1024x1024\"`\n",
    "  - DALL-E 3: `\"1024x1024\"`, `\"1792x1024\"`, or `\"1024x1792\"`\n",
    "- **n**: Number of images to generate (1-10 for DALL-E 2, only 1 for DALL-E 3)\n",
    "- **response_format**: `\"url\"` (default) or `\"b64_json\"`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Practical Example: Network Diagram for Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple network diagram\n",
    "prompt = \"\"\"A simple network diagram showing a router connected to a firewall, \n",
    "with three workstations behind it. Clean, technical style with clear labels. \n",
    "Minimalist and professional. White background.\"\"\"\n",
    "\n",
    "print(\"üé® Generating image...\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-2\",  # Using cheaper model\n",
    "    prompt=prompt,\n",
    "    size=\"512x512\",    # Medium size for documentation\n",
    "    n=1                # Generate 1 image\n",
    ")\n",
    "\n",
    "# Get the image URL\n",
    "image_url = response.data[0].url\n",
    "print(f\"‚úÖ Image generated!\")\n",
    "print(f\"URL: {image_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and display the image\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the image\n",
    "image_response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "# Save to file\n",
    "image_path = \"/content/network_diagram.png\"\n",
    "image.save(image_path)\n",
    "print(f\"üíæ Saved to: {image_path}\")\n",
    "\n",
    "# Display in notebook\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Important Note About AI-Generated Technical Diagrams\n",
    "\n",
    "‚ö†Ô∏è **While AI-generated technical diagrams may not always be 100% accurate, they're useful for:**\n",
    "- Quick conceptual illustrations in documentation\n",
    "- Initial drafts to communicate ideas\n",
    "- Placeholder visuals during development\n",
    "\n",
    "**Always review and verify technical accuracy before using in production documentation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 2: Generate Multiple Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple variations to choose from\n",
    "prompt = \"\"\"An icon representing IT support and technical help. \n",
    "Simple, modern, professional. Blue and white colors.\"\"\"\n",
    "\n",
    "print(\"üé® Generating 3 variations...\\n\")\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=prompt,\n",
    "    size=\"256x256\",  # Small size for icons\n",
    "    n=3              # Generate 3 options\n",
    ")\n",
    "\n",
    "# Display all variations\n",
    "for i, image_data in enumerate(response.data, 1):\n",
    "    image_url = image_data.url\n",
    "    image_response = requests.get(image_url)\n",
    "    image = Image.open(BytesIO(image_response.content))\n",
    "    \n",
    "    print(f\"Option {i}:\")\n",
    "    display(image)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Expecting technical accuracy from AI-generated diagrams**\n",
    "   - AI images are conceptual, not technically precise\n",
    "   - Always verify technical details\n",
    "\n",
    "2. **Using high-cost options (DALL-E 3) unnecessarily**\n",
    "   - DALL-E 2 is often sufficient for documentation needs\n",
    "   - Save DALL-E 3 for when quality really matters\n",
    "\n",
    "3. **Very long prompts**\n",
    "   - DALL-E 2 max: 1000 characters\n",
    "   - DALL-E 3 max: 4000 characters\n",
    "   - Keep prompts concise and focused\n",
    "\n",
    "4. **Vague prompts**\n",
    "   - ‚ùå Bad: \"network diagram\"\n",
    "   - ‚úÖ Good: \"simple network diagram with router, firewall, and 3 workstations, minimalist style, white background\"\n",
    "\n",
    "5. **Not specifying style**\n",
    "   - Always mention: \"professional\", \"minimalist\", \"technical\", \"clean\", etc.\n",
    "   - Specify color scheme if important\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Vision (Image Analysis)\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "Vision-capable models can \"see\" and analyze images. They can:\n",
    "- Describe what's in an image\n",
    "- Read text from screenshots\n",
    "- Identify error messages\n",
    "- Analyze hardware components\n",
    "- Recognize UI elements\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "**Highly practical for IT support:**\n",
    "- **Error screenshot analysis** - \"What error is shown and how to fix it?\"\n",
    "- **Hardware diagnostics** - \"What component is this?\"\n",
    "- **UI troubleshooting** - \"Which button should the user click?\"\n",
    "- **Cable verification** - \"Is this cable connected correctly?\"\n",
    "- **Configuration review** - \"What settings are shown in this screenshot?\"\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "- `gpt-4o`: Highest quality vision analysis\n",
    "- `gpt-5-nano`: Cheaper, still very capable (recommended for most use cases)\n",
    "\n",
    "## üîë Key Parameters\n",
    "\n",
    "- **model**: `\"gpt-5-nano\"` (cheaper) or `\"gpt-4o\"` (higher quality)\n",
    "- **messages**: Array with both text and image content types\n",
    "- **max_completion_tokens**: Limit response length\n",
    "\n",
    "**Image input options:**\n",
    "- URL: `{\"type\": \"image_url\", \"image_url\": {\"url\": \"https://...\"}}`\n",
    "- Base64: `{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,...\"}}`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Practical Example: Error Screenshot Analysis\n",
    "\n",
    "### üì∏ Upload Your Image\n",
    "\n",
    "To use this example:\n",
    "1. Click the üìÅ folder icon in the left sidebar\n",
    "2. Click the upload button (üì§)\n",
    "3. Upload an error screenshot or any IT-related image\n",
    "4. Update the `image_path` variable below with your filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Analyze an uploaded image file\n",
    "import base64\n",
    "\n",
    "# Update this path with your uploaded image\n",
    "image_path = \"/content/error_screenshot.png\"  # Change this to your image filename\n",
    "\n",
    "# Check if file exists\n",
    "import os\n",
    "if not os.path.exists(image_path):\n",
    "    print(\"‚ö†Ô∏è Image file not found!\")\n",
    "    print(\"Please upload an image file and update the image_path variable above.\")\n",
    "    print(\"\\nFor demonstration, we'll create a simple example...\")\n",
    "else:\n",
    "    # Read and encode the image\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    # Analyze the image\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",  # Using cheaper vision model\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"Please analyze this screenshot:\n",
    "                        1. What error or issue is shown?\n",
    "                        2. What are the likely causes?\n",
    "                        3. What troubleshooting steps would you recommend?\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        reasoning_effort=\"minimal\",\n",
    "        verbosity=\"high\",\n",
    "        max_completion_tokens=500\n",
    "    )\n",
    "    \n",
    "    analysis = response.choices[0].message.content\n",
    "    \n",
    "    print(\"üîç Image Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(analysis)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 2: Analyze Image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Analyze an image from a URL\n",
    "# Example: Analyzing a computer hardware image\n",
    "\n",
    "# Note: Replace with your own image URL for testing\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/USB_Icon.svg/512px-USB_Icon.svg.png\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is shown in this image? Please describe it in detail.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    reasoning_effort=\"minimal\",\n",
    "    verbosity=\"high\",\n",
    "    max_completion_tokens=300\n",
    ")\n",
    "\n",
    "print(\"üîç Analysis:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Example 3: Hardware Component Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable function for IT support image analysis\n",
    "def analyze_it_image(image_input, question):\n",
    "    \"\"\"\n",
    "    Analyze an IT-related image.\n",
    "    \n",
    "    Args:\n",
    "        image_input (str): File path or URL to the image\n",
    "        question (str): What you want to know about the image\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'success' boolean and 'analysis' text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine if input is a file path or URL\n",
    "        if image_input.startswith('http://') or image_input.startswith('https://'):\n",
    "            # It's a URL\n",
    "            image_url = image_input\n",
    "        else:\n",
    "            # It's a file path - encode to base64\n",
    "            with open(image_input, \"rb\") as image_file:\n",
    "                base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "            image_url = f\"data:image/png;base64,{base64_image}\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5-nano\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": question},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            reasoning_effort=\"minimal\",\n",
    "            verbosity=\"high\",\n",
    "            max_completion_tokens=400\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"analysis\": response.choices[0].message.content,\n",
    "            \"tokens_used\": response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"üì∏ This is a reusable function for analyzing IT images.\")\n",
    "print(\"\\nExample usage:\")\n",
    "print('result = analyze_it_image(\"/content/error.png\", \"What error is shown?\")')\n",
    "print('result = analyze_it_image(\"https://example.com/cable.jpg\", \"Is this cable connected properly?\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Image files too large**\n",
    "   - Recommended: Keep images under 20MB\n",
    "   - Compress large images before uploading\n",
    "   - Use tools like PIL to resize: `image.thumbnail((1024, 1024))`\n",
    "\n",
    "2. **Unclear or low-resolution screenshots**\n",
    "   - Ensure text in screenshots is readable\n",
    "   - Higher resolution = better analysis\n",
    "   - Avoid blurry or pixelated images\n",
    "\n",
    "3. **Not providing enough context in the text prompt**\n",
    "   - ‚ùå Bad: \"What's in this image?\"\n",
    "   - ‚úÖ Good: \"What error is shown in this Windows screenshot and what are possible solutions?\"\n",
    "\n",
    "4. **Assuming the model can see very small text**\n",
    "   - If text is important, make sure it's legible in the screenshot\n",
    "   - Zoom in or crop to the relevant area\n",
    "\n",
    "5. **Incorrect image encoding**\n",
    "   - Ensure proper base64 encoding\n",
    "   - Include correct MIME type (image/png, image/jpeg, etc.)\n",
    "\n",
    "6. **Not handling file path errors**\n",
    "   - Always check if file exists before trying to read it\n",
    "   - Use try/except blocks for error handling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Cost Awareness & Best Practices\n",
    "\n",
    "## üí∞ Understanding API Costs\n",
    "\n",
    "OpenAI charges based on usage. Here's a general overview:\n",
    "\n",
    "### Pricing by Feature (Approximate)\n",
    "\n",
    "| Feature | Model | Cost |\n",
    "|---------|-------|------|\n",
    "| **Content Moderation** | moderation-latest | **FREE** |\n",
    "| **Text Generation** | gpt-5-nano | $0.05 / 1M input tokens<br>$0.40 / 1M output tokens |\n",
    "| | gpt-4o | $2.50 / 1M input tokens<br>$10.00 / 1M output tokens |\n",
    "| **Audio (TTS)** | tts-1 | $15.00 / 1M characters |\n",
    "| | tts-1-hd | $30.00 / 1M characters |\n",
    "| **Audio (Transcription)** | whisper-1 | $0.006 / minute |\n",
    "| **Image Generation** | dall-e-2 (1024√ó1024) | $0.020 / image |\n",
    "| | dall-e-3 (1024√ó1024) | $0.040 / image |\n",
    "| **Vision Analysis** | gpt-4o | $2.50 / 1M input tokens<br>$10.00 / 1M output tokens |\n",
    "\n",
    "‚ö†Ô∏è **Note**: These are approximate prices as of early 2025. Always check the [official OpenAI pricing page](https://openai.com/pricing) for current rates.\n",
    "\n",
    "### üìä What's a Token?\n",
    "\n",
    "- 1 token ‚âà 4 characters\n",
    "- 1 token ‚âà 0.75 words\n",
    "- 100 words ‚âà 133 tokens\n",
    "- 1,000 characters ‚âà 250 tokens\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Cost Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate approximate cost for a text generation request\n",
    "\n",
    "def estimate_text_cost(input_text, output_tokens_estimate=200, model=\"gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Estimate the cost of a text generation request.\n",
    "    \n",
    "    Args:\n",
    "        input_text (str): The input text\n",
    "        output_tokens_estimate (int): Expected output length in tokens\n",
    "        model (str): Model name\n",
    "    \n",
    "    Returns:\n",
    "        dict: Cost breakdown\n",
    "    \"\"\"\n",
    "    # Rough token estimation (actual tokenization is more complex)\n",
    "    input_tokens = len(input_text) // 4\n",
    "    \n",
    "    # Pricing (per 1M tokens)\n",
    "    if model == \"gpt-5-nano\":\n",
    "        input_cost_per_1m = 0.05\n",
    "        output_cost_per_1m = 0.40\n",
    "    elif model == \"gpt-4o\":\n",
    "        input_cost_per_1m = 2.50\n",
    "        output_cost_per_1m = 10.00\n",
    "    else:\n",
    "        input_cost_per_1m = 0.05  # Default to nano pricing\n",
    "        output_cost_per_1m = 0.40\n",
    "    \n",
    "    input_cost = (input_tokens / 1_000_000) * input_cost_per_1m\n",
    "    output_cost = (output_tokens_estimate / 1_000_000) * output_cost_per_1m\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_estimate\": output_tokens_estimate,\n",
    "        \"total_tokens\": input_tokens + output_tokens_estimate,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "\n",
    "# Example\n",
    "sample_ticket = \"My computer won't start. The screen is black and I hear beeping sounds.\"\n",
    "cost = estimate_text_cost(sample_ticket, output_tokens_estimate=300)\n",
    "\n",
    "print(\"üí∞ Cost Estimate:\")\n",
    "print(f\"Input tokens: {cost['input_tokens']}\")\n",
    "print(f\"Output tokens (estimated): {cost['output_tokens_estimate']}\")\n",
    "print(f\"Total tokens: {cost['total_tokens']}\")\n",
    "print(f\"\\nEstimated cost: ${cost['total_cost']:.6f}\")\n",
    "print(f\"  Input: ${cost['input_cost']:.6f}\")\n",
    "print(f\"  Output: ${cost['output_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices Summary\n",
    "\n",
    "### 1. Content Safety\n",
    "- ‚úÖ **Always use moderation for user-generated content**\n",
    "- ‚úÖ Check moderation BEFORE sending to other APIs\n",
    "- ‚úÖ It's FREE - use it liberally!\n",
    "\n",
    "### 2. Model Selection\n",
    "- ‚úÖ **Choose the cheapest model that meets your needs**\n",
    "  - Text: `gpt-5-nano` for most tasks (uses Responses API)\n",
    "  - Audio TTS: `tts-1` (not `tts-1-hd`)\n",
    "  - Images: `dall-e-2` (not `dall-e-3`)\n",
    "  - Vision: `gpt-4o` (currently the main vision model)\n",
    "- ‚úÖ Only upgrade to premium models when quality difference matters\n",
    "\n",
    "### 3. API Usage for gpt-5-nano\n",
    "- ‚úÖ **Use the Responses API**: `client.responses.create()`\n",
    "- ‚úÖ **Access output correctly**: `response.output_text`\n",
    "- ‚úÖ **Control verbosity**: Use `text={\"verbosity\": \"low|medium|high\"}`\n",
    "  - \"low\": Brief responses, saves tokens\n",
    "  - \"medium\": Balanced (default)\n",
    "  - \"high\": Detailed, comprehensive responses\n",
    "\n",
    "### 4. Error Handling\n",
    "- ‚úÖ **Always use try/except blocks**\n",
    "- ‚úÖ Handle rate limits gracefully\n",
    "- ‚úÖ Provide helpful error messages to users\n",
    "- ‚úÖ Log errors for debugging\n",
    "\n",
    "### 5. Security\n",
    "- ‚úÖ **NEVER hardcode API keys in production**\n",
    "- ‚úÖ Use environment variables or secrets management\n",
    "- ‚úÖ Rotate API keys periodically\n",
    "- ‚úÖ Set up usage alerts in OpenAI dashboard\n",
    "\n",
    "### 6. Monitoring\n",
    "- ‚úÖ **Track your API usage regularly**\n",
    "- ‚úÖ Set up billing alerts\n",
    "- ‚úÖ Monitor which endpoints cost the most\n",
    "- ‚úÖ Review and optimize high-usage areas\n",
    "\n",
    "### 7. Prompt Engineering\n",
    "- ‚úÖ Be specific about what you want\n",
    "- ‚úÖ Include system instructions directly in the input\n",
    "- ‚úÖ Provide examples when helpful\n",
    "- ‚úÖ Iterate and improve prompts based on results\n",
    "\n",
    "### 8. Model-Specific Settings\n",
    "- ‚úÖ `gpt-5-nano` uses the **Responses API**:\n",
    "  ```python\n",
    "  response = client.responses.create(\n",
    "      model=\"gpt-5-nano\",\n",
    "      input=\"your prompt here\",\n",
    "      text={\"verbosity\": \"high\"}\n",
    "  )\n",
    "  output = response.output_text\n",
    "  ```\n",
    "- ‚úÖ Much cheaper than gpt-4o: $0.05/1M input vs $2.50/1M\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Usage Tracking Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple usage tracker\n",
    "class UsageTracker:\n",
    "    def __init__(self):\n",
    "        self.total_requests = 0\n",
    "        self.total_tokens = 0\n",
    "        self.estimated_cost = 0.0\n",
    "    \n",
    "    def track_request(self, usage, model=\"gpt-5-nano\"):\n",
    "        \"\"\"Track a single API request.\"\"\"\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        input_tokens = usage.input_tokens\n",
    "        output_tokens = usage.output_tokens\n",
    "        total = usage.total_tokens\n",
    "        \n",
    "        self.total_tokens += total\n",
    "        \n",
    "        # Calculate cost (gpt-5-nano pricing: $0.05/1M input, $0.40/1M output)\n",
    "        if model == \"gpt-5-nano\":\n",
    "            cost = (input_tokens / 1_000_000) * 0.05 + (output_tokens / 1_000_000) * 0.40\n",
    "        else:\n",
    "            cost = (input_tokens / 1_000_000) * 2.50 + (output_tokens / 1_000_000) * 10.00\n",
    "        \n",
    "        self.estimated_cost += cost\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate a usage report.\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üìä USAGE REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total Requests: {self.total_requests}\")\n",
    "        print(f\"Total Tokens: {self.total_tokens:,}\")\n",
    "        print(f\"Estimated Cost: ${self.estimated_cost:.4f}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "# Example usage\n",
    "tracker = UsageTracker()\n",
    "\n",
    "# Make a test request\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=\"What is VPN?\",\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "# Track it\n",
    "tracker.track_request(response.usage, model=OPENAI_MODEL)\n",
    "\n",
    "# View report\n",
    "tracker.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Mini-Project: Support Ticket Analyzer\n",
    "\n",
    "## üéØ Project Description\n",
    "\n",
    "Let's combine everything we've learned! We'll build a **Support Ticket Analyzer** that:\n",
    "\n",
    "1. ‚úÖ **Checks if the ticket content is appropriate** (Moderation)\n",
    "2. üìä **Analyzes the urgency and issue type** (Text Generation)\n",
    "3. ‚úâÔ∏è **Drafts a professional response** (Text Generation)\n",
    "\n",
    "This is a practical tool you could adapt for real IT support workflows!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è TODO Version (Try It Yourself First!)\n",
    "\n",
    "Try to complete the function below on your own. Fill in the TODO sections using what you've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_support_ticket_TODO(ticket_content):\n",
    "    \"\"\"\n",
    "    Analyze a support ticket and generate a response.\n",
    "    \n",
    "    Args:\n",
    "        ticket_content (str): The customer's ticket message\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results including safety check, urgency, issue type, and draft response\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO 1: Check content safety with moderation API\n",
    "    # Hint: Use client.moderations.create()\n",
    "    # Store result in a variable called 'is_safe'\n",
    "    \n",
    "    # If content is not safe, return early\n",
    "    # if not is_safe:\n",
    "    #     return {\"is_safe\": False, \"message\": \"Inappropriate content detected\"}\n",
    "    \n",
    "    # TODO 2: Analyze ticket urgency and issue type\n",
    "    # Create a prompt that asks the AI to identify:\n",
    "    #   - Urgency level (Low, Medium, High, Critical)\n",
    "    #   - Issue type (Hardware, Software, Network, Access, Other)\n",
    "    # Hint: Use client.chat.completions.create() with a good system prompt\n",
    "    \n",
    "    # TODO 3: Generate a professional response\n",
    "    # Create another AI call to draft a response to the customer\n",
    "    # The response should be:\n",
    "    #   - Professional and empathetic\n",
    "    #   - Address the specific issue mentioned\n",
    "    #   - Provide clear next steps\n",
    "    \n",
    "    return {\n",
    "        \"is_safe\": True,  # Replace with actual result\n",
    "        \"urgency\": \"TODO\",\n",
    "        \"issue_type\": \"TODO\",\n",
    "        \"draft_response\": \"TODO\"\n",
    "    }\n",
    "\n",
    "print(\"üéì TODO: Complete the function above, then test it with the test cases below!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Complete Solution\n",
    "\n",
    "Here's a full implementation of the support ticket analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_support_ticket(ticket_content):\n",
    "    \"\"\"\n",
    "    Analyze a support ticket and generate a response.\n",
    "    \n",
    "    Args:\n",
    "        ticket_content (str): The customer's ticket message\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results including safety check, urgency, issue type, and draft response\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Check content safety\n",
    "        print(\"üîç Checking content safety...\")\n",
    "        moderation_response = client.moderations.create(input=ticket_content)\n",
    "        is_safe = not moderation_response.results[0].flagged\n",
    "        \n",
    "        if not is_safe:\n",
    "            flagged_categories = [\n",
    "                category for category, flagged \n",
    "                in moderation_response.results[0].categories \n",
    "                if flagged\n",
    "            ]\n",
    "            return {\n",
    "                \"is_safe\": False,\n",
    "                \"flagged_categories\": flagged_categories,\n",
    "                \"message\": \"‚ö†Ô∏è Inappropriate content detected. Ticket requires manual review.\"\n",
    "            }\n",
    "        \n",
    "        print(\"‚úÖ Content is safe\\n\")\n",
    "        \n",
    "        # Step 2: Analyze urgency and issue type\n",
    "        print(\"üìä Analyzing urgency and issue type...\")\n",
    "        analysis_prompt = f\"\"\"You are an IT ticket classification assistant. Be concise.\n",
    "\n",
    "Analyze this support ticket and provide:\n",
    "1. Urgency level: Critical / High / Medium / Low\n",
    "2. Issue type: Hardware / Software / Network / Access / Account / Other\n",
    "\n",
    "Provide your response in this exact format:\n",
    "Urgency: [level]\n",
    "Issue Type: [type]\n",
    "\n",
    "Ticket: {ticket_content}\n",
    "\"\"\"\n",
    "        \n",
    "        analysis_response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=analysis_prompt,\n",
    "            text={\"verbosity\": \"low\"}  # Low verbosity for classification\n",
    "        )\n",
    "        \n",
    "        analysis_text = analysis_response.output_text\n",
    "        \n",
    "        # Parse the response\n",
    "        urgency = \"Medium\"  # Default\n",
    "        issue_type = \"Other\"  # Default\n",
    "        \n",
    "        for line in analysis_text.split('\\n'):\n",
    "            if 'urgency:' in line.lower():\n",
    "                urgency = line.split(':')[1].strip()\n",
    "            elif 'issue type:' in line.lower():\n",
    "                issue_type = line.split(':')[1].strip()\n",
    "        \n",
    "        print(f\"  Urgency: {urgency}\")\n",
    "        print(f\"  Issue Type: {issue_type}\\n\")\n",
    "        \n",
    "        # Step 3: Generate professional response\n",
    "        print(\"‚úçÔ∏è Drafting response...\")\n",
    "        response_prompt = f\"\"\"You are a helpful IT support professional.\n",
    "\n",
    "Draft a professional IT support response to this ticket.\n",
    "\n",
    "Guidelines:\n",
    "- Be empathetic and professional\n",
    "- Acknowledge the issue\n",
    "- Provide clear troubleshooting steps or next actions\n",
    "- Include estimated response time if urgent\n",
    "- Keep it concise (3-4 short paragraphs)\n",
    "\n",
    "Ticket urgency: {urgency}\n",
    "Issue type: {issue_type}\n",
    "\n",
    "Ticket content: {ticket_content}\n",
    "\"\"\"\n",
    "        \n",
    "        response_generation = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=response_prompt,\n",
    "            text={\"verbosity\": \"high\"}\n",
    "        )\n",
    "        \n",
    "        draft_response = response_generation.output_text\n",
    "        \n",
    "        print(\"‚úÖ Analysis complete!\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"is_safe\": True,\n",
    "            \"urgency\": urgency,\n",
    "            \"issue_type\": issue_type,\n",
    "            \"draft_response\": draft_response,\n",
    "            \"success\": True\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"‚ùå Error analyzing ticket: {str(e)}\"\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Support Ticket Analyzer function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Cases\n",
    "\n",
    "Let's test the analyzer with different types of tickets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Urgent Password Reset Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ticket_1 = \"\"\"\n",
    "I've been locked out of my account and can't reset my password! \n",
    "The password reset link isn't working and I have an important \n",
    "presentation in 1 hour. I really need access ASAP!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Urgent Password Reset\")\n",
    "print(\"=\"ym * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_1}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_1)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: General Software Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ticket_2 = \"\"\"\n",
    "Hi, I'm trying to understand how to use the shared calendar feature \n",
    "in Outlook. Can someone explain how to share my calendar with my team \n",
    "and set the appropriate permissions? Thanks!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"ym * 70)\n",
    "print(\"TEST 2: General Software Question\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_2}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_2)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Frustrated User (Professional but Upset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ticket_3 = \"\"\"\n",
    "This is the third time this month the VPN has gone down! I'm trying to \n",
    "work from home and I can't access any company resources. This is extremely \n",
    "frustrating and affecting my productivity. When will this be fixed properly?\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 3: Frustrated User\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_3}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_3)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Project Complete!\n",
    "\n",
    "Congratulations! You've built a functional support ticket analyzer that:\n",
    "- ‚úÖ Checks content safety automatically\n",
    "- ‚úÖ Classifies urgency and issue type\n",
    "- ‚úÖ Generates helpful draft responses\n",
    "\n",
    "### üöÄ Ideas for Enhancement:\n",
    "\n",
    "1. **Add ticket routing** - Automatically assign tickets to teams based on issue type\n",
    "2. **Knowledge base integration** - Search KB articles related to the issue\n",
    "3. **Multi-language support** - Detect language and respond accordingly\n",
    "4. **Sentiment analysis** - Track customer satisfaction trends\n",
    "5. **Auto-escalation** - Automatically escalate critical issues\n",
    "6. **Response templates** - Build a library of pre-approved response templates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üö® Error Handling Reference\n",
    "\n",
    "Here are common errors you might encounter and how to handle them:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Error Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAIError, RateLimitError, APIError, AuthenticationError\n",
    "\n",
    "def make_robust_api_call(prompt, max_retries=3):\n",
    "    \"\"\"\n",
    "    Make an API call with comprehensive error handling.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response or error information\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                input=prompt,\n",
    "                text={\"verbosity\": \"high\"}\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response.output_text\n",
    "            }\n",
    "        \n",
    "        # Error 1: Rate Limit - Too many requests\n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s...\n",
    "            print(f\"‚ö†Ô∏è Rate limit hit. Waiting {wait_time} seconds...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"rate_limit\",\n",
    "                \"message\": \"Rate limit exceeded. Please try again later.\"\n",
    "            }\n",
    "        \n",
    "        # Error 2: Authentication - Invalid API key\n",
    "        except AuthenticationError as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"authentication\",\n",
    "                \"message\": \"Invalid API key. Please check your credentials.\"\n",
    "            }\n",
    "        \n",
    "        # Error 3: API Error - OpenAI service issues\n",
    "        except APIError as e:\n",
    "            print(f\"‚ö†Ô∏è API error occurred: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"api_error\",\n",
    "                \"message\": f\"OpenAI API error: {str(e)}\"\n",
    "            }\n",
    "        \n",
    "        # Error 4: Any other errors\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"unknown\",\n",
    "                \"message\": f\"Unexpected error: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        \"success\": False,\n",
    "        \"error_type\": \"max_retries\",\n",
    "        \"message\": \"Maximum retry attempts exceeded.\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Robust API call function created!\")\n",
    "print(\"\\nThis function handles:\")\n",
    "print(\"  1. Rate limits with exponential backoff\")\n",
    "print(\"  2. Authentication errors\")\n",
    "print(\"  3. API service errors\")\n",
    "print(\"  4. Unknown errors\")\n",
    "print(\"  5. Automatic retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a normal request\n",
    "result = make_robust_api_call(\"What is an IP address?\")\n",
    "\n",
    "if result['success']:\n",
    "    print(\"‚úÖ Success!\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error ({result['error_type']}): {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Error Reference Table\n",
    "\n",
    "| Error Type | Common Cause | Solution |\n",
    "|------------|--------------|----------|\n",
    "| **Rate Limit** | Too many requests per minute | Implement exponential backoff, reduce request frequency |\n",
    "| **Authentication** | Invalid/expired API key | Check API key, regenerate if needed |\n",
    "| **Model Not Found** | Typo in model name | Verify model name matches OpenAI docs |\n",
    "| **Token Limit Exceeded** | Input/output too long | Reduce prompt length or max_completion_tokens |\n",
    "| **Timeout** | Request took too long | Increase timeout, check network connection |\n",
    "| **Invalid Request** | Missing required parameters | Check API documentation for required fields |\n",
    "| **Content Policy** | Violated OpenAI policies | Use moderation API first, adjust content |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Troubleshooting Appendix\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### Issue 1: \"Invalid API Key\" Error\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "AuthenticationError: Incorrect API key provided\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Verify your API key is correct (no extra spaces)\n",
    "2. Check if key is properly set in Colab Secrets\n",
    "3. Ensure key hasn't been revoked in OpenAI dashboard\n",
    "4. Try regenerating the API key\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 2: \"Model Not Found\" Error\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "NotFoundError: No such model: gpt-5-nano\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Update `OPENAI_MODEL` variable to a valid model (e.g., \"gpt-5-nano\")\n",
    "2. Check OpenAI's current model availability\n",
    "3. Verify your account has access to the model\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 3: Rate Limits\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "RateLimitError: Rate limit exceeded\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Wait 60 seconds and try again\n",
    "2. Implement the robust error handling shown above\n",
    "3. Upgrade your OpenAI account tier for higher limits\n",
    "4. Batch requests instead of making many rapid calls\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 4: File Not Found (Images/Audio)\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "FileNotFoundError: [Errno 2] No such file or directory: '/content/image.png'\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Check the file path is correct\n",
    "2. Ensure file was uploaded to Colab\n",
    "3. Use absolute paths (starting with `/content/`)\n",
    "4. Add file existence check: `os.path.exists(file_path)`\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 5: Colab Disconnection\n",
    "\n",
    "**Symptoms:**\n",
    "- Notebook stops responding\n",
    "- \"Reconnecting...\" message\n",
    "\n",
    "**Solutions:**\n",
    "1. Keep your browser tab active\n",
    "2. Run cells periodically to keep session alive\n",
    "3. Save important output to files regularly\n",
    "4. Consider upgrading to Colab Pro for longer runtimes\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 6: Out of Memory\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ResourceExhausted: Out of memory\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Process large files in smaller chunks\n",
    "2. Delete large variables when done: `del variable_name`\n",
    "3. Restart runtime: Runtime ‚Üí Restart runtime\n",
    "4. Compress images before processing\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 7: Module Import Errors\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "ModuleNotFoundError: No module named 'openai'\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "1. Re-run the installation cell: `!pip install -q openai pillow requests`\n",
    "2. Restart runtime if installation was incomplete\n",
    "3. Check for typos in import statements\n",
    "\n",
    "---\n",
    "\n",
    "## Getting Help\n",
    "\n",
    "If you're still stuck:\n",
    "\n",
    "1. **OpenAI Documentation**: https://platform.openai.com/docs\n",
    "2. **OpenAI Community Forum**: https://community.openai.com\n",
    "3. **Check API Status**: https://status.openai.com\n",
    "4. **Review Error Messages**: They usually contain helpful information\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Next Steps\n",
    "\n",
    "Congratulations on completing this tutorial! Here are some ways to continue your learning:\n",
    "\n",
    "## üìö Further Learning\n",
    "\n",
    "1. **Advanced Prompting Techniques**\n",
    "   - Few-shot learning (providing examples)\n",
    "   - Chain-of-thought prompting\n",
    "   - System prompt engineering\n",
    "\n",
    "2. **Function Calling**\n",
    "   - Make the AI call your custom functions\n",
    "   - Build more interactive systems\n",
    "   - Integrate with databases and APIs\n",
    "\n",
    "3. **Embeddings & Vector Search**\n",
    "   - Build semantic search systems\n",
    "   - Create knowledge bases\n",
    "   - Implement RAG (Retrieval Augmented Generation)\n",
    "\n",
    "4. **Fine-tuning**\n",
    "   - Customize models for specific use cases\n",
    "   - Train on your organization's data\n",
    "   - Optimize for domain-specific tasks\n",
    "\n",
    "## üõ†Ô∏è Practice Projects\n",
    "\n",
    "Try building these projects on your own:\n",
    "\n",
    "1. **Automated Email Responder** - Draft replies to common IT support emails\n",
    "2. **Knowledge Base Q&A Bot** - Answer questions using your documentation\n",
    "3. **Code Documentation Generator** - Create docs from code comments\n",
    "4. **Meeting Summarizer** - Transcribe and summarize recorded meetings\n",
    "5. **Alert Classifier** - Categorize and prioritize system alerts\n",
    "\n",
    "## üìñ Resources\n",
    "\n",
    "- **OpenAI Cookbook**: https://cookbook.openai.com\n",
    "- **API Reference**: https://platform.openai.com/docs/api-reference\n",
    "- **Best Practices**: https://platform.openai.com/docs/guides/production-best-practices\n",
    "- **Pricing**: https://openai.com/pricing\n",
    "\n",
    "## üí° Tips for Production Use\n",
    "\n",
    "When moving beyond experimentation:\n",
    "\n",
    "1. ‚úÖ Implement proper error handling and logging\n",
    "2. ‚úÖ Set up usage monitoring and alerts\n",
    "3. ‚úÖ Use environment variables for configuration\n",
    "4. ‚úÖ Implement rate limiting in your application\n",
    "5. ‚úÖ Cache results when appropriate\n",
    "6. ‚úÖ Test thoroughly with edge cases\n",
    "7. ‚úÖ Have human review for critical decisions\n",
    "8. ‚úÖ Stay updated with OpenAI's model releases\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ You're Ready!\n",
    "\n",
    "You now have the foundational knowledge to build AI-powered IT support tools. Start small, experiment often, and gradually build more complex systems.\n",
    "\n",
    "**Good luck with your AI journey!** üöÄ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
