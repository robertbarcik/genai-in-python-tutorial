{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-2BRwbDcRb7"
   },
   "source": [
    "# ğŸ› ï¸ Built-in Tools: Extending LLM Capabilities\n",
    "\n",
    "In this notebook, you'll learn how to use OpenAI's built-in tools that extend Large Language Models beyond text generation, giving them the ability to search the web, access files, execute code, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "1. **Understand the Tools ecosystem** - What built-in tools are available and when to use each\n",
    "2. **Use Web Search** - Enable LLMs to search the internet for up-to-date information\n",
    "3. **Connect to Remote MCP Servers** - Leverage the Model Context Protocol for external integrations\n",
    "4. **Implement File Search** - Query your own documents using vector-based search (RAG)\n",
    "5. **Use Code Interpreter** - Let the LLM write and execute Python code for data analysis\n",
    "6. **Understand Computer Use** - Learn about the cutting-edge capability for GUI automation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "**Important:** This notebook builds upon concepts from the Function Calling notebook (Notebook 11). While function calling is a type of tool, it allows you to define **custom functions** that the LLM can call. This notebook focuses on **built-in tools** provided by OpenAI that require no custom implementation.\n",
    "\n",
    "If you haven't completed the Function Calling notebook, we recommend doing so first to understand:\n",
    "- How tools work in general\n",
    "- The request-response cycle\n",
    "- Tool schemas and parameters\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Built-in Tools Overview\n",
    "\n",
    "OpenAI provides several built-in tools that you can enable with just a few lines of code:\n",
    "\n",
    "| Tool | Purpose | Use Case |\n",
    "|------|---------|----------|\n",
    "| **Web Search** | Search the internet | Current events, real-time data, fact-checking |\n",
    "| **Remote MCP** | Connect to external services | Third-party integrations, external APIs |\n",
    "| **File Search** | Query uploaded documents | Knowledge bases, documentation, RAG |\n",
    "| **Code Interpreter** | Execute Python code | Data analysis, calculations, visualizations |\n",
    "| **Computer Use** | Control computer interfaces | GUI automation, web browsing (advanced) |\n",
    "\n",
    "Unlike function calling where you define and implement functions yourself, these tools are **hosted by OpenAI** - you simply enable them and the model handles the rest.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCRjY0rRcRb9"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ”§ Setup\n",
    "\n",
    "Before we dive into the tools, let's set up our environment.\n",
    "\n",
    "## ğŸ“¦ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0nzd7zNcRb-"
   },
   "outputs": [],
   "source": [
    "# Install the OpenAI library\n",
    "!pip install -q openai\n",
    "\n",
    "# Suppress deprecation warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t09mzjuHcRb_"
   },
   "source": [
    "## ğŸ”‘ API Key Configuration\n",
    "\n",
    "You have two methods to provide your OpenAI API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the ğŸ”‘ icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lx2_Rby3cRb_"
   },
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"ğŸ’¡ To use Colab secrets: Go to ğŸ”‘ (left sidebar) â†’ Add new secret â†’ Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"âŒ ERROR: No API key provided!\")\n",
    "\n",
    "print(\"âœ… Authentication configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcVZaT48cRb_"
   },
   "source": [
    "## ğŸš€ Initialize OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bS5dyrocRb_"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Cost-efficient model with tool support\n",
    "# Note: Some tools like Computer Use require specific models (e.g., \"computer-use-preview\")\n",
    "\n",
    "print(\"âœ… OpenAI client initialized successfully!\")\n",
    "print(f\"ğŸ¤– Selected Model: {OPENAI_MODEL}\")\n",
    "print(\"\\nğŸ“ Ready to explore built-in tools!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6wxsgpjcRcA"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸŒ 1. Web Search\n",
    "\n",
    "## What is Web Search?\n",
    "\n",
    "The **Web Search** tool allows the LLM to search the internet for up-to-date information. This is incredibly useful because:\n",
    "\n",
    "- LLMs have a **knowledge cutoff date** - they don't know about recent events\n",
    "- Real-time information (weather, stock prices, news) requires live data\n",
    "- Fact-checking and verification often need current sources\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. You enable web search by adding `{\"type\": \"web_search_preview\"}` to the tools array\n",
    "2. When the model needs current information, it automatically searches the web\n",
    "3. Search results are incorporated into the model's response\n",
    "4. The model cites its sources in the response\n",
    "\n",
    "## ğŸ’¡ Key Points\n",
    "\n",
    "- **Automatic**: The model decides when to search based on the query\n",
    "- **Cited**: Responses include source URLs for verification\n",
    "- **Current**: Access to information beyond the training data cutoff\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Example: Basic Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSAD9LrRcRcA"
   },
   "outputs": [],
   "source": [
    "# Enable web search and ask about current events\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],  # Enable web search\n",
    "    input=\"What are the latest developments in AI this week?\"\n",
    ")\n",
    "\n",
    "print(\"ğŸŒ Web Search Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fhE6_yKcRcA"
   },
   "source": [
    "## ğŸ§ª Example: Real-Time Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEVEMvBlcRcA"
   },
   "outputs": [],
   "source": [
    "# Ask about something that requires real-time data\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What is the current weather in Tokyo, Japan?\"\n",
    ")\n",
    "\n",
    "print(\"ğŸŒ¤ï¸ Weather Query Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GmuXZBQcRcB"
   },
   "source": [
    "## ğŸ§ª Example: Fact-Checking with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8WT0ySKcRcB"
   },
   "outputs": [],
   "source": [
    "# Ask a question that requires verified information\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"Who won the most recent Nobel Prize in Physics and for what discovery?\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ† Nobel Prize Query:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoWJj-GscRcB"
   },
   "source": [
    "## ğŸ” Inspecting Web Search Results\n",
    "\n",
    "The response object contains detailed information about the web search, including the sources used. Let's examine the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSQIabftcRcC"
   },
   "outputs": [],
   "source": [
    "# Make a query and inspect the full response\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What are the top 3 most popular programming languages in 2025?\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Response Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸ“ Main Response:\")\n",
    "print(response.output_text)\n",
    "\n",
    "# Check for web search results in the output\n",
    "print(f\"\\nğŸ” Output Items:\")\n",
    "for i, item in enumerate(response.output):\n",
    "    print(f\"  {i+1}. Type: {item.type}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnryWtJUcRcC"
   },
   "source": [
    "## ğŸ§ª Example: Combining Web Search with Conversation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4pPg7T_cRcC"
   },
   "outputs": [],
   "source": [
    "# Use web search with a multi-turn conversation\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful research assistant. Use web search to find accurate, current information. Always cite your sources.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I'm researching electric vehicles. What are the best-selling EVs globally right now?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "print(\"ğŸš— EV Research Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMr1sObvcRcC"
   },
   "source": [
    "## ğŸ“Š Web Search: When to Use It\n",
    "\n",
    "| âœ… Good Use Cases | âŒ Not Ideal For |\n",
    "|-------------------|------------------|\n",
    "| Current events and news | Historical facts (model already knows) |\n",
    "| Real-time data (weather, stocks) | Creative writing |\n",
    "| Recent product releases | General knowledge questions |\n",
    "| Fact-checking claims | Private/internal information |\n",
    "| Research with citations | Tasks requiring no external data |\n",
    "\n",
    "## ğŸ’° Pricing Note\n",
    "\n",
    "Web search incurs additional costs beyond standard token pricing. Each search query has an associated cost, so use it judiciously for queries that truly need current information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wqvbbBVcRcC"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ”Œ 2. Remote MCP (Model Context Protocol)\n",
    "\n",
    "## What is MCP?\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard introduced by Anthropic in November 2024 and adopted by OpenAI in March 2025. It standardizes how AI systems integrate with external tools, systems, and data sources.\n",
    "\n",
    "Think of MCP as a **universal adapter** - instead of building custom integrations for every external service, MCP provides a standard interface that any AI model can use to communicate with any MCP-compatible server.\n",
    "\n",
    "## How Remote MCP Works\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Your App  â”‚â”€â”€â”€â”€â–¶â”‚   OpenAI API    â”‚â”€â”€â”€â”€â–¶â”‚  Remote MCP      â”‚\n",
    "â”‚             â”‚     â”‚   (Responses)   â”‚     â”‚  Server          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚                        â”‚\n",
    "                           â”‚   Tools are listed     â”‚\n",
    "                           â”‚   and called           â”‚\n",
    "                           â”‚   automatically        â”‚\n",
    "                           â–¼                        â–¼\n",
    "                    Model decides          External service\n",
    "                    when to use tools      executes actions\n",
    "```\n",
    "\n",
    "1. You specify an MCP server URL in the tools array\n",
    "2. OpenAI's runtime connects to the server and discovers available tools\n",
    "3. The model can then call these tools as needed\n",
    "4. Results are returned and incorporated into the response\n",
    "\n",
    "## ğŸ’¡ Key Benefits\n",
    "\n",
    "- **No Custom Code**: OpenAI handles the tool calling automatically\n",
    "- **Standardized**: Works with any MCP-compatible server\n",
    "- **Secure**: Authentication handled via headers\n",
    "- **Scalable**: Connect to multiple MCP servers simultaneously\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ Using the Fetch MCP Server\n",
    "\n",
    "For this demo, we'll use the **Fetch MCP Server** - a free, public MCP server that can fetch and process web content. This server:\n",
    "- Retrieves content from URLs\n",
    "- Converts HTML to markdown for easier processing\n",
    "- Requires no API key (beyond your OpenAI key)\n",
    "\n",
    "### Example: Fetching Web Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGZU8TlUcRcD"
   },
   "outputs": [],
   "source": [
    "# Using the Fetch MCP server to retrieve web content\n",
    "# This is a public MCP server that fetches URLs and converts HTML to markdown\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"fetch\",\n",
    "            \"server_url\": \"https://mcp.fetch.tools/sse\",\n",
    "            \"require_approval\": \"never\"  # Auto-approve tool calls for this demo\n",
    "        }\n",
    "    ],\n",
    "    input=\"Fetch the content from https://example.com and summarize what you find.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ”Œ MCP Fetch Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSzCC06UcRcD"
   },
   "source": [
    "## ğŸ” Understanding MCP Response Structure\n",
    "\n",
    "When using MCP, the response includes information about which tools were discovered and used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjuLzGYWcRcD"
   },
   "outputs": [],
   "source": [
    "# Make an MCP request and examine the response structure\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"fetch\",\n",
    "            \"server_url\": \"https://mcp.fetch.tools/sse\",\n",
    "            \"require_approval\": \"never\"\n",
    "        }\n",
    "    ],\n",
    "    input=\"Fetch https://httpbin.org/json and tell me what data it contains.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š MCP Response Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Examine each item in the output\n",
    "for i, item in enumerate(response.output):\n",
    "    print(f\"\\nğŸ“¦ Output Item {i+1}:\")\n",
    "    print(f\"   Type: {item.type}\")\n",
    "\n",
    "    # Check for MCP-specific items\n",
    "    if item.type == \"mcp_list_tools\":\n",
    "        print(f\"   Server: {item.server_label}\")\n",
    "        print(f\"   Tools discovered: {len(item.tools)}\")\n",
    "        for tool in item.tools:\n",
    "            print(f\"     - {tool.get('name', 'unnamed')}\")\n",
    "    elif item.type == \"mcp_call\":\n",
    "        print(f\"   Tool called: {item.name}\")\n",
    "    elif item.type == \"message\":\n",
    "        print(f\"   Content preview: {item.content[0].text[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nğŸ“ Final Response:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlX62IM0cRcD"
   },
   "source": [
    "## ğŸ§ª Example: Fetching Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9h8QctNcRcD"
   },
   "outputs": [],
   "source": [
    "# Use MCP to fetch and analyze documentation\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"fetch\",\n",
    "            \"server_url\": \"https://mcp.fetch.tools/sse\",\n",
    "            \"require_approval\": \"never\"\n",
    "        }\n",
    "    ],\n",
    "    input=\"Fetch the Python requests library documentation from https://requests.readthedocs.io/en/latest/ and give me a quick overview of its main features.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“š Documentation Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orSIifUXcRcD"
   },
   "source": [
    "## ğŸ¢ MCP in Production\n",
    "\n",
    "While we used the free Fetch server for this demo, in production you can connect to many other MCP servers:\n",
    "\n",
    "| MCP Server | Purpose | Requires API Key |\n",
    "|------------|---------|------------------|\n",
    "| Fetch | Web content retrieval | No |\n",
    "| GitMCP | GitHub repository access | No (public repos) |\n",
    "| Zapier | Workflow automation | Yes (Zapier) |\n",
    "| Stripe | Payment processing | Yes (Stripe) |\n",
    "| Shopify | E-commerce data | Yes (Shopify) |\n",
    "| Twilio | Messaging/SMS | Yes (Twilio) |\n",
    "\n",
    "### Adding Authentication\n",
    "\n",
    "For servers that require authentication, you can pass headers:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"type\": \"mcp\",\n",
    "    \"server_label\": \"my_service\",\n",
    "    \"server_url\": \"https://mcp.myservice.com/sse\",\n",
    "    \"headers\": {\n",
    "        \"Authorization\": \"Bearer YOUR_API_KEY\"\n",
    "    },\n",
    "    \"require_approval\": \"never\"\n",
    "}\n",
    "```\n",
    "\n",
    "## ğŸ’° Pricing Note\n",
    "\n",
    "There is **no additional cost** to call remote MCP servers through OpenAI - you're simply billed for the output tokens as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYrs4weBcRcD"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ 3. File Search (Vector Store RAG)\n",
    "\n",
    "## What is File Search?\n",
    "\n",
    "**File Search** is OpenAI's built-in Retrieval-Augmented Generation (RAG) solution. It allows you to:\n",
    "\n",
    "1. Upload your own documents (PDFs, text files, etc.)\n",
    "2. OpenAI automatically chunks, embeds, and indexes them\n",
    "3. Query your documents using natural language\n",
    "4. Get answers grounded in your specific content\n",
    "\n",
    "This is essentially **hosted RAG** - OpenAI handles all the complexity of:\n",
    "- Text chunking\n",
    "- Embedding generation\n",
    "- Vector storage\n",
    "- Similarity search\n",
    "\n",
    "## How It Works\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Your Files â”‚â”€â”€â”€â”€â–¶â”‚  Vector Store   â”‚â”€â”€â”€â”€â–¶â”‚  File Search     â”‚\n",
    "â”‚  (upload)   â”‚     â”‚  (auto-indexed) â”‚     â”‚  Tool            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚                        â”‚\n",
    "                           â”‚   Chunks stored        â”‚  User query\n",
    "                           â”‚   as embeddings        â”‚  searches\n",
    "                           â–¼                        â–¼\n",
    "                    Automatic               Relevant chunks\n",
    "                    processing              added to context\n",
    "```\n",
    "\n",
    "## ğŸ’¡ Key Benefits\n",
    "\n",
    "- **No Infrastructure**: No need to set up your own vector database\n",
    "- **Automatic Processing**: Files are chunked and embedded automatically\n",
    "- **Simple API**: Just upload files and query\n",
    "- **Scalable**: Handles large document collections\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¤ Step 1: Upload Files\n",
    "\n",
    "First, we need to upload our demo files. For this tutorial, we have three files about a fictional company \"TechStart Inc.\":\n",
    "- `company_policies.txt` - HR policies and guidelines\n",
    "- `product_catalog.txt` - Product descriptions and pricing\n",
    "- `faq.txt` - Frequently asked questions\n",
    "\n",
    "**Note:** You'll need to upload these files to Google Colab first. Use the file browser on the left sidebar to upload the files from the `demo_files` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff7X4AJDcRcE"
   },
   "outputs": [],
   "source": [
    "# First, let's upload our demo files to Colab\n",
    "# Run this cell and select the three .txt files from the demo_files folder\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ğŸ“¤ Please upload the demo files:\")\n",
    "print(\"   - company_policies.txt\")\n",
    "print(\"   - product_catalog.txt\")\n",
    "print(\"   - faq.txt\")\n",
    "print(\"\\nClick 'Choose Files' below to select them...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f\"\\nâœ… Uploaded {len(uploaded)} file(s):\")\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"   - {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE8T0yWBcRcE"
   },
   "source": [
    "## ğŸ“¤ Step 2: Upload Files to OpenAI\n",
    "\n",
    "Now we upload the files to OpenAI's storage. This makes them available for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCHpALsucRcE"
   },
   "outputs": [],
   "source": [
    "# Upload files to OpenAI\n",
    "file_paths = [\"company_policies.txt\", \"product_catalog.txt\", \"faq.txt\"]\n",
    "uploaded_files = []\n",
    "\n",
    "print(\"ğŸ“¤ Uploading files to OpenAI...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file = client.files.create(\n",
    "                file=f,\n",
    "                purpose=\"assistants\"  # Purpose for file search\n",
    "            )\n",
    "            uploaded_files.append(file)\n",
    "            print(f\"âœ… Uploaded: {file_path}\")\n",
    "            print(f\"   File ID: {file.id}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        print(\"   Please upload the demo files first (run the previous cell)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸ“ Total files uploaded: {len(uploaded_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cuswx6k3cRcE"
   },
   "source": [
    "## ğŸ—„ï¸ Step 3: Create a Vector Store\n",
    "\n",
    "A Vector Store is a container that holds your indexed files. When you add files to a vector store, OpenAI automatically:\n",
    "1. Chunks the text into smaller pieces\n",
    "2. Creates embeddings for each chunk\n",
    "3. Stores everything for efficient retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZTiRetpcRcE"
   },
   "outputs": [],
   "source": [
    "# Create a vector store\n",
    "print(\"ğŸ—„ï¸ Creating Vector Store...\")\n",
    "\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"TechStart Knowledge Base\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Vector Store created!\")\n",
    "print(f\"   ID: {vector_store.id}\")\n",
    "print(f\"   Name: {vector_store.name}\")\n",
    "print(f\"   Status: {vector_store.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRAuTXZocRcE"
   },
   "source": [
    "## ğŸ“ Step 4: Add Files to the Vector Store\n",
    "\n",
    "Now we attach our uploaded files to the vector store for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PmRj8QFcRcE"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Add files to the vector store\n",
    "print(\"ğŸ“ Adding files to Vector Store...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for file in uploaded_files:\n",
    "    client.vector_stores.files.create(\n",
    "        vector_store_id=vector_store.id,\n",
    "        file_id=file.id\n",
    "    )\n",
    "    print(f\"   Added: {file.filename}\")\n",
    "\n",
    "# Wait for processing to complete\n",
    "print(\"\\nâ³ Waiting for files to be processed...\")\n",
    "\n",
    "while True:\n",
    "    vs_status = client.vector_stores.retrieve(vector_store.id)\n",
    "    file_counts = vs_status.file_counts\n",
    "\n",
    "    print(f\"   Status: {file_counts.completed}/{file_counts.total} files processed\", end=\"\\r\")\n",
    "\n",
    "    if file_counts.completed == file_counts.total:\n",
    "        break\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nâœ… All files processed and indexed!\")\n",
    "print(f\"   Total chunks: {vs_status.usage.total_count if hasattr(vs_status.usage, 'total_count') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0QVJQuFcRcF"
   },
   "source": [
    "## ğŸ” Step 5: Query Your Documents\n",
    "\n",
    "Now comes the exciting part - querying your documents using natural language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIQH0iTBcRcF"
   },
   "outputs": [],
   "source": [
    "# Store the vector store ID for use in queries\n",
    "VECTOR_STORE_ID = vector_store.id\n",
    "\n",
    "def search_documents(query):\n",
    "    \"\"\"Search documents using the file search tool.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"file_search\",\n",
    "                \"vector_store_ids\": [VECTOR_STORE_ID]\n",
    "            }\n",
    "        ],\n",
    "        input=query\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "print(\"âœ… Search function ready!\")\n",
    "print(f\"ğŸ“ Using Vector Store: {VECTOR_STORE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaS90mtzcRcF"
   },
   "source": [
    "## ğŸ§ª Example Queries\n",
    "\n",
    "Let's test our document search with various questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4Wju_lmcRcF"
   },
   "outputs": [],
   "source": [
    "# Query 1: HR Policy Question\n",
    "print(\"ğŸ“‹ Query 1: Vacation Policy\")\n",
    "print(\"=\" * 60)\n",
    "result = search_documents(\"How many vacation days do employees get per year?\")\n",
    "print(result)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6GIJ-o9cRcF"
   },
   "outputs": [],
   "source": [
    "# Query 2: Product Information\n",
    "print(\"ğŸ’° Query 2: Product Pricing\")\n",
    "print(\"=\" * 60)\n",
    "result = search_documents(\"What is the pricing for DataFlow Pro?\")\n",
    "print(result)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJAUnLJtcRcF"
   },
   "outputs": [],
   "source": [
    "# Query 3: Technical Question\n",
    "print(\"ğŸ” Query 3: Security Features\")\n",
    "print(\"=\" * 60)\n",
    "result = search_documents(\"What MFA methods are supported by SecureAuth 360?\")\n",
    "print(result)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hWuplb0cRcF"
   },
   "outputs": [],
   "source": [
    "# Query 4: FAQ Question\n",
    "print(\"â“ Query 4: Account Management\")\n",
    "print(\"=\" * 60)\n",
    "result = search_documents(\"How do I reset my password if I forgot it?\")\n",
    "print(result)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lmd62m-RcRcG"
   },
   "outputs": [],
   "source": [
    "# Query 5: Cross-document query\n",
    "print(\"ğŸ”„ Query 5: Cross-Document Search\")\n",
    "print(\"=\" * 60)\n",
    "result = search_documents(\"What are the remote work requirements and what equipment is provided to remote workers?\")\n",
    "print(result)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzXCXMYpcRcG"
   },
   "source": [
    "## ğŸ§¹ Cleanup: Delete Vector Store (Optional)\n",
    "\n",
    "When you're done, you can delete the vector store to avoid storage charges. Remember, the first 1GB is free, but it's good practice to clean up resources you're not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rcr9siFmcRcG"
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to delete the vector store\n",
    "# This will delete all indexed data - only do this when you're done!\n",
    "\n",
    "# client.vector_stores.delete(VECTOR_STORE_ID)\n",
    "# print(f\"ğŸ—‘ï¸ Vector Store {VECTOR_STORE_ID} deleted!\")\n",
    "\n",
    "print(\"ğŸ’¡ To delete the vector store, uncomment the lines above and run this cell.\")\n",
    "print(f\"   Vector Store ID: {VECTOR_STORE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27ntrK3GcRcG"
   },
   "source": [
    "## ğŸ’° File Search Pricing\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Vector Storage | First 1GB free, then $0.10/GB/day |\n",
    "| Search Queries | $2.50 per 1,000 queries |\n",
    "\n",
    "For a tutorial with small files like ours (< 30KB total), storage is essentially free. You only pay for the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgxPGxvrcRcG"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ’» 4. Code Interpreter\n",
    "\n",
    "## What is Code Interpreter?\n",
    "\n",
    "**Code Interpreter** allows the LLM to write and execute Python code in a secure, sandboxed environment. This is incredibly powerful for:\n",
    "\n",
    "- **Data Analysis**: Process CSV files, calculate statistics, find patterns\n",
    "- **Math & Calculations**: Solve complex equations, perform numerical analysis\n",
    "- **Visualizations**: Create charts and graphs\n",
    "- **File Processing**: Transform, clean, and manipulate data\n",
    "- **Problem Solving**: Write and test code to solve programming challenges\n",
    "\n",
    "## How It Works\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   User      â”‚â”€â”€â”€â”€â–¶â”‚   LLM writes    â”‚â”€â”€â”€â”€â–¶â”‚  Sandboxed       â”‚\n",
    "â”‚   Query     â”‚     â”‚   Python code   â”‚     â”‚  Python Runtime  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚                        â”‚\n",
    "                           â”‚   Code generation      â”‚  Code execution\n",
    "                           â”‚                        â”‚\n",
    "                           â–¼                        â–¼\n",
    "                    Model refines            Results returned\n",
    "                    if errors occur          (data, charts, files)\n",
    "```\n",
    "\n",
    "## ğŸ’¡ Key Features\n",
    "\n",
    "- **Iterative**: If code fails, the model can debug and retry\n",
    "- **Libraries**: Access to pandas, numpy, matplotlib, and more\n",
    "- **File Output**: Can generate files (images, CSVs) that you can download\n",
    "- **Secure**: Code runs in an isolated sandbox\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Example: Mathematical Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcukzsZpcRcG"
   },
   "outputs": [],
   "source": [
    "# Use code interpreter for complex calculations\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    input=\"Calculate the first 20 Fibonacci numbers and find their sum. Also calculate the golden ratio approximation using the last two numbers.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¢ Mathematical Calculation:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0kcyg3ncRcG"
   },
   "source": [
    "## ğŸ” Inspecting Code Interpreter Output\n",
    "\n",
    "Let's examine what happens behind the scenes when Code Interpreter runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeT8zXaycRcG"
   },
   "outputs": [],
   "source": [
    "# Make a request and inspect the code that was executed\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    input=\"Generate 100 random numbers between 1 and 1000, then calculate the mean, median, standard deviation, and find the top 5 largest numbers.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Code Interpreter Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Examine the output items to see the code execution\n",
    "for i, item in enumerate(response.output):\n",
    "    print(f\"\\nğŸ“¦ Output Item {i+1}: {item.type}\")\n",
    "\n",
    "    if item.type == \"code_interpreter_call\":\n",
    "        print(f\"\\n   ğŸ“ Code Executed:\")\n",
    "        print(\"   \" + \"-\" * 40)\n",
    "        # The code is in the input field\n",
    "        if hasattr(item, 'input'):\n",
    "            for line in item.input.split('\\n'):\n",
    "                print(f\"   {line}\")\n",
    "        print(\"   \" + \"-\" * 40)\n",
    "\n",
    "        # Show the output/result\n",
    "        if hasattr(item, 'output'):\n",
    "            print(f\"\\n   ğŸ“¤ Output:\")\n",
    "            print(f\"   {item.output[:500]}...\" if len(str(item.output)) > 500 else f\"   {item.output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nğŸ“ Final Response:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNvIbYk7cRcG"
   },
   "source": [
    "## ğŸ§ª Example: Data Analysis with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2-0W-UdcRcG"
   },
   "outputs": [],
   "source": [
    "# Create sample sales data for analysis\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    input=\"\"\"Create a sample dataset of monthly sales data for a company over 2 years (24 months).\n",
    "    Include columns: month, sales_amount, expenses, and region (North, South, East, West).\n",
    "    Then analyze the data to find:\n",
    "    1. Total sales and expenses per region\n",
    "    2. Average monthly profit (sales - expenses)\n",
    "    3. The best and worst performing months\n",
    "    4. Which region has the highest profit margin\"\"\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ˆ Sales Data Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyEpfLw7cRcG"
   },
   "source": [
    "## ğŸ§ª Example: Creating Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sc1xDuWcRcH"
   },
   "outputs": [],
   "source": [
    "# Ask code interpreter to create a visualization\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    input=\"\"\"Create a visualization showing the growth of major social media platforms from 2010 to 2024.\n",
    "    Use estimated user numbers (in billions) for Facebook, YouTube, Instagram, TikTok, and Twitter/X.\n",
    "    Create a line chart with clear labels and a legend.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Visualization Created:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "\n",
    "# Check if there are any file outputs (images)\n",
    "for item in response.output:\n",
    "    if hasattr(item, 'type') and 'file' in item.type.lower():\n",
    "        print(f\"\\nğŸ–¼ï¸ Generated file: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT2kEynrcRcH"
   },
   "source": [
    "## ğŸ§ª Example: Solving Programming Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuwUqj8IcRcH"
   },
   "outputs": [],
   "source": [
    "# Ask code interpreter to solve a programming challenge\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    input=\"\"\"Write a Python function to check if a number is a prime number.\n",
    "    Then use it to find all prime numbers between 1 and 100.\n",
    "    Finally, calculate what percentage of numbers from 1-100 are prime.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¢ Prime Number Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIC_eGECcRcH"
   },
   "source": [
    "## ğŸ§ª Example: Working with Uploaded Files\n",
    "\n",
    "Code Interpreter can also process files you upload. Let's create a sample CSV and analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bPyi3occRcH"
   },
   "outputs": [],
   "source": [
    "# First, create a sample CSV file locally\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Generate sample employee data\n",
    "departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance']\n",
    "data = []\n",
    "\n",
    "for i in range(50):\n",
    "    data.append({\n",
    "        'employee_id': f'EMP{1000 + i}',\n",
    "        'department': random.choice(departments),\n",
    "        'salary': random.randint(50000, 150000),\n",
    "        'years_experience': random.randint(1, 20),\n",
    "        'performance_score': round(random.uniform(2.0, 5.0), 1)\n",
    "    })\n",
    "\n",
    "# Write to CSV\n",
    "with open('employee_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"âœ… Created employee_data.csv with 50 records\")\n",
    "print(\"\\nPreview:\")\n",
    "for row in data[:5]:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oL-c79-ocRcH"
   },
   "outputs": [],
   "source": [
    "# Upload the CSV to OpenAI\n",
    "with open('employee_data.csv', 'rb') as f:\n",
    "    csv_file = client.files.create(\n",
    "        file=f,\n",
    "        purpose='assistants'\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Uploaded CSV file\")\n",
    "print(f\"   File ID: {csv_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pjyqRY2cRcH"
   },
   "outputs": [],
   "source": [
    "# Use code interpreter to analyze the uploaded file\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"code_interpreter\",\n",
    "            \"file_ids\": [csv_file.id]  # Attach the file to code interpreter\n",
    "        }\n",
    "    ],\n",
    "    input=\"\"\"Analyze the employee data CSV file. Please provide:\n",
    "    1. Basic statistics (count, mean, median) for salary and years_experience\n",
    "    2. Average salary by department\n",
    "    3. Correlation between years_experience and salary\n",
    "    4. Top 5 highest paid employees\n",
    "    5. Which department has the highest average performance score?\"\"\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Employee Data Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(response.output_text)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sIL-Pd0cRcH"
   },
   "source": [
    "## ğŸ’° Code Interpreter Pricing\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Container Session | $0.03 per session |\n",
    "| Session Duration | Active for 1 hour, 30-minute idle timeout |\n",
    "\n",
    "Each request that uses Code Interpreter starts a container session. The session remains active for subsequent requests within the timeout period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4Gg9NYTcRcH"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ–¥ï¸ 5. Computer Use (Preview)\n",
    "\n",
    "## What is Computer Use?\n",
    "\n",
    "**Computer Use** (also called Computer-Using Agent or CUA) is OpenAI's most advanced tool that allows AI models to interact with graphical user interfaces (GUIs) - just like a human would.\n",
    "\n",
    "The model can:\n",
    "- View screenshots of a computer screen\n",
    "- Control mouse movements and clicks\n",
    "- Type on a virtual keyboard\n",
    "- Navigate websites, fill forms, and interact with applications\n",
    "\n",
    "## How It Works\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Screenshot â”‚â”€â”€â”€â”€â–¶â”‚   CUA Model     â”‚â”€â”€â”€â”€â–¶â”‚  Action Output   â”‚\n",
    "â”‚  (pixels)   â”‚     â”‚   (analyzes)    â”‚     â”‚  (click, type)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â–²                                           â”‚\n",
    "       â”‚                                           â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    Loop until task complete\n",
    "```\n",
    "\n",
    "1. Your application captures a screenshot\n",
    "2. The screenshot is sent to the CUA model\n",
    "3. The model analyzes the image and decides what action to take\n",
    "4. It returns an action (click coordinates, text to type, etc.)\n",
    "5. Your application executes the action and captures a new screenshot\n",
    "6. The loop continues until the task is complete\n",
    "\n",
    "## ğŸ’¡ Key Capabilities\n",
    "\n",
    "- **Vision-Based**: Works with any GUI, no special APIs needed\n",
    "- **Adaptive**: Can handle errors and unexpected situations\n",
    "- **Multi-Step**: Completes complex, multi-step workflows\n",
    "- **Universal**: Works on websites, desktop apps, and more\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”’ Current Limitations\n",
    "\n",
    "Computer Use is currently in **research preview** with some restrictions:\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|----------|\n",
    "| **Availability** | Usage tiers 3-5 only |\n",
    "| **Pricing** | $3/1M input tokens, $12/1M output tokens |\n",
    "| **Infrastructure** | Requires you to provide the computer environment |\n",
    "| **Safety** | Recommended to use isolated VMs or containers |\n",
    "\n",
    "## ğŸ“ API Structure (Reference)\n",
    "\n",
    "Here's what a Computer Use API call looks like (not executable in this notebook):\n",
    "\n",
    "```python\n",
    "# NOTE: This is for reference only - requires special access and infrastructure\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"computer-use-preview\",  # Special model for computer use\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"computer_use_preview\",\n",
    "            \"display_width\": 1920,\n",
    "            \"display_height\": 1080,\n",
    "            \"environment\": \"browser\"  # or \"desktop\"\n",
    "        }\n",
    "    ],\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Go to google.com and search for 'OpenAI API documentation'\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/png\",\n",
    "                        \"data\": \"<base64-encoded-screenshot>\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The response contains actions like:\n",
    "# - {\"type\": \"click\", \"x\": 500, \"y\": 300}\n",
    "# - {\"type\": \"type\", \"text\": \"OpenAI API documentation\"}\n",
    "# - {\"type\": \"key\", \"key\": \"enter\"}\n",
    "# - {\"type\": \"scroll\", \"direction\": \"down\", \"amount\": 3}\n",
    "```\n",
    "\n",
    "## ğŸŒŸ Use Cases\n",
    "\n",
    "Computer Use is ideal for:\n",
    "\n",
    "1. **Browser Automation**: Form filling, web scraping, testing\n",
    "2. **Desktop Automation**: Interacting with legacy applications\n",
    "3. **Data Entry**: Automating repetitive data entry tasks\n",
    "4. **Testing**: End-to-end UI testing without writing test scripts\n",
    "5. **Personal Assistants**: Booking travel, managing calendars, online shopping\n",
    "\n",
    "## ğŸ”— Resources\n",
    "\n",
    "For more information about Computer Use:\n",
    "\n",
    "- [OpenAI Computer-Using Agent](https://openai.com/index/computer-using-agent/)\n",
    "- [OpenAI CUA Sample App](https://github.com/openai/openai-cua-sample-app)\n",
    "- [Operator (Consumer Product)](https://openai.com/index/introducing-operator/)\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: As of July 2025, Operator (the consumer-facing version of Computer Use) has been integrated into ChatGPT as \"agent mode\", making these capabilities more accessible to end users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6n6kDqycRcI"
   },
   "source": [
    "---\n",
    "\n",
    "# ğŸ“š Summary and Best Practices\n",
    "\n",
    "## Tool Comparison Matrix\n",
    "\n",
    "| Tool | Best For | Complexity | Cost |\n",
    "|------|----------|------------|------|\n",
    "| **Web Search** | Current events, real-time data | Low | Per query |\n",
    "| **Remote MCP** | External integrations | Medium | Tokens only |\n",
    "| **File Search** | Document Q&A, knowledge bases | Medium | Storage + queries |\n",
    "| **Code Interpreter** | Data analysis, calculations | Low | Per session |\n",
    "| **Computer Use** | GUI automation, complex workflows | High | Premium pricing |\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Choose the Right Tool\n",
    "\n",
    "```\n",
    "Need current information?      â†’ Web Search\n",
    "Need external service data?    â†’ Remote MCP\n",
    "Need to query your documents?  â†’ File Search\n",
    "Need calculations/analysis?    â†’ Code Interpreter\n",
    "Need GUI interaction?          â†’ Computer Use\n",
    "Need custom logic?             â†’ Function Calling (Notebook 11)\n",
    "```\n",
    "\n",
    "### 2. Combine Tools When Needed\n",
    "\n",
    "You can enable multiple tools in a single request:\n",
    "\n",
    "```python\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[\n",
    "        {\"type\": \"web_search_preview\"},\n",
    "        {\"type\": \"code_interpreter\"}\n",
    "    ],\n",
    "    input=\"Search for the latest stock price of Apple and calculate the P/E ratio.\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Consider Costs\n",
    "\n",
    "- Web Search and Code Interpreter have per-use charges\n",
    "- File Search has storage costs (first 1GB free)\n",
    "- Remote MCP is token-cost only\n",
    "- Computer Use is significantly more expensive\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Additional Resources\n",
    "\n",
    "- [OpenAI Tools Documentation](https://platform.openai.com/docs/guides/tools)\n",
    "- [OpenAI Cookbook](https://cookbook.openai.com/)\n",
    "- [MCP Tool Guide](https://cookbook.openai.com/examples/mcp/mcp_tool_guide)\n",
    "- [File Search Guide](https://platform.openai.com/docs/guides/tools-file-search)\n",
    "- [Code Interpreter Guide](https://platform.openai.com/docs/guides/tools-code-interpreter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2NQbC2ucRcI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
