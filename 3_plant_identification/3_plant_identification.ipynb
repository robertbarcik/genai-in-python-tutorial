{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå∏ Plant Identification with Multimodal AI\n",
    "\n",
    "Welcome to Exercise #5 in the Generative AI course! In this notebook, you'll learn how to use multimodal AI to identify plants from images and create annotated visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Use multimodal AI** - Work with models that understand both text and images\n",
    "2. **Identify plants from photos** - Leverage vision models for species recognition\n",
    "3. **Annotate images programmatically** - Add text overlays to create informative visuals\n",
    "4. **Build practical AI applications** - Combine multiple techniques into a complete workflow\n",
    "\n",
    "---\n",
    "\n",
    "## üíº Business Value: Why Multimodal AI Matters\n",
    "\n",
    "Multimodal AI has transformative applications across industries:\n",
    "\n",
    "- **üåæ Agriculture**: Automated crop disease detection and species monitoring\n",
    "- **üåø Biodiversity Research**: Large-scale plant surveys and conservation tracking\n",
    "- **üè™ Retail**: Plant identification apps for garden centers and nurseries\n",
    "- **üì± Consumer Apps**: Educational tools for hikers, gardeners, and nature enthusiasts\n",
    "- **üî¨ Scientific Research**: Accelerating botanical research and documentation\n",
    "\n",
    "Traditional computer vision required thousands of labeled images and custom model training. Modern multimodal AI can identify plants with just a simple API call!\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß† Theory: Understanding Multimodal AI\n",
    "\n",
    "## What is Multimodal AI?\n",
    "\n",
    "**Multimodal AI** refers to artificial intelligence systems that can process and understand multiple types of data simultaneously - such as text, images, audio, and video. Unlike traditional AI models that work with only one type of input, multimodal models can \"see\" images and \"read\" text, enabling them to understand context in ways that mirror human perception.\n",
    "\n",
    "Vision-capable language models like GPT-4o and GPT-5-nano combine two powerful capabilities:\n",
    "1. **Computer Vision**: Understanding what's in an image (objects, scenes, text, colors, spatial relationships)\n",
    "2. **Natural Language Processing**: Describing findings in human language and following text instructions\n",
    "\n",
    "This combination is revolutionary because it allows us to ask questions about images in plain English and receive detailed, contextual answers.\n",
    "\n",
    "## How Do Vision Models Work?\n",
    "\n",
    "At a high level, vision-language models work through these stages:\n",
    "\n",
    "1. **Image Encoding**: The image is processed through neural networks that identify visual features (edges, shapes, textures, objects)\n",
    "2. **Feature Extraction**: Important visual elements are converted into numerical representations that capture their meaning\n",
    "3. **Multimodal Fusion**: Visual features are combined with text input (your prompt) in a shared representation space\n",
    "4. **Language Generation**: The model generates a text response based on both the visual and textual understanding\n",
    "\n",
    "Modern vision models are trained on millions of image-text pairs, learning associations between visual concepts and language descriptions. This training enables them to recognize objects, scenes, and even abstract concepts they've never explicitly been taught about.\n",
    "\n",
    "## Real-World Use Cases Beyond Plant Identification\n",
    "\n",
    "- **üè• Healthcare**: Medical image analysis, X-ray interpretation, skin condition diagnosis\n",
    "- **üöó Automotive**: Autonomous vehicle perception, road sign recognition, obstacle detection\n",
    "- **üè™ Retail**: Visual search, product recognition, automated inventory management\n",
    "- **‚ôø Accessibility**: Describing images for visually impaired users, reading text in photos\n",
    "- **üîç Content Moderation**: Detecting inappropriate images, verifying content authenticity\n",
    "- **üèóÔ∏è Manufacturing**: Quality control, defect detection, assembly verification\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "- ‚úÖ **Multimodal AI processes multiple data types** (text + images) simultaneously\n",
    "- ‚úÖ **Vision models understand images** through neural networks trained on millions of examples\n",
    "- ‚úÖ **Natural language prompts control behavior** - you can ask questions about images in plain English\n",
    "- ‚úÖ **No custom training required** - pre-trained models work out-of-the-box for many tasks\n",
    "- ‚úÖ **Applications span industries** from healthcare to agriculture to accessibility\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Key Point: Prompt Engineering for Vision Tasks\n",
    "\n",
    "When working with vision models, **prompt precision matters**. Vague prompts like \"What is this?\" may yield verbose or unfocused responses. Specific prompts like \"Identify the type of flower in this image. Provide only the common name.\" produce concise, actionable results. We'll explore this principle hands-on in the identification section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Setup\n",
    "\n",
    "Before we can identify plants, we need to:\n",
    "1. Configure our OpenAI API key for authentication\n",
    "2. Install required Python packages for image processing and API communication\n",
    "3. Import all necessary libraries\n",
    "\n",
    "Let's start with API configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies\n",
    "\n",
    "We need several Python packages:\n",
    "- **openai**: Official OpenAI Python client for API access\n",
    "- **opencv-python**: Image processing library for loading, manipulating, and annotating images\n",
    "- **matplotlib**: Visualization library for displaying images in the notebook\n",
    "- **numpy**: Numerical computing library for array operations\n",
    "- **requests**: HTTP library for downloading images from URLs\n",
    "- **pillow**: Additional image processing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai opencv-python matplotlib requests numpy pillow\n",
    "\n",
    "# Suppress deprecation warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API client for vision and language models\n",
    "from openai import OpenAI\n",
    "\n",
    "# Image processing with OpenCV\n",
    "import cv2\n",
    "\n",
    "# Visualization with Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical operations with NumPy\n",
    "import numpy as np\n",
    "\n",
    "# HTTP requests for downloading images\n",
    "import requests\n",
    "\n",
    "# System and OS utilities\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üéâ Setup complete - ready to identify plants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì∏ Image Loading and Display\n",
    "\n",
    "Before we can identify plants, we need to load our flower images and display them. We'll work with two flower images hosted online.\n",
    "\n",
    "In this section, we'll:\n",
    "1. Define the URLs for our flower images\n",
    "2. Create a reusable function to download and display images\n",
    "3. View both flowers in their original form\n",
    "\n",
    "Let's start by defining our image sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Define Image URLs\n",
    "\n",
    "We'll use two flower images from Wikimedia Commons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the flower images we'll identify\n",
    "flower_url_1 = \"https://upload.wikimedia.org/wikipedia/commons/6/6f/Path_krupina.jpg\"\n",
    "flower_url_2 = \"https://upload.wikimedia.org/wikipedia/commons/2/26/Path_krupina_2.jpg\"\n",
    "\n",
    "print(\"‚úÖ Image URLs configured:\")\n",
    "print(f\"   Flower 1: {flower_url_1}\")\n",
    "print(f\"   Flower 2: {flower_url_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Create Image Display Function\n",
    "\n",
    "Let's create a reusable function that downloads an image from a URL and displays it in the notebook. This function will handle:\n",
    "- Downloading the image using proper HTTP headers\n",
    "- Decoding the image data with OpenCV\n",
    "- Converting from BGR (OpenCV default) to RGB (display format)\n",
    "- Rendering the image with Matplotlib\n",
    "- Error handling for network or image issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_url(url):\n",
    "    \"\"\"\n",
    "    Download and display an image from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the image to display\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The image array in RGB format, or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the image from URL with proper headers\n",
    "        # User-Agent header helps avoid blocking from some servers\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        # Convert the response content to a numpy array\n",
    "        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "        \n",
    "        # Decode the image array into OpenCV format (BGR color space)\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"‚ùå Failed to decode image from {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert from BGR (OpenCV default) to RGB (standard display format)\n",
    "        # OpenCV reads images in BGR, but matplotlib expects RGB\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display the image using matplotlib\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.axis('off')  # Hide axis labels for cleaner display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return image_rgb\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error downloading image: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Image display function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ Display Flower Image 1\n",
    "\n",
    "Let's load and view our first flower image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì∏ Flower Image 1:\")\n",
    "print(f\"   Loading from: {flower_url_1}\")\n",
    "print()\n",
    "\n",
    "image_1 = display_image_from_url(flower_url_1)\n",
    "\n",
    "if image_1 is not None:\n",
    "    print(f\"‚úÖ Image loaded successfully! Dimensions: {image_1.shape[1]}x{image_1.shape[0]} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ Display Flower Image 2\n",
    "\n",
    "Now let's view our second flower image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì∏ Flower Image 2:\")\n",
    "print(f\"   Loading from: {flower_url_2}\")\n",
    "print()\n",
    "\n",
    "image_2 = display_image_from_url(flower_url_2)\n",
    "\n",
    "if image_2 is not None:\n",
    "    print(f\"‚úÖ Image loaded successfully! Dimensions: {image_2.shape[1]}x{image_2.shape[0]} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç Plant Identification with Vision AI\n",
    "\n",
    "Now comes the exciting part - using multimodal AI to identify the flowers! We'll send each image to the gpt-5-nano vision model along with a text prompt asking for identification.\n",
    "\n",
    "## How Vision API Calls Work\n",
    "\n",
    "When working with vision-capable models, we structure our API request with:\n",
    "1. **Model specification**: `gpt-5-nano` (cost-efficient vision model)\n",
    "2. **Multimodal input**: A combination of text prompt and image URL\n",
    "3. **Text configuration**: Optional verbosity settings to control response length\n",
    "\n",
    "The model analyzes the image and generates a text response based on what it \"sees\" and our instructions.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Key Point: Prompt Precision\n",
    "\n",
    "**Why specific prompts matter**: Vision models can provide detailed descriptions, scientific names, growing conditions, and more. If we ask \"What is this?\", we might get a paragraph of information. For our use case (annotating images), we need just the common name.\n",
    "\n",
    "**Our strategy**: We use the prompt \"Please identify the type of flower in this image. Only provide a common name of the flower, no other words.\" This instructs the model to give us a concise, single-piece of information perfect for image annotation.\n",
    "\n",
    "Compare these prompts:\n",
    "- ‚ùå Vague: \"What is this?\" ‚Üí Response: \"This appears to be a flowering plant, likely from the...\"\n",
    "- ‚úÖ Specific: \"Only provide a common name of the flower, no other words.\" ‚Üí Response: \"Daisy\"\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "- ‚úÖ **Vision models understand images** and can identify objects, plants, and scenes\n",
    "- ‚úÖ **Prompt engineering controls output format** - be specific about what you want\n",
    "- ‚úÖ **Input combines text and images** using a structured format\n",
    "- ‚úÖ **Response extraction is simple** - use `response.output_text` directly\n",
    "- ‚úÖ **Error handling is essential** - network issues and API errors can occur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Create Identification Function\n",
    "\n",
    "Let's create a function that takes an image URL and returns the identified flower name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_flower(url_to_flower):\n",
    "    \"\"\"\n",
    "    Identify a flower in an image using OpenAI's vision model.\n",
    "    \n",
    "    Args:\n",
    "        url_to_flower (str): URL of the flower image to identify\n",
    "    \n",
    "    Returns:\n",
    "        str: The common name of the identified flower, or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a multimodal API request with both text and image\n",
    "        # The input parameter accepts a structured format for combining modalities\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            # Text prompt - gives instructions to the model\n",
    "                            \"type\": \"input_text\",\n",
    "                            \"text\": \"Please identify the type of flower in this image. Only provide a common name of the flower, no other words.\"\n",
    "                        },\n",
    "                        {\n",
    "                            # Image input - the flower photo to analyze\n",
    "                            \"type\": \"input_image\",\n",
    "                            \"image_url\": url_to_flower\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            text={\"verbosity\": \"low\"}  # Low verbosity for concise responses\n",
    "        )\n",
    "        \n",
    "        # Extract the identification result from the response\n",
    "        # No complex parsing needed - just access the output_text attribute\n",
    "        flower_name = response.output_text.strip()\n",
    "        \n",
    "        return flower_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any errors (API errors, network issues, etc.)\n",
    "        print(f\"‚ùå Error identifying flower: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Flower identification function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ Identify Flower 1\n",
    "\n",
    "Let's identify the first flower using our vision model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Identifying Flower 1...\")\n",
    "print(f\"   Sending to {OPENAI_MODEL} vision model...\")\n",
    "print()\n",
    "\n",
    "flower_name_1 = identify_flower(flower_url_1)\n",
    "\n",
    "if flower_name_1:\n",
    "    print(f\"üå∏ Identified: {flower_name_1}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to identify flower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ Identify Flower 2\n",
    "\n",
    "Now let's identify the second flower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Identifying Flower 2...\")\n",
    "print(f\"   Sending to {OPENAI_MODEL} vision model...\")\n",
    "print()\n",
    "\n",
    "flower_name_2 = identify_flower(flower_url_2)\n",
    "\n",
    "if flower_name_2:\n",
    "    print(f\"üå∏ Identified: {flower_name_2}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to identify flower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úèÔ∏è Image Annotation\n",
    "\n",
    "Now that we've identified both flowers, let's create beautiful annotated images by overlaying the plant names on the original photos. This makes the results visually clear and shareable.\n",
    "\n",
    "## How Image Annotation Works\n",
    "\n",
    "To add text to an image, we need to:\n",
    "1. **Calculate appropriate font size** based on image dimensions (larger images need larger text)\n",
    "2. **Create a background rectangle** for the text to ensure readability over any color\n",
    "3. **Position the text** at a visually appealing location (typically bottom of the image)\n",
    "4. **Render the text** with high contrast (white text on black background)\n",
    "\n",
    "OpenCV provides all the tools we need for this image manipulation.\n",
    "\n",
    "Let's create a function to handle annotation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Create Annotation Function\n",
    "\n",
    "This function downloads an image, adds a text overlay with background, and displays the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_text(url, text):\n",
    "    \"\"\"\n",
    "    Download an image, add text annotation with background, and display it.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the image to annotate\n",
    "        text (str): Text to overlay on the image\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The annotated image in RGB format, or None if error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the image from URL\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Decode the image into OpenCV format\n",
    "        image_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"‚ùå Failed to decode image from {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Get image dimensions\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Calculate font size based on image dimensions\n",
    "        # Larger images need proportionally larger text for readability\n",
    "        font_scale = min(width, height) / 500  # Scale factor based on smaller dimension\n",
    "        font_scale = max(font_scale, 0.5)  # Minimum font size\n",
    "        \n",
    "        # Set font properties\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX  # Clean, readable font\n",
    "        thickness = max(int(font_scale * 2), 1)  # Thickness scales with font size\n",
    "        \n",
    "        # Calculate text size to determine background rectangle dimensions\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            text, font, font_scale, thickness\n",
    "        )\n",
    "        \n",
    "        # Calculate text position (centered horizontally, near bottom)\n",
    "        text_x = (width - text_width) // 2  # Center horizontally\n",
    "        text_y = height - 40  # 40 pixels from bottom\n",
    "        \n",
    "        # Define background rectangle coordinates with padding\n",
    "        padding = 15  # Pixels of padding around text\n",
    "        rect_x1 = text_x - padding\n",
    "        rect_y1 = text_y - text_height - padding\n",
    "        rect_x2 = text_x + text_width + padding\n",
    "        rect_y2 = text_y + baseline + padding\n",
    "        \n",
    "        # Draw semi-transparent black background rectangle\n",
    "        # This ensures text is readable regardless of background colors\n",
    "        overlay = image.copy()\n",
    "        cv2.rectangle(overlay, (rect_x1, rect_y1), (rect_x2, rect_y2), \n",
    "                     (0, 0, 0), -1)  # Black filled rectangle\n",
    "        \n",
    "        # Blend the rectangle with the original image (0.7 opacity)\n",
    "        alpha = 0.7  # Transparency factor\n",
    "        cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "        \n",
    "        # Draw white text on top of the black background\n",
    "        cv2.putText(image, text, (text_x, text_y), font, font_scale,\n",
    "                   (255, 255, 255), thickness, cv2.LINE_AA)  # White text, anti-aliased\n",
    "        \n",
    "        # Convert from BGR to RGB for display\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display the annotated image\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return image_rgb\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error downloading image: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error annotating image: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Image annotation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Display Annotated Flower 1\n",
    "\n",
    "Let's create and display the first annotated image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì∏ Final Result - Annotated Image 1:\")\n",
    "print(f\"   Adding text: '{flower_name_1}'\")\n",
    "print()\n",
    "\n",
    "if flower_name_1:\n",
    "    annotated_image_1 = display_image_with_text(flower_url_1, flower_name_1)\n",
    "    if annotated_image_1 is not None:\n",
    "        print(\"\\n‚úÖ Annotated image created successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping annotation - no flower name available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Display Annotated Flower 2\n",
    "\n",
    "Now let's create and display the second annotated image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì∏ Final Result - Annotated Image 2:\")\n",
    "print(f\"   Adding text: '{flower_name_2}'\")\n",
    "print()\n",
    "\n",
    "if flower_name_2:\n",
    "    annotated_image_2 = display_image_with_text(flower_url_2, flower_name_2)\n",
    "    if annotated_image_2 is not None:\n",
    "        print(\"\\n‚úÖ Annotated image created successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping annotation - no flower name available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ Validation and Results\n",
    "\n",
    "## What You Should See\n",
    "\n",
    "If everything worked correctly, you should now see:\n",
    "\n",
    "1. **Two original flower images** displayed in the \"Image Loading\" section\n",
    "2. **Two identification results** printed in the \"Plant Identification\" section\n",
    "3. **Two annotated images** with the flower names overlaid on black backgrounds\n",
    "\n",
    "## Understanding the Results\n",
    "\n",
    "**Model behavior variation**: Vision models may provide slightly different names across runs:\n",
    "- Example variations: \"Daisy\" vs \"Common Daisy\" vs \"White Daisy\"\n",
    "- All refer to the same plant - models balance scientific accuracy with common usage\n",
    "- This is normal behavior, not an error\n",
    "\n",
    "**Verifying identification accuracy**:\n",
    "- Compare the model's identification with your own observation\n",
    "- Search the plant name online to see if images match\n",
    "- For scientific applications, you may want to request scientific names in your prompt\n",
    "- Consider adding confidence scores to your application (requires prompt modification)\n",
    "\n",
    "## Troubleshooting Common Issues\n",
    "\n",
    "If you encounter problems:\n",
    "\n",
    "- **Images not loading**: Check your internet connection and verify URLs are accessible\n",
    "- **API errors**: Verify your API key is valid and has sufficient credits\n",
    "- **Incorrect identification**: Try adjusting the prompt for more specific instructions\n",
    "- **Text not visible**: Ensure the annotation function completed without errors\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've successfully built a complete multimodal AI application that:\n",
    "- ‚úÖ Downloads images from URLs\n",
    "- ‚úÖ Identifies plant species using vision AI\n",
    "- ‚úÖ Annotates images with identification results\n",
    "- ‚úÖ Handles errors gracefully throughout the pipeline\n",
    "\n",
    "This workflow can be adapted for countless other visual recognition tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üí° Best Practices for Multimodal AI\n",
    "\n",
    "Based on what we've learned, here are key best practices for working with vision models:\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Prompt Engineering\n",
    "- ‚úÖ **Be specific about output format** - Request exactly what you need (\"only common name\" vs. \"detailed description\")\n",
    "- ‚úÖ **Use clear, direct language** - Avoid ambiguous phrasing like \"tell me about this\"\n",
    "- ‚úÖ **Specify verbosity** - Use `text={\"verbosity\": \"low\"}` for concise answers, \"high\" for detailed analysis\n",
    "- ‚úÖ **Test and iterate** - Experiment with different prompts to find what works best for your use case\n",
    "\n",
    "### Model Selection\n",
    "- ‚úÖ **Choose cost-effective models** - Use `gpt-5-nano` for routine tasks; it's significantly cheaper than `gpt-4o`\n",
    "- ‚úÖ **Consider your use case** - Vision models work for text-only questions too, but text-only models are faster and cheaper\n",
    "- ‚úÖ **Understand model capabilities** - Vision models excel at visual tasks but aren't magical; they can make mistakes\n",
    "\n",
    "### Image Quality and Access\n",
    "- ‚úÖ **Use high-quality images** - Clear, well-lit photos produce better identification results\n",
    "- ‚úÖ **Ensure URL accessibility** - Images must be publicly accessible; private URLs won't work\n",
    "- ‚úÖ **Consider image size** - Very large images may be slower to process; optimize when possible\n",
    "- ‚úÖ **Test image loading separately** - Verify images download correctly before making API calls\n",
    "\n",
    "### Error Handling\n",
    "- ‚úÖ **Always use try-except blocks** - Network issues, API errors, and image problems will occur\n",
    "- ‚úÖ **Provide helpful error messages** - Tell users what went wrong and how to fix it\n",
    "- ‚úÖ **Validate inputs** - Check that API keys, URLs, and responses are valid before proceeding\n",
    "- ‚úÖ **Handle API response variations** - Models may return slightly different formats; be flexible\n",
    "\n",
    "### Cost Management\n",
    "- ‚úÖ **Monitor API usage** - Track costs, especially for production applications\n",
    "- ‚úÖ **Use appropriate verbosity** - Don't request detailed responses when brief ones suffice\n",
    "- ‚úÖ **Cache results when possible** - Store identification results to avoid re-analyzing the same images\n",
    "- ‚úÖ **Consider batch processing** - Process multiple images in sequence rather than making redundant calls\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Mistakes to Avoid\n",
    "\n",
    "### 1. Not Handling Failed Image Downloads\n",
    "**Problem**: Images from URLs can fail to download due to network issues, dead links, or access restrictions.\n",
    "\n",
    "**Solution**: Always wrap image operations in try-except blocks and return None on failure.\n",
    "\n",
    "### 2. Vague Prompts Leading to Inconsistent Results\n",
    "**Problem**: Asking \"What is this?\" may return paragraphs of text when you only need a name.\n",
    "\n",
    "**Solution**: Be explicit: \"Identify the flower type. Provide only the common name, nothing else.\"\n",
    "\n",
    "### 3. Not Validating API Responses\n",
    "**Problem**: API calls can fail, return errors, or provide unexpected formats.\n",
    "\n",
    "**Solution**: Check that responses exist and contain expected data before using them.\n",
    "\n",
    "### 4. Ignoring Image Format Conversions\n",
    "**Problem**: OpenCV uses BGR color space, but matplotlib expects RGB, causing color distortion.\n",
    "\n",
    "**Solution**: Always convert: `image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)`\n",
    "\n",
    "### 5. Hardcoding Image Sizes and Positions\n",
    "**Problem**: Text annotations sized for one image won't scale properly to images of different dimensions.\n",
    "\n",
    "**Solution**: Calculate font sizes and positions proportionally based on image dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Security Considerations\n",
    "\n",
    "- **Never hardcode API keys** in production code - use environment variables or secrets management\n",
    "- **Validate URLs before downloading** - protect against malicious or inappropriate content\n",
    "- **Be mindful of image content** - use moderation APIs if processing user-submitted images\n",
    "- **Respect rate limits** - implement backoff strategies to avoid account suspension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Next Steps and Extensions\n",
    "\n",
    "Congratulations! You've built a complete plant identification system. Here's how you can extend what you've learned:\n",
    "\n",
    "## üî¨ Practice Exercise Ideas\n",
    "\n",
    "### 1. Try Different Images\n",
    "Replace the URLs with your own flower images or other plants:\n",
    "- Garden plants\n",
    "- Houseplants\n",
    "- Trees and shrubs\n",
    "- Vegetables and herbs\n",
    "\n",
    "### 2. Batch Processing\n",
    "Modify the code to process a list of image URLs:\n",
    "```python\n",
    "image_urls = [url1, url2, url3, ...]\n",
    "for url in image_urls:\n",
    "    identify_and_annotate(url)\n",
    "```\n",
    "\n",
    "### 3. Add More Information\n",
    "Modify the prompt to request additional details:\n",
    "- Scientific name\n",
    "- Plant family\n",
    "- Growing conditions\n",
    "- Native region\n",
    "\n",
    "Then parse the response and create multi-line annotations.\n",
    "\n",
    "### 4. Save Annotated Images\n",
    "Add code to save the annotated images to files:\n",
    "```python\n",
    "cv2.imwrite('annotated_flower.jpg', annotated_image)\n",
    "```\n",
    "\n",
    "### 5. Build a User Interface\n",
    "Create an interactive interface where users can:\n",
    "- Upload their own images\n",
    "- Select what information to display\n",
    "- Download annotated results\n",
    "\n",
    "### 6. Compare Multiple Models\n",
    "Try different OpenAI vision models and compare results:\n",
    "- `gpt-5-nano` (cheapest, fast)\n",
    "- `gpt-4o` (most capable, expensive)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What's Next in the Course?\n",
    "\n",
    "In upcoming exercises, you'll learn:\n",
    "- Working with video content and audio transcription\n",
    "- Extracting structured data from unstructured text\n",
    "- Advanced prompt engineering techniques\n",
    "- Building complete AI-powered applications\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Real-World Application Ideas\n",
    "\n",
    "Consider how you might apply multimodal AI in your domain:\n",
    "\n",
    "- **Education**: Interactive learning apps for botany students\n",
    "- **Agriculture**: Crop disease identification from phone photos\n",
    "- **Retail**: Customer service bots that identify products from images\n",
    "- **Healthcare**: Medical image analysis assistants\n",
    "- **Manufacturing**: Quality control and defect detection\n",
    "- **Accessibility**: Image description services for visually impaired users\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Additional Resources\n",
    "\n",
    "To deepen your understanding:\n",
    "\n",
    "- [OpenAI Vision API Documentation](https://platform.openai.com/docs/guides/vision)\n",
    "- [OpenCV Python Tutorials](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)\n",
    "- [Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Best Practices for Production](https://platform.openai.com/docs/guides/production-best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully completed Exercise #5 and learned how to:\n",
    "- ‚úÖ Work with multimodal AI models\n",
    "- ‚úÖ Process and display images programmatically\n",
    "- ‚úÖ Make vision API calls with proper structure\n",
    "- ‚úÖ Annotate images with dynamic text overlays\n",
    "- ‚úÖ Handle errors and edge cases gracefully\n",
    "- ‚úÖ Apply prompt engineering to vision tasks\n",
    "\n",
    "Keep experimenting, and see you in the next exercise! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
