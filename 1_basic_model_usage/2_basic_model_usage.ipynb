{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKeBDwcr5tt7"
   },
   "source": [
    "# ü§ñ Basic OpenAI API Usage\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "\n",
    "1. **Content Moderation** - Check user messages for inappropriate content\n",
    "2. **Text Generation** - Draft responses, summarize documents, assist with troubleshooting\n",
    "3. **Audio Features** - Convert text to speech and transcribe audio\n",
    "4. **Image Generation** - Create simple visuals for documentation\n",
    "5. **Vision Analysis** - Analyze error screenshots and hardware images\n",
    "6. **Cost Management** - Use the most cost-effective models for each task\n",
    "\n",
    "---\n",
    "\n",
    "## üåê What is an API?\n",
    "\n",
    "An **API (Application Programming Interface)** is a way for different software programs to communicate with each other.\n",
    "\n",
    "### üéØ What is an API Endpoint?\n",
    "\n",
    "An **endpoint** is a specific function or service the API provides. For example:\n",
    "- `/v1/chat/completions` - Text generation endpoint\n",
    "- `/v1/moderations` - Content moderation endpoint\n",
    "- `/v1/images/generations` - Image generation endpoint\n",
    "\n",
    "Each endpoint has a specific purpose and accepts different parameters.\n",
    "\n",
    "### üì¨ Request/Response Structure\n",
    "\n",
    "API communication follows a simple pattern:\n",
    "\n",
    "1. **Request**: You send data to the API\n",
    "   - Headers (authentication, content type)\n",
    "   - Parameters (instructions, settings)\n",
    "   - Body (the actual data to process)\n",
    "\n",
    "2. **Response**: The API sends back results\n",
    "   - Status code (200 = success, 429 = rate limit, etc.)\n",
    "   - Data (the result you requested)\n",
    "   - Metadata (usage stats, IDs, etc.)\n",
    "\n",
    "### ‚ö†Ô∏è Important Concepts\n",
    "\n",
    "**Rate Limits**: APIs limit how many requests you can make per minute/day to prevent abuse. If you exceed these limits, you'll receive an error.\n",
    "\n",
    "**Costs**: Most API calls cost money based on usage:\n",
    "- Text models charge per \"token\" (roughly 4 characters)\n",
    "- Image models charge per image generated\n",
    "- Audio models charge per character (TTS) or per minute (transcription)\n",
    "\n",
    "**Security Best Practice**: ‚ö†Ô∏è **NEVER hardcode API keys in production code!** Always use environment variables, secrets management, or secure configuration systems.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "795QXNl_5tt9"
   },
   "source": [
    "---\n",
    "\n",
    "# üîß Setup\n",
    "\n",
    "First, we need to configure your OpenAI API key and install required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COs_IBx75tt-"
   },
   "source": [
    "## üì¶ Install Dependencies\n",
    "\n",
    "We'll install three libraries:\n",
    "- **openai**: Official OpenAI Python client for API access\n",
    "- **pillow**: Image processing library for displaying and manipulating images\n",
    "- **requests**: HTTP library for downloading files from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atYfYChq5tt-"
   },
   "outputs": [],
   "source": [
    "!pip install -q openai pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuCHOx6k5tt_"
   },
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two methods to provide your API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mvv_bwzM5tt_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "# Options: \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-3.5-turbo\", \"gpt-5-nano\", etc.\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQVK_nrS5tt_"
   },
   "source": [
    "## üöÄ Initialize OpenAI Client\n",
    "\n",
    "Now let's create a client instance to interact with the OpenAI API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FxFDAyP5tt_"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQsuh18o5tuA"
   },
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Content Moderation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "The moderation API detects potentially harmful or inappropriate content in text. It checks for:\n",
    "- Hate speech\n",
    "- Harassment and bullying\n",
    "- Violence and graphic content\n",
    "- Self-harm content\n",
    "- Sexual content\n",
    "- And more...\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "Use moderation **before** processing user-submitted content:\n",
    "- Support ticket messages\n",
    "- Chat messages\n",
    "- Email content\n",
    "- User feedback forms\n",
    "\n",
    "## üí° Why It Matters\n",
    "\n",
    "1. **Protects your API account** - OpenAI can suspend accounts that process harmful content\n",
    "2. **Ensures safe interactions** - Filters out inappropriate content before your team sees it\n",
    "3. **FREE to use** - The moderation API is completely free for OpenAI users!\n",
    "\n",
    "## üîë Key Parameters\n",
    "\n",
    "- **input**: The text to check (string or array of strings)\n",
    "\n",
    "**Response contains:**\n",
    "- **flagged**: Boolean indicating if content violates policies\n",
    "- **categories**: Dictionary of specific violation types (hate, violence, etc.)\n",
    "- **category_scores**: Confidence scores (0-1) for each category\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuO3tIP35tuA"
   },
   "source": [
    "## üíª Practical Example: Checking Customer Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pYCV1Y25tuA"
   },
   "outputs": [],
   "source": [
    "# Example 1: Frustrated but appropriate customer message\n",
    "customer_message_1 = \"\"\"This is absolutely ridiculous! Your service is terrible\n",
    "and I'm furious about this issue with my account!\"\"\"\n",
    "\n",
    "# Check the message using moderation API\n",
    "response = client.moderations.create(input=customer_message_1)\n",
    "\n",
    "result = response.results[0]\n",
    "\n",
    "print(\"üîç Moderation Check Results:\")\n",
    "print(f\"Is Flagged: {result.flagged}\")\n",
    "print(f\"\\nCategory Flags:\")\n",
    "for category, flagged in result.categories:\n",
    "    if flagged:\n",
    "        print(f\"  ‚ö†Ô∏è {category}: {flagged}\")\n",
    "\n",
    "if not result.flagged:\n",
    "    print(\"\\n‚úÖ Message is safe to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kfv6Vt8h5tuA"
   },
   "outputs": [],
   "source": [
    "# Example 2: Inappropriate message (DO NOT USE IN PRODUCTION)\n",
    "# This example demonstrates what gets flagged\n",
    "inappropriate_message = \"\"\"I hate your company and all your stupid employees.\n",
    "You're all idiots and deserve to lose your jobs.\"\"\"\n",
    "\n",
    "response = client.moderations.create(input=inappropriate_message)\n",
    "result = response.results[0]\n",
    "\n",
    "print(\"üîç Moderation Check Results:\")\n",
    "print(f\"Is Flagged: {result.flagged}\")\n",
    "print(f\"\\nCategory Flags:\")\n",
    "for category, flagged in result.categories:\n",
    "    if flagged:\n",
    "        print(f\"  ‚ö†Ô∏è {category}: {flagged}\")\n",
    "\n",
    "if result.flagged:\n",
    "    print(\"\\nüö´ Message contains inappropriate content - DO NOT process with API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxJoqlKv5tuA"
   },
   "source": [
    "## üõ†Ô∏è Reusable Function\n",
    "\n",
    "Let's create a simple function you can use in your projects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWUVp3r55tuB"
   },
   "outputs": [],
   "source": [
    "def check_content_safety(text):\n",
    "    \"\"\"\n",
    "    Check if text content is safe to process.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'is_safe' boolean and 'flagged_categories' list\n",
    "    \"\"\"\n",
    "    response = client.moderations.create(input=text)\n",
    "    result = response.results[0]\n",
    "\n",
    "    # Get all flagged categories\n",
    "    flagged_categories = [category for category, flagged in result.categories if flagged]\n",
    "\n",
    "    return {\n",
    "        \"is_safe\": not result.flagged,\n",
    "        \"flagged_categories\": flagged_categories\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "test_message = \"My printer won't work and I need help urgently!\"\n",
    "safety_check = check_content_safety(test_message)\n",
    "\n",
    "print(f\"Is safe: {safety_check['is_safe']}\")\n",
    "if safety_check['flagged_categories']:\n",
    "    print(f\"Flagged for: {', '.join(safety_check['flagged_categories'])}\")\n",
    "else:\n",
    "    print(\"No violations detected ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA7Me1gI5tuB"
   },
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Not checking all category flags individually** - Just checking `flagged` isn't enough; sometimes you need to know *what* was flagged\n",
    "\n",
    "2. **Forgetting to check moderation before sending to main API** - Always moderate user content first to protect your account\n",
    "\n",
    "3. **Over-relying on moderation** - The API is very good but not 100% perfect. Consider it as a helpful filter, not absolute protection\n",
    "\n",
    "4. **Not handling edge cases** - Very short messages or special characters might behave unexpectedly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2W4GH-q5tuB"
   },
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Text Generation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "Text generation models create human-like text responses using advanced language models. They can:\n",
    "- Answer questions\n",
    "- Draft responses\n",
    "- Summarize content\n",
    "- Provide troubleshooting guidance\n",
    "- Explain technical concepts\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "- **Drafting support ticket responses** - Save time with AI-generated first drafts\n",
    "- **Summarizing documentation** - Condense long technical docs into key points\n",
    "- **Troubleshooting assistance** - Generate step-by-step diagnostic procedures\n",
    "- **Knowledge base creation** - Turn technical info into user-friendly content\n",
    "\n",
    "## üí∞ Cost Implications\n",
    "\n",
    "Text generation is charged **per token**.\n",
    "\n",
    "**What's a token?** Roughly 4 characters or 0.75 words. For example:\n",
    "- \"Hello\" = 1 token\n",
    "- \"Hello, how are you?\" = 5 tokens\n",
    "- 100 words ‚âà 133 tokens\n",
    "\n",
    "You're charged for **both input and output tokens**.\n",
    "\n",
    "## üìè Context Window\n",
    "\n",
    "Models have limits on how much text they can process at once (input + output). For example:\n",
    "- `gpt-5-nano`: 128K tokens (~96,000 words)\n",
    "- `gpt-3.5-turbo`: 16K tokens (~12,000 words)\n",
    "\n",
    "## üîë Key Parameters for gpt-5-nano (Responses API)\n",
    "\n",
    "**Important**: gpt-5-nano uses the **Responses API**, not the Chat Completions API!\n",
    "\n",
    "- **model**: \"gpt-5-nano\" (one of the cheapest models)\n",
    "- **input**: Your prompt or question (string)\n",
    "- **text**: Dictionary with optional settings:\n",
    "  - **verbosity**: \"low\", \"medium\", or \"high\" - controls response length and detail\n",
    "    - \"low\": Brief, concise responses\n",
    "    - \"medium\": Balanced responses (default)\n",
    "    - \"high\": Detailed, comprehensive responses\n",
    "\n",
    "**Response structure**:\n",
    "- Use `response.output_text` to get the generated text\n",
    "- Use `response.usage.total_tokens` for token count\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Your prompt here\",\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "output = response.output_text\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M4bFLcH5tuC"
   },
   "source": [
    "## üíª Example 1: Drafting a Support Ticket Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBYz2e845tuC"
   },
   "outputs": [],
   "source": [
    "# Scenario: User reports printer not working\n",
    "user_ticket = \"\"\"My printer won't print anything. I tried turning it off and on\n",
    "but nothing works. I need to print reports for a meeting in 30 minutes.\"\"\"\n",
    "\n",
    "# System prompt defines the assistant's role and behavior\n",
    "system_prompt = \"\"\"You are an IT support assistant. Respond professionally,\n",
    "empathetically, and provide clear troubleshooting steps. Keep responses concise but thorough.\"\"\"\n",
    "\n",
    "# Combine system prompt with user message\n",
    "full_input = f\"{system_prompt}\\n\\nUser message: {user_ticket}\"\n",
    "\n",
    "# Make the API call using Responses API\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "# Extract the response\n",
    "draft_response = response.output_text\n",
    "\n",
    "print(\"üìù Draft Response:\")\n",
    "print(draft_response)\n",
    "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVpB3WNB5tuC"
   },
   "source": [
    "## üíª Example 2: Summarizing Technical Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZxlYEpU5tuC"
   },
   "outputs": [],
   "source": [
    "# Scenario: Summarize a long technical document\n",
    "long_technical_doc = \"\"\"\n",
    "VPN Configuration Guide:\n",
    "\n",
    "To establish a secure VPN connection, users must first ensure they have the latest\n",
    "VPN client installed (version 8.2 or higher). The installation package can be\n",
    "downloaded from the company portal under Software > Security Tools.\n",
    "\n",
    "Once installed, launch the VPN client and enter your corporate credentials.\n",
    "The username should be in the format: firstname.lastname@company.com.\n",
    "Use your standard Windows password.\n",
    "\n",
    "After successful authentication, select the appropriate VPN gateway from the dropdown:\n",
    "- US-EAST-01: For users in North America\n",
    "- EU-WEST-01: For users in Europe\n",
    "- ASIA-PAC-01: For users in Asia Pacific\n",
    "\n",
    "Click Connect and wait for the status indicator to turn green. This typically takes\n",
    "15-30 seconds. Once connected, you'll have access to internal resources including\n",
    "file shares, internal websites, and database servers.\n",
    "\n",
    "If you experience connection issues, first verify your internet connection is stable.\n",
    "Then check if your antivirus software is blocking the VPN client. Common antivirus\n",
    "programs that may interfere include McAfee and Norton. Add an exception for the\n",
    "VPN client executable if necessary.\n",
    "\n",
    "For persistent issues, contact IT support with your error message and the VPN.log\n",
    "file located in C:\\\\Program Files\\\\CompanyVPN\\\\logs\\\\.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"Summarize this technical documentation in 3-4 bullet points for end users.\"\n",
    "\n",
    "# Combine system prompt with document\n",
    "full_input = f\"{system_prompt}\\n\\n{long_technical_doc}\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"medium\"}  # Using medium for concise summary\n",
    ")\n",
    "\n",
    "summary = response.output_text\n",
    "\n",
    "print(\"üìã Summary:\")\n",
    "print(summary)\n",
    "print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")\n",
    "print(f\"   Input: {response.usage.input_tokens}, Output: {response.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULf4-zmQ5tuD"
   },
   "source": [
    "## üíª Example 3: Troubleshooting Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DATFTk-D5tuD"
   },
   "outputs": [],
   "source": [
    "# Scenario: User describes an error\n",
    "user_description = \"I keep getting 'Access Denied' when trying to open the shared drive.\"\n",
    "\n",
    "system_prompt = \"\"\"You are a troubleshooting assistant. Provide step-by-step diagnostic steps.\n",
    "Number each step clearly. Focus on the most common causes first.\"\"\"\n",
    "\n",
    "# Combine system prompt with user description\n",
    "full_input = f\"{system_prompt}\\n\\nUser issue: {user_description}\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=full_input,\n",
    "    text={\"verbosity\": \"high\"}\n",
    ")\n",
    "\n",
    "troubleshooting_steps = response.output_text\n",
    "\n",
    "print(\"üîß Troubleshooting Steps:\")\n",
    "print(troubleshooting_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9xDuFUu5tuD"
   },
   "source": [
    "## üõ°Ô∏è Error Handling\n",
    "\n",
    "Always wrap API calls in try/except blocks to handle potential errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rx64Uizo5tuD"
   },
   "outputs": [],
   "source": [
    "def generate_response_safely(user_message, system_message=\"You are a helpful IT assistant.\"):\n",
    "    \"\"\"\n",
    "    Generate a response with proper error handling.\n",
    "\n",
    "    Args:\n",
    "        user_message (str): The user's input\n",
    "        system_message (str): System prompt for the assistant\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains 'success' boolean, 'response' text, and optional 'error' message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Combine system prompt with user message\n",
    "        full_input = f\"{system_message}\\n\\nUser: {user_message}\"\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=full_input,\n",
    "            text={\"verbosity\": \"high\"}\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": response.output_text,\n",
    "            \"tokens_used\": response.usage.total_tokens\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"response\": None\n",
    "        }\n",
    "\n",
    "# Test the function\n",
    "result = generate_response_safely(\"How do I reset my password?\")\n",
    "\n",
    "if result[\"success\"]:\n",
    "    print(\"‚úÖ Response generated successfully\")\n",
    "    print(result[\"response\"])\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7xoeXjo5tuD"
   },
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Vague prompts leading to generic responses**\n",
    "   - ‚ùå Bad: \"Help with printer\"\n",
    "   - ‚úÖ Good: \"Provide step-by-step troubleshooting for a printer that won't print\"\n",
    "\n",
    "2. **Not setting appropriate max_completion_tokens**\n",
    "   - Set limits to control costs and prevent overly long responses\n",
    "   - 100 tokens ‚âà 75 words\n",
    "\n",
    "3. **Forgetting to handle API errors**\n",
    "   - Always use try/except blocks\n",
    "   - Common errors: rate limits, invalid API key, network issues\n",
    "\n",
    "4. **Not using system prompts effectively**\n",
    "   - System prompts set the behavior and tone\n",
    "   - Be specific about desired output format and style\n",
    "\n",
    "5. **Using unsupported parameters**\n",
    "   - `gpt-5-nano` only supports the default temperature (1)\n",
    "   - Don't specify temperature parameter or you'll get an error\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmAfKBiw5tuD"
   },
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Audio Features (Text-to-Speech & Transcription)\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "OpenAI provides two audio capabilities:\n",
    "1. **Text-to-Speech (TTS)**: Convert text into natural-sounding speech\n",
    "2. **Transcription**: Convert audio recordings into text\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "**Text-to-Speech:**\n",
    "- Creating audio guides for common procedures\n",
    "- Accessibility features for visually impaired users\n",
    "- Automated phone system messages\n",
    "- Training materials\n",
    "\n",
    "**Transcription:**\n",
    "- Converting support call recordings to text\n",
    "- Creating searchable records of meetings\n",
    "- Documenting verbal troubleshooting sessions\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "**TTS Models:**\n",
    "- `tts-1`: Faster, cheaper, good quality\n",
    "- `tts-1-hd`: Higher quality, more expensive\n",
    "\n",
    "**Transcription Model:**\n",
    "- `whisper-1`: OpenAI's speech recognition model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fX7HF_E5tuE"
   },
   "source": [
    "## üîä Part A: Text-to-Speech\n",
    "\n",
    "### üîë Key Parameters\n",
    "\n",
    "- **model**: `\"tts-1\"` (cheaper, faster) or `\"tts-1-hd\"` (higher quality)\n",
    "- **voice**: Choose from 6 voices:\n",
    "  - `alloy`: Neutral, balanced\n",
    "  - `echo`: Male, clear\n",
    "  - `fable`: Male, expressive\n",
    "  - `onyx`: Male, deep\n",
    "  - `nova`: Female, energetic\n",
    "  - `shimmer`: Female, soft\n",
    "- **input**: Text to convert (max 4096 characters)\n",
    "- **speed**: 0.25 to 4.0 (default 1.0)\n",
    "\n",
    "### üíª Practical Example: Create Password Reset Audio Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BipoZaR5tuE"
   },
   "outputs": [],
   "source": [
    "# Text for our audio guide\n",
    "password_reset_guide = \"\"\"\n",
    "Welcome to the password reset guide. Here are the steps:\n",
    "\n",
    "First, go to the login page and click 'Forgot Password'.\n",
    "\n",
    "Second, enter your work email address.\n",
    "\n",
    "Third, check your email for a reset link. This may take a few minutes.\n",
    "\n",
    "Finally, click the link and create a new password.\n",
    "\n",
    "Remember: your password must be at least 8 characters with numbers and symbols.\n",
    "\n",
    "If you need further assistance, contact the IT help desk. Thank you.\n",
    "\"\"\"\n",
    "\n",
    "# Generate speech\n",
    "print(\"üéôÔ∏è Generating audio...\")\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",  # Using cheaper model\n",
    "    voice=\"nova\",   # Female, energetic voice\n",
    "    input=password_reset_guide,\n",
    "    speed=1.0\n",
    ")\n",
    "\n",
    "# Save the audio file\n",
    "audio_file_path = \"/content/password_reset_guide.mp3\"\n",
    "response.stream_to_file(audio_file_path)\n",
    "\n",
    "print(f\"‚úÖ Audio saved to: {audio_file_path}\")\n",
    "print(\"\\n‚ñ∂Ô∏è You can play it using the file browser on the left sidebar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMF67fqf5tuE"
   },
   "outputs": [],
   "source": [
    "# Display audio player in notebook\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(audio_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFrm6XJA5tuE"
   },
   "source": [
    "## üé§ Part B: Transcription (Speech-to-Text)\n",
    "\n",
    "### üîë Key Parameters\n",
    "\n",
    "- **model**: `\"whisper-1\"` (OpenAI's speech recognition model)\n",
    "- **file**: Audio file to transcribe (max 25MB)\n",
    "- **language**: Optional ISO-639-1 code (e.g., \"en\" for English) for better accuracy\n",
    "- **response_format**: \"json\", \"text\", \"srt\", \"vtt\", or \"verbose_json\"\n",
    "\n",
    "**Supported formats**: mp3, mp4, mpeg, mpga, m4a, wav, webm\n",
    "\n",
    "### üíª Practical Example: Transcribe the Audio We Just Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6m2LTu25tuE"
   },
   "outputs": [],
   "source": [
    "# Transcribe the password reset guide we just created\n",
    "print(\"üé§ Transcribing audio...\")\n",
    "\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        language=\"en\"  # Specify English for better accuracy\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Transcription complete!\\n\")\n",
    "print(\"üìù Transcribed Text:\")\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIiw8L7L5tuE"
   },
   "source": [
    "### üîÑ Compare Original vs Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dWFgBhb5tuE"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Original Text ‚Üí Audio ‚Üí Transcription\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìÑ ORIGINAL TEXT:\")\n",
    "print(password_reset_guide)\n",
    "\n",
    "print(\"\\nüéôÔ∏è TRANSCRIBED TEXT:\")\n",
    "print(transcript.text)\n",
    "\n",
    "print(\"\\n‚ú® Notice how accurate the transcription is!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFqEAWAB5tuE"
   },
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "### Text-to-Speech:\n",
    "1. **Text too long** - Maximum 4096 characters per request. Split longer text into chunks.\n",
    "2. **Wrong speed settings** - Speed too fast (>1.5) can reduce clarity\n",
    "3. **Not choosing appropriate voice** - Test different voices for your use case\n",
    "\n",
    "### Transcription:\n",
    "1. **Audio files too large** - Whisper-1 has a 25MB limit. Compress large files first.\n",
    "2. **Wrong audio format** - Ensure your file is in a supported format (mp3, wav, etc.)\n",
    "3. **Not specifying language** - Adding the language parameter improves accuracy\n",
    "4. **Poor audio quality** - Background noise and low volume reduce transcription accuracy\n",
    "5. **Very long files** - Consider splitting files longer than 30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pw3-_hq5tuE"
   },
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Image Generation\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "The image generation API creates images from text descriptions using DALL-E models. You provide a text prompt, and it generates an original image.\n",
    "\n",
    "## üéØ When to Use It\n",
    "\n",
    "- **Quick mockups** - Visualize concepts before creating proper designs\n",
    "- **Documentation visuals** - Create diagrams and illustrations for guides\n",
    "- **Training materials** - Generate images for presentations and tutorials\n",
    "- **Placeholder images** - Quick visuals during development\n",
    "\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "- `dall-e-3`: Latest DALL-E model, high quality, better prompt following (recommended)\n",
    "- `dall-e-2`: Cheaper, good quality, faster\n",
    "\n",
    "**We'll use DALL-E 3 for better quality and prompt understanding.**\n",
    "\n",
    "## üîë Key Parameters\n",
    "\n",
    "- **model**: `\"dall-e-3\"` (recommended) or `\"dall-e-2\"`\n",
    "- **prompt**: Description of the image (max 4000 characters for DALL-E 3)\n",
    "- **size**: Image dimensions\n",
    "  - DALL-E 3: `\"1024x1024\"` (square), `\"1792x1024\"` (landscape), or `\"1024x1792\"` (portrait)\n",
    "  - DALL-E 2: `\"256x256\"`, `\"512x512\"`, or `\"1024x1024\"`\n",
    "- **quality**: `\"standard\"` or `\"hd\"` (DALL-E 3 only, default: \"standard\")\n",
    "  - \"hd\": Creates images with finer details and greater consistency (costs more)\n",
    "  - \"standard\": Regular quality, more cost-effective\n",
    "- **style**: `\"vivid\"` or `\"natural\"` (DALL-E 3 only)\n",
    "  - \"vivid\": Hyper-real, dramatic images\n",
    "  - \"natural\": More natural-looking images\n",
    "- **n**: Number of images to generate\n",
    "  - DALL-E 3: **Only supports n=1** (one image per request)\n",
    "  - DALL-E 2: Supports 1-10 images per request\n",
    "- **response_format**: `\"url\"` (default) or `\"b64_json\"`\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKJsKSN_5tuF"
   },
   "source": [
    "## üíª Practical Example: Network Diagram for Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXXTy03p5tuF"
   },
   "outputs": [],
   "source": [
    "# Create a simple network diagram\n",
    "prompt = \"\"\"A simple network diagram showing a router connected to a firewall,\n",
    "with three workstations behind it. Clean, technical style with clear labels.\n",
    "Minimalist and professional. White background.\"\"\"\n",
    "\n",
    "print(\"üé® Generating image...\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=prompt,\n",
    "    size=\"1024x1024\",      # Square format for documentation\n",
    "    quality=\"standard\",    # Standard quality for cost efficiency\n",
    "    style=\"natural\",       # Natural style for technical diagrams\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Get the image URL\n",
    "image_url = response.data[0].url\n",
    "print(f\"‚úÖ Image generated!\")\n",
    "print(f\"URL: {image_url}\")\n",
    "\n",
    "# DALL-E 3 provides a revised/enhanced prompt\n",
    "print(f\"\\nüìù Revised prompt: {response.data[0].revised_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4cZ7GTx5tuF"
   },
   "outputs": [],
   "source": [
    "# Download and display the image\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Download the image\n",
    "image_response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "# Save to file\n",
    "image_path = \"/content/network_diagram.png\"\n",
    "image.save(image_path)\n",
    "print(f\"üíæ Saved to: {image_path}\")\n",
    "\n",
    "# Display in notebook\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XLduNO05tuG"
   },
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Vision (Image Analysis)\n",
    "\n",
    "## üìñ What it Does\n",
    "\n",
    "Vision-capable models can \"see\" and analyze images. They can:\n",
    "- Describe what's in an image\n",
    "- Read text from screenshots\n",
    "- Identify error messages\n",
    "- Analyze hardware components\n",
    "- Recognize UI elements\n",
    "\n",
    "\n",
    "## üí∞ Models\n",
    "\n",
    "- `gpt-5-nano`: Cost-efficient vision model (recommended for most use cases)\n",
    "- `gpt-4o`: Highest quality vision analysis\n",
    "\n",
    "\n",
    "**Image input options:**\n",
    "- URL: `{\"type\": \"input_image\", \"image_url\": \"https://...\"}`\n",
    "- Base64: `{\"type\": \"input_image\", \"image_url\": \"data:image/jpeg;base64,...\"}`\n",
    "\n",
    "**Key differences from text-only API:**\n",
    "- Use `input` parameter (list format) instead of simple string\n",
    "- Content is an array with `input_text` and `input_image` objects\n",
    "- Access output with `response.output_text` (same as text API)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvi_zTg-5tuG"
   },
   "source": [
    "## üíª Practical Example: Error Screenshot Analysis\n",
    "\n",
    "### üì∏ Upload Your Image\n",
    "\n",
    "To use this example:\n",
    "1. Click the üìÅ folder icon in the left sidebar\n",
    "2. Click the upload button (üì§)\n",
    "3. Upload an error screenshot or any IT-related image\n",
    "4. Update the `image_path` variable below with your filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlETGxjp5tuG"
   },
   "outputs": [],
   "source": [
    "# Method 1: Analyze an uploaded image file\n",
    "import base64\n",
    "\n",
    "# Update this path with your uploaded image\n",
    "image_path = \"/content/error_screenshot.png\"  # Change this to your image filename\n",
    "\n",
    "# Check if file exists\n",
    "import os\n",
    "if not os.path.exists(image_path):\n",
    "    print(\"‚ö†Ô∏è Image file not found!\")\n",
    "    print(\"Please upload an image file and update the image_path variable above.\")\n",
    "    print(\"\\nFor demonstration, we'll create a simple example...\")\n",
    "else:\n",
    "    # Read and encode the image\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Analyze the image using Responses API\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": \"\"\"Please analyze this screenshot:\n",
    "                        1. What error or issue is shown?\n",
    "                        2. What are the likely causes?\n",
    "                        3. What troubleshooting steps would you recommend?\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        text={\"verbosity\": \"high\"}\n",
    "    )\n",
    "\n",
    "    analysis = response.output_text\n",
    "\n",
    "    print(\"üîç Image Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(analysis)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìä Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsmwLW465tuG"
   },
   "source": [
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "1. **Image files too large**\n",
    "   - Recommended: Keep images under 20MB\n",
    "   - Compress large images before uploading\n",
    "   - Use tools like PIL to resize: `image.thumbnail((1024, 1024))`\n",
    "\n",
    "2. **Unclear or low-resolution screenshots**\n",
    "   - Ensure text in screenshots is readable\n",
    "   - Higher resolution = better analysis\n",
    "   - Avoid blurry or pixelated images\n",
    "\n",
    "3. **Not providing enough context in the text prompt**\n",
    "   - ‚ùå Bad: \"What's in this image?\"\n",
    "   - ‚úÖ Good: \"What error is shown in this Windows screenshot and what are possible solutions?\"\n",
    "\n",
    "4. **Assuming the model can see very small text**\n",
    "   - If text is important, make sure it's legible in the screenshot\n",
    "   - Zoom in or crop to the relevant area\n",
    "\n",
    "5. **Incorrect image encoding**\n",
    "   - Ensure proper base64 encoding\n",
    "   - Include correct MIME type (image/png, image/jpeg, etc.)\n",
    "\n",
    "6. **Not handling file path errors**\n",
    "   - Always check if file exists before trying to read it\n",
    "   - Use try/except blocks for error handling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHeL9YVC5tuH"
   },
   "source": [
    "## üí° Cost Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_57-5Mj85tuH"
   },
   "outputs": [],
   "source": [
    "# Example: Calculate approximate cost for a text generation request\n",
    "\n",
    "def estimate_text_cost(input_text, output_tokens_estimate=200, model=\"gpt-5-nano\"):\n",
    "    \"\"\"\n",
    "    Estimate the cost of a text generation request.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text\n",
    "        output_tokens_estimate (int): Expected output length in tokens\n",
    "        model (str): Model name\n",
    "\n",
    "    Returns:\n",
    "        dict: Cost breakdown\n",
    "    \"\"\"\n",
    "    # Rough token estimation (actual tokenization is more complex)\n",
    "    input_tokens = len(input_text) // 4\n",
    "\n",
    "    # Pricing (per 1M tokens)\n",
    "    if model == \"gpt-5-nano\":\n",
    "        input_cost_per_1m = 0.05\n",
    "        output_cost_per_1m = 0.40\n",
    "    elif model == \"gpt-4o\":\n",
    "        input_cost_per_1m = 2.50\n",
    "        output_cost_per_1m = 10.00\n",
    "    else:\n",
    "        input_cost_per_1m = 0.05  # Default to nano pricing\n",
    "        output_cost_per_1m = 0.40\n",
    "\n",
    "    input_cost = (input_tokens / 1_000_000) * input_cost_per_1m\n",
    "    output_cost = (output_tokens_estimate / 1_000_000) * output_cost_per_1m\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_estimate\": output_tokens_estimate,\n",
    "        \"total_tokens\": input_tokens + output_tokens_estimate,\n",
    "        \"input_cost\": input_cost,\n",
    "        \"output_cost\": output_cost,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "\n",
    "# Example\n",
    "sample_ticket = \"My computer won't start. The screen is black and I hear beeping sounds.\"\n",
    "cost = estimate_text_cost(sample_ticket, output_tokens_estimate=300)\n",
    "\n",
    "print(\"üí∞ Cost Estimate:\")\n",
    "print(f\"Input tokens: {cost['input_tokens']}\")\n",
    "print(f\"Output tokens (estimated): {cost['output_tokens_estimate']}\")\n",
    "print(f\"Total tokens: {cost['total_tokens']}\")\n",
    "print(f\"\\nEstimated cost: ${cost['total_cost']:.6f}\")\n",
    "print(f\"  Input: ${cost['input_cost']:.6f}\")\n",
    "print(f\"  Output: ${cost['output_cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxQ6IXgc5tuH"
   },
   "source": [
    "## ‚úÖ Best Practices Summary\n",
    "\n",
    "### 1. Content Safety\n",
    "- ‚úÖ **Always use moderation for user-generated content**\n",
    "- ‚úÖ Check moderation BEFORE sending to other APIs\n",
    "- ‚úÖ It's FREE - use it liberally!\n",
    "\n",
    "### 2. Model Selection\n",
    "- ‚úÖ **Choose the cheapest model that meets your needs**\n",
    "  - Text: `gpt-5-nano` for most tasks (uses Responses API)\n",
    "  - Audio TTS: `tts-1` (not `tts-1-hd`)\n",
    "  - Images: `dall-e-3` for high quality, `dall-e-2` for cost savings\n",
    "  - Vision: `gpt-4o` (currently the main vision model)\n",
    "- ‚úÖ Only upgrade to premium models when quality difference matters\n",
    "\n",
    "### 3. API Usage for gpt-5-nano\n",
    "- ‚úÖ **Use the Responses API**: `client.responses.create()`\n",
    "- ‚úÖ **Access output correctly**: `response.output_text`\n",
    "- ‚úÖ **Control verbosity**: Use `text={\"verbosity\": \"low|medium|high\"}`\n",
    "  - \"low\": Brief responses, saves tokens\n",
    "  - \"medium\": Balanced (default)\n",
    "  - \"high\": Detailed, comprehensive responses\n",
    "\n",
    "### 4. Image Generation with DALL-E 3\n",
    "- ‚úÖ **Remember n=1 limitation**: DALL-E 3 only generates 1 image per request\n",
    "- ‚úÖ **Use quality wisely**: \"standard\" for most cases, \"hd\" only when needed\n",
    "- ‚úÖ **Choose appropriate style**: \"vivid\" for graphics, \"natural\" for realistic images\n",
    "- ‚úÖ **Check revised_prompt**: See how DALL-E 3 enhanced your prompt\n",
    "\n",
    "### 5. Error Handling\n",
    "- ‚úÖ **Always use try/except blocks**\n",
    "- ‚úÖ Handle rate limits gracefully\n",
    "- ‚úÖ Provide helpful error messages to users\n",
    "- ‚úÖ Log errors for debugging\n",
    "\n",
    "### 6. Security\n",
    "- ‚úÖ **NEVER hardcode API keys in production**\n",
    "- ‚úÖ Use environment variables or secrets management\n",
    "- ‚úÖ Rotate API keys periodically\n",
    "- ‚úÖ Set up usage alerts in OpenAI dashboard\n",
    "\n",
    "### 7. Monitoring\n",
    "- ‚úÖ **Track your API usage regularly**\n",
    "- ‚úÖ Set up billing alerts\n",
    "- ‚úÖ Monitor which endpoints cost the most\n",
    "- ‚úÖ Review and optimize high-usage areas\n",
    "\n",
    "### 8. Prompt Engineering\n",
    "- ‚úÖ Be specific about what you want\n",
    "- ‚úÖ Include system instructions directly in the input\n",
    "- ‚úÖ Provide examples when helpful\n",
    "- ‚úÖ Iterate and improve prompts based on results\n",
    "\n",
    "### 9. Model-Specific Settings\n",
    "- ‚úÖ `gpt-5-nano` uses the **Responses API**:\n",
    "  ```python\n",
    "  response = client.responses.create(\n",
    "      model=\"gpt-5-nano\",\n",
    "      input=\"your prompt here\",\n",
    "      text={\"verbosity\": \"high\"}\n",
    "  )\n",
    "  output = response.output_text\n",
    "  ```\n",
    "- ‚úÖ Much cheaper than gpt-4o\n",
    "- ‚úÖ DALL-E 3 provides automatic prompt enhancement via `revised_prompt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoUoyMU75tuH"
   },
   "source": [
    "\n",
    "\n",
    "# 7Ô∏è‚É£ Mini-Project: Support Ticket Analyzer\n",
    "\n",
    "## üéØ Project Description\n",
    "\n",
    "Let's combine everything we've learned! We'll build a **Support Ticket Analyzer** that:\n",
    "\n",
    "1. ‚úÖ **Checks if the ticket content is appropriate** (Moderation)\n",
    "2. üìä **Analyzes the urgency and issue type** (Text Generation)\n",
    "3. ‚úâÔ∏è **Drafts a professional response** (Text Generation)\n",
    "\n",
    "This is a practical tool you could adapt for real IT support workflows!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wPsOE9a5tuH"
   },
   "source": [
    "## üõ†Ô∏è TODO Version (Try It Yourself First!)\n",
    "\n",
    "Try to complete the function below on your own. Fill in the TODO sections using what you've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t8uhvOL5tuH"
   },
   "outputs": [],
   "source": [
    "def analyze_support_ticket_TODO(ticket_content):\n",
    "    \"\"\"\n",
    "    Analyze a support ticket and generate a response.\n",
    "\n",
    "    Args:\n",
    "        ticket_content (str): The customer's ticket message\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including safety check, urgency, issue type, and draft response\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO 1: Check content safety with moderation API\n",
    "    # Hint: Use client.moderations.create()\n",
    "    # Store result in a variable called 'is_safe'\n",
    "\n",
    "    # If content is not safe, return early\n",
    "    # if not is_safe:\n",
    "    #     return {\"is_safe\": False, \"message\": \"Inappropriate content detected\"}\n",
    "\n",
    "    # TODO 2: Analyze ticket urgency and issue type\n",
    "    # Create a prompt that asks the AI to identify:\n",
    "    #   - Urgency level (Low, Medium, High, Critical)\n",
    "    #   - Issue type (Hardware, Software, Network, Access, Other)\n",
    "    # Hint: Use client.chat.completions.create() with a good system prompt\n",
    "\n",
    "    # TODO 3: Generate a professional response\n",
    "    # Create another AI call to draft a response to the customer\n",
    "    # The response should be:\n",
    "    #   - Professional and empathetic\n",
    "    #   - Address the specific issue mentioned\n",
    "    #   - Provide clear next steps\n",
    "\n",
    "    return {\n",
    "        \"is_safe\": True,  # Replace with actual result\n",
    "        \"urgency\": \"TODO\",\n",
    "        \"issue_type\": \"TODO\",\n",
    "        \"draft_response\": \"TODO\"\n",
    "    }\n",
    "\n",
    "print(\"üéì TODO: Complete the function above, then test it with the test cases below!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlZoSP_35tuI"
   },
   "source": [
    "## ‚úÖ Complete Solution\n",
    "\n",
    "Here's a full implementation of the support ticket analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpe8rV9m5tuI"
   },
   "outputs": [],
   "source": [
    "def analyze_support_ticket(ticket_content):\n",
    "    \"\"\"\n",
    "    Analyze a support ticket and generate a response.\n",
    "\n",
    "    Args:\n",
    "        ticket_content (str): The customer's ticket message\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including safety check, urgency, issue type, and draft response\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Check content safety\n",
    "        print(\"üîç Checking content safety...\")\n",
    "        moderation_response = client.moderations.create(input=ticket_content)\n",
    "        is_safe = not moderation_response.results[0].flagged\n",
    "\n",
    "        if not is_safe:\n",
    "            flagged_categories = [\n",
    "                category for category, flagged\n",
    "                in moderation_response.results[0].categories\n",
    "                if flagged\n",
    "            ]\n",
    "            return {\n",
    "                \"is_safe\": False,\n",
    "                \"flagged_categories\": flagged_categories,\n",
    "                \"message\": \"‚ö†Ô∏è Inappropriate content detected. Ticket requires manual review.\"\n",
    "            }\n",
    "\n",
    "        print(\"‚úÖ Content is safe\\n\")\n",
    "\n",
    "        # Step 2: Analyze urgency and issue type\n",
    "        print(\"üìä Analyzing urgency and issue type...\")\n",
    "        analysis_prompt = f\"\"\"You are an IT ticket classification assistant. Be concise.\n",
    "\n",
    "Analyze this support ticket and provide:\n",
    "1. Urgency level: Critical / High / Medium / Low\n",
    "2. Issue type: Hardware / Software / Network / Access / Account / Other\n",
    "\n",
    "Provide your response in this exact format:\n",
    "Urgency: [level]\n",
    "Issue Type: [type]\n",
    "\n",
    "Ticket: {ticket_content}\n",
    "\"\"\"\n",
    "\n",
    "        analysis_response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=analysis_prompt,\n",
    "            text={\"verbosity\": \"low\"}  # Low verbosity for classification\n",
    "        )\n",
    "\n",
    "        analysis_text = analysis_response.output_text\n",
    "\n",
    "        # Parse the response\n",
    "        urgency = \"Medium\"  # Default\n",
    "        issue_type = \"Other\"  # Default\n",
    "\n",
    "        for line in analysis_text.split('\\n'):\n",
    "            if 'urgency:' in line.lower():\n",
    "                urgency = line.split(':')[1].strip()\n",
    "            elif 'issue type:' in line.lower():\n",
    "                issue_type = line.split(':')[1].strip()\n",
    "\n",
    "        print(f\"  Urgency: {urgency}\")\n",
    "        print(f\"  Issue Type: {issue_type}\\n\")\n",
    "\n",
    "        # Step 3: Generate professional response\n",
    "        print(\"‚úçÔ∏è Drafting response...\")\n",
    "        response_prompt = f\"\"\"You are a helpful IT support professional.\n",
    "\n",
    "Draft a professional IT support response to this ticket.\n",
    "\n",
    "Guidelines:\n",
    "- Be empathetic and professional\n",
    "- Acknowledge the issue\n",
    "- Provide clear troubleshooting steps or next actions\n",
    "- Include estimated response time if urgent\n",
    "- Keep it concise (3-4 short paragraphs)\n",
    "\n",
    "Ticket urgency: {urgency}\n",
    "Issue type: {issue_type}\n",
    "\n",
    "Ticket content: {ticket_content}\n",
    "\"\"\"\n",
    "\n",
    "        response_generation = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=response_prompt,\n",
    "            text={\"verbosity\": \"high\"}\n",
    "        )\n",
    "\n",
    "        draft_response = response_generation.output_text\n",
    "\n",
    "        print(\"‚úÖ Analysis complete!\\n\")\n",
    "\n",
    "        return {\n",
    "            \"is_safe\": True,\n",
    "            \"urgency\": urgency,\n",
    "            \"issue_type\": issue_type,\n",
    "            \"draft_response\": draft_response,\n",
    "            \"success\": True\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"message\": f\"‚ùå Error analyzing ticket: {str(e)}\"\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Support Ticket Analyzer function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZqQ2TG95tuI"
   },
   "source": [
    "## üß™ Test Cases\n",
    "\n",
    "Let's test the analyzer with different types of tickets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUe6WIAL5tuI"
   },
   "source": [
    "### Test 1: Urgent Password Reset Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_FvwSlR5tuI"
   },
   "outputs": [],
   "source": [
    "test_ticket_1 = \"\"\"\n",
    "I've been locked out of my account and can't reset my password!\n",
    "The password reset link isn't working and I have an important\n",
    "presentation in 1 hour. I really need access ASAP!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Urgent Password Reset\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_1}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_1)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmolgI205tuI"
   },
   "source": [
    "### Test 2: General Software Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-6cOsF35tuI"
   },
   "outputs": [],
   "source": [
    "test_ticket_2 = \"\"\"\n",
    "Hi, I'm trying to understand how to use the shared calendar feature\n",
    "in Outlook. Can someone explain how to share my calendar with my team\n",
    "and set the appropriate permissions? Thanks!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 2: General Software Question\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_2}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_2)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tza955gI5tuI"
   },
   "source": [
    "### Test 3: Frustrated User (Professional but Upset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct_7V1AB5tuI"
   },
   "outputs": [],
   "source": [
    "test_ticket_3 = \"\"\"\n",
    "This is the third time this month the VPN has gone down! I'm trying to\n",
    "work from home and I can't access any company resources. This is extremely\n",
    "frustrating and affecting my productivity. When will this be fixed properly?\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 3: Frustrated User\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Ticket Content:\\n{test_ticket_3}\\n\")\n",
    "\n",
    "result = analyze_support_ticket(test_ticket_3)\n",
    "\n",
    "if result['success']:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Safe: {result['is_safe']}\")\n",
    "    print(f\"Urgency: {result['urgency']}\")\n",
    "    print(f\"Issue Type: {result['issue_type']}\")\n",
    "    print(f\"\\n‚úâÔ∏è DRAFT RESPONSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['draft_response'])\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11aG7wzn5tuJ"
   },
   "source": [
    "---\n",
    "\n",
    "# üö® Error Handling Reference\n",
    "\n",
    "Here are common errors you might encounter and how to handle them:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LG1nXr9Q5tuJ"
   },
   "source": [
    "## Common Error Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boYJdeBA5tuJ"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAIError, RateLimitError, APIError, AuthenticationError\n",
    "\n",
    "def make_robust_api_call(prompt, max_retries=3):\n",
    "    \"\"\"\n",
    "    Make an API call with comprehensive error handling.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Response or error information\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                input=prompt,\n",
    "                text={\"verbosity\": \"high\"}\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"response\": response.output_text\n",
    "            }\n",
    "\n",
    "        # Error 1: Rate Limit - Too many requests\n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s...\n",
    "            print(f\"‚ö†Ô∏è Rate limit hit. Waiting {wait_time} seconds...\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"rate_limit\",\n",
    "                \"message\": \"Rate limit exceeded. Please try again later.\"\n",
    "            }\n",
    "\n",
    "        # Error 2: Authentication - Invalid API key\n",
    "        except AuthenticationError as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"authentication\",\n",
    "                \"message\": \"Invalid API key. Please check your credentials.\"\n",
    "            }\n",
    "\n",
    "        # Error 3: API Error - OpenAI service issues\n",
    "        except APIError as e:\n",
    "            print(f\"‚ö†Ô∏è API error occurred: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"api_error\",\n",
    "                \"message\": f\"OpenAI API error: {str(e)}\"\n",
    "            }\n",
    "\n",
    "        # Error 4: Any other errors\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error_type\": \"unknown\",\n",
    "                \"message\": f\"Unexpected error: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"success\": False,\n",
    "        \"error_type\": \"max_retries\",\n",
    "        \"message\": \"Maximum retry attempts exceeded.\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Robust API call function created!\")\n",
    "print(\"\\nThis function handles:\")\n",
    "print(\"  1. Rate limits with exponential backoff\")\n",
    "print(\"  2. Authentication errors\")\n",
    "print(\"  3. API service errors\")\n",
    "print(\"  4. Unknown errors\")\n",
    "print(\"  5. Automatic retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjKDVUis5tuJ"
   },
   "source": [
    "## Test the Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7JpOHKk5tuJ"
   },
   "outputs": [],
   "source": [
    "# Test with a normal request\n",
    "result = make_robust_api_call(\"What is an IP address?\")\n",
    "\n",
    "if result['success']:\n",
    "    print(\"‚úÖ Success!\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error ({result['error_type']}): {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxvV3yv25tuJ"
   },
   "source": [
    "## üìã Error Reference Table\n",
    "\n",
    "| Error Type | Common Cause | Solution |\n",
    "|------------|--------------|----------|\n",
    "| **Rate Limit** | Too many requests per minute | Implement exponential backoff, reduce request frequency |\n",
    "| **Authentication** | Invalid/expired API key | Check API key, regenerate if needed |\n",
    "| **Model Not Found** | Typo in model name | Verify model name matches OpenAI docs |\n",
    "| **Token Limit Exceeded** | Input/output too long | Reduce prompt length or max_completion_tokens |\n",
    "| **Timeout** | Request took too long | Increase timeout, check network connection |\n",
    "| **Invalid Request** | Missing required parameters | Check API documentation for required fields |\n",
    "| **Content Policy** | Violated OpenAI policies | Use moderation API first, adjust content |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzMsMKG65tuJ"
   },
   "source": [
    "\n",
    "\n",
    "## üìñ Resources\n",
    "\n",
    "- **OpenAI Cookbook**: https://cookbook.openai.com\n",
    "- **API Reference**: https://platform.openai.com/docs/api-reference\n",
    "- **Best Practices**: https://platform.openai.com/docs/guides/production-best-practices\n",
    "- **Pricing**: https://openai.com/pricing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOgmR46O712W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
