{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ó Hugging Face & Transformers Library\n",
    "\n",
    "Welcome to the Hugging Face tutorial! In this notebook, you'll learn how to leverage the power of pre-trained models and the Hugging Face ecosystem to build state-of-the-art AI applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Power of Transfer Learning\n",
    "\n",
    "### üéØ What is Transfer Learning?\n",
    "\n",
    "**Transfer learning** is the practice of taking a model that has already been trained on a large dataset and adapting it to your specific task. Instead of training a model from scratch (which requires massive amounts of data, compute power, and time), you leverage knowledge the model has already learned.\n",
    "\n",
    "Think of it like this: Imagine you want to become a chef specializing in Italian cuisine. You have two options:\n",
    "\n",
    "1. **Training from scratch**: Start with zero cooking knowledge. Learn what heat is, how to hold a knife, basic food safety, cooking techniques, ingredient properties, etc. This takes years.\n",
    "\n",
    "2. **Transfer learning**: You're already a skilled chef who knows cooking fundamentals. Now you just need to learn Italian-specific techniques, ingredients, and recipes. This takes months, not years.\n",
    "\n",
    "### üöÄ Why Transfer Learning Matters\n",
    "\n",
    "Modern language models like GPT, BERT, and others are trained on billions of words from the internet. This training:\n",
    "- Costs millions of dollars in compute resources\n",
    "- Takes weeks or months on specialized hardware\n",
    "- Requires massive datasets (hundreds of gigabytes of text)\n",
    "\n",
    "With transfer learning, you can use these pre-trained models **for free** and adapt them to your needs in minutes or hours instead of weeks!\n",
    "\n",
    "### üí° Key Benefits\n",
    "\n",
    "‚úÖ **Faster development** - Get results in minutes, not months\n",
    "\n",
    "‚úÖ **Less data required** - Pre-trained models already understand language\n",
    "\n",
    "‚úÖ **Lower costs** - No need for expensive GPU clusters\n",
    "\n",
    "‚úÖ **Better performance** - Leverage models trained on massive datasets\n",
    "\n",
    "‚úÖ **Proven solutions** - Use models tested by millions of users\n",
    "\n",
    "### üìö What You'll Learn\n",
    "\n",
    "In this notebook, you'll discover how to:\n",
    "\n",
    "1. Navigate the Hugging Face ecosystem and find the right models\n",
    "2. Use pipelines for instant AI capabilities with just a few lines of code\n",
    "3. Work with models and tokenizers for deeper control\n",
    "4. Run powerful language models locally on your machine\n",
    "5. Build practical applications like chatbots and text analyzers\n",
    "\n",
    "Let's get started! üéâ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Welcome to Hugging Face\n",
    "\n",
    "### 2.1 What is Hugging Face? ü§ó\n",
    "\n",
    "**Hugging Face** is a company and platform that has become the \"GitHub of machine learning.\" Founded in 2016 as a chatbot company, Hugging Face pivoted in 2018 to focus on democratizing AI by making state-of-the-art models accessible to everyone.\n",
    "\n",
    "**Their mission**: Make good machine learning accessible to everyone.\n",
    "\n",
    "**Why it's popular**:\n",
    "- Free and open-source tools\n",
    "- Largest collection of pre-trained models (500,000+ models)\n",
    "- Easy-to-use APIs that abstract away complexity\n",
    "- Active community sharing models, datasets, and knowledge\n",
    "- Excellent documentation and tutorials\n",
    "- Works with all major ML frameworks (PyTorch, TensorFlow, JAX)\n",
    "\n",
    "Today, Hugging Face is used by over 1 million developers and companies like Google, Microsoft, Meta, and Amazon.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 The Hugging Face Hub üåê\n",
    "\n",
    "The Hugging Face Hub is the central platform with four main components:\n",
    "\n",
    "#### ü§ñ **Model Hub**\n",
    "- 500,000+ pre-trained models for every task imaginable\n",
    "- Text generation, classification, translation, image generation, speech recognition, and more\n",
    "- Models from leading research labs and the community\n",
    "- Filter by task, language, license, and performance\n",
    "\n",
    "#### üìä **Datasets Hub**\n",
    "- 100,000+ datasets ready to use\n",
    "- Text, image, audio, and multimodal datasets\n",
    "- Preprocessing and loading handled automatically\n",
    "- Community contributions and standard benchmarks\n",
    "\n",
    "#### üöÄ **Spaces**\n",
    "- Host and share ML applications (demos)\n",
    "- Deploy models as web apps with Gradio or Streamlit\n",
    "- Try models interactively before coding\n",
    "\n",
    "#### üìñ **Documentation**\n",
    "- Comprehensive guides and tutorials\n",
    "- API references and examples\n",
    "- Community forums and discussions\n",
    "\n",
    "üí° **Key Point**: The Hub is your one-stop shop for everything you need to build AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Understanding Model Cards üìã\n",
    "\n",
    "Every model on Hugging Face has a **Model Card** - a document that describes the model in detail. Think of it as a \"nutrition label\" for AI models.\n",
    "\n",
    "**What Model Cards contain**:\n",
    "\n",
    "1. **Model Description**: What the model does and how it was trained\n",
    "2. **Intended Use**: What tasks it's designed for\n",
    "3. **Training Data**: What data was used (important for understanding biases)\n",
    "4. **Performance Metrics**: How well it performs on benchmarks\n",
    "5. **Limitations**: Known weaknesses and edge cases\n",
    "6. **Bias and Fairness**: Potential biases in the model\n",
    "7. **How to Use**: Code examples and usage instructions\n",
    "8. **License**: Legal terms for usage\n",
    "\n",
    "#### üîç How to Evaluate Models Using Model Cards\n",
    "\n",
    "Before using a model, check:\n",
    "\n",
    "‚úÖ **Task alignment**: Does it match your use case?\n",
    "\n",
    "‚úÖ **Performance**: Are the metrics acceptable for your needs?\n",
    "\n",
    "‚úÖ **Language support**: Does it handle your target language(s)?\n",
    "\n",
    "‚úÖ **Model size**: Will it fit in your available memory?\n",
    "\n",
    "‚úÖ **License**: Can you use it for your purpose (commercial, research, etc.)?\n",
    "\n",
    "‚úÖ **Last updated**: Is it actively maintained or outdated?\n",
    "\n",
    "‚úÖ **Downloads/likes**: High numbers often indicate quality and reliability\n",
    "\n",
    "#### ‚öñÔ∏è Licensing Considerations\n",
    "\n",
    "Common licenses you'll see:\n",
    "\n",
    "- **Apache 2.0 / MIT**: Very permissive, use for anything including commercial\n",
    "- **CC-BY**: Requires attribution, usually OK for commercial use\n",
    "- **CC-BY-NC**: Non-commercial only\n",
    "- **Custom licenses**: Read carefully - may have restrictions\n",
    "\n",
    "‚ö†Ô∏è **Common Mistake**: Assuming all models are free for commercial use. Always check the license!\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Monetization and Sustainability üí∞\n",
    "\n",
    "You might wonder: \"How does Hugging Face make money if everything is free?\"\n",
    "\n",
    "**Free Tier** (what we'll use):\n",
    "- Access to all open-source models\n",
    "- Use of the Transformers library\n",
    "- Public model/dataset hosting\n",
    "- Community support\n",
    "\n",
    "**Hugging Face Pro** ($9/month):\n",
    "- Private model/dataset hosting\n",
    "- Higher compute limits for Spaces\n",
    "- Early access to features\n",
    "- Priority support\n",
    "\n",
    "**Enterprise Solutions**:\n",
    "- Inference Endpoints (hosted API for models)\n",
    "- AutoTrain (automated model training)\n",
    "- Private Hub instances\n",
    "- Expert support and consulting\n",
    "\n",
    "**Why this matters**: A sustainable business model means the platform will continue to be maintained and improved. You can rely on it for long-term projects.\n",
    "\n",
    "üí° **Key Takeaway**: The free tier is incredibly generous and sufficient for learning, prototyping, and many production use cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting Started: Setting Up Your Hugging Face Account\n",
    "\n",
    "### 3.1 Creating an Account üìù\n",
    "\n",
    "To get the most out of Hugging Face, you'll want to create a free account. Here's how:\n",
    "\n",
    "**Step-by-step instructions**:\n",
    "\n",
    "1. Go to [huggingface.co](https://huggingface.co)\n",
    "2. Click the \"Sign Up\" button in the top right\n",
    "3. Enter your email address and create a password\n",
    "4. Verify your email address (check your inbox)\n",
    "5. Complete your profile (optional but recommended)\n",
    "\n",
    "That's it! You now have access to the entire Hugging Face ecosystem.\n",
    "\n",
    "üí° **Note**: You can use most features without an account, but having one allows you to save models, create Spaces, and access private resources.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Generating Access Tokens üîë\n",
    "\n",
    "#### What are Access Tokens?\n",
    "\n",
    "An **access token** is a secret key that authenticates your code when accessing Hugging Face resources. Think of it like a password specifically for your programs.\n",
    "\n",
    "#### When do you need a token?\n",
    "\n",
    "- Downloading **gated models** (models requiring agreement to terms)\n",
    "- Uploading models or datasets to the Hub\n",
    "- Accessing private repositories\n",
    "- Using Inference Endpoints\n",
    "\n",
    "For this tutorial, most models work without a token, but it's good practice to have one.\n",
    "\n",
    "#### How to create a token:\n",
    "\n",
    "1. Log in to [huggingface.co](https://huggingface.co)\n",
    "2. Click your profile picture ‚Üí **Settings**\n",
    "3. Navigate to **Access Tokens** in the left sidebar\n",
    "4. Click **New token**\n",
    "5. Give it a descriptive name (e.g., \"Colab Notebook\")\n",
    "6. Select **Read** access (unless you need to upload models)\n",
    "7. Click **Generate token**\n",
    "8. **Copy the token immediately** - you won't be able to see it again!\n",
    "\n",
    "#### üîí CRITICAL: Security Best Practices\n",
    "\n",
    "‚ö†Ô∏è **NEVER commit tokens to git repositories!**\n",
    "\n",
    "‚ö†Ô∏è **NEVER hardcode tokens in shared code!**\n",
    "\n",
    "‚ö†Ô∏è **NEVER share tokens in screenshots or documentation!**\n",
    "\n",
    "**Safe practices**:\n",
    "\n",
    "‚úÖ Store tokens in environment variables\n",
    "\n",
    "‚úÖ Use Colab Secrets (we'll show you how below)\n",
    "\n",
    "‚úÖ Use `.env` files that are gitignored\n",
    "\n",
    "‚úÖ Rotate tokens periodically (delete old ones, create new ones)\n",
    "\n",
    "‚úÖ Use \"Read\" tokens when you don't need write access\n",
    "\n",
    "If you accidentally expose a token:\n",
    "1. Go to your Access Tokens page immediately\n",
    "2. Delete the compromised token\n",
    "3. Create a new one\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Installing Required Libraries üì¶\n",
    "\n",
    "We'll need several libraries for this tutorial:\n",
    "\n",
    "- **`transformers`**: The main library for using pre-trained models\n",
    "- **`tokenizers`**: Fast tokenization (usually installed with transformers)\n",
    "- **`datasets`**: Easy access to datasets from the Hub\n",
    "- **`torch`**: PyTorch, the deep learning framework (backend)\n",
    "- **`accelerate`**: Simplifies running models on different hardware\n",
    "- **`sentencepiece`**: Tokenizer used by many models\n",
    "\n",
    "#### System Requirements\n",
    "\n",
    "**For pipelines and small models**:\n",
    "- Python 3.8+\n",
    "- 4GB RAM\n",
    "- No GPU required\n",
    "\n",
    "**For large models (like we'll use later)**:\n",
    "- 8GB+ RAM (16GB recommended)\n",
    "- GPU with 8GB+ VRAM (optional but much faster)\n",
    "\n",
    "**Good news**: Google Colab provides all of this for free!\n",
    "\n",
    "Let's install everything now:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# The -q flag makes installation quiet (less output)\n",
    "!pip install -q transformers tokenizers datasets torch accelerate sentencepiece protobuf\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"\\nüì¶ Installed packages:\")\n",
    "print(\"  - transformers: Core library for using pre-trained models\")\n",
    "print(\"  - tokenizers: Fast text tokenization\")\n",
    "print(\"  - datasets: Access to Hugging Face datasets\")\n",
    "print(\"  - torch: PyTorch deep learning framework\")\n",
    "print(\"  - accelerate: Hardware optimization\")\n",
    "print(\"  - sentencepiece: Tokenizer for many models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Setting Up Authentication (Optional)\n",
    "\n",
    "If you have a Hugging Face access token, you can configure it here. **This is optional for this tutorial** - most models we'll use don't require authentication.\n",
    "\n",
    "We'll use **Colab Secrets** to store the token securely:\n",
    "\n",
    "1. Click the üîë icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `HF_TOKEN`\n",
    "4. Value: Your Hugging Face token\n",
    "5. Enable notebook access\n",
    "\n",
    "Then run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load Hugging Face token from Colab secrets\n",
    "# This is optional - skip if you don't have a token\n",
    "\n",
    "HF_TOKEN = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ Hugging Face token loaded from Colab secrets\")\n",
    "    \n",
    "    # Log in to Hugging Face\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Successfully authenticated with Hugging Face\")\n",
    "    \n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No token found - continuing without authentication\")\n",
    "    print(\"   This is fine! Most models don't require authentication.\")\n",
    "    print(\"   You only need a token for gated models or uploading content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! You're now set up and ready to start using Hugging Face. Let's explore the Transformers library! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Transformers Library: High-Level APIs\n",
    "\n",
    "### 4.1 Philosophy: Making AI Accessible üéØ\n",
    "\n",
    "The `transformers` library is designed with a clear philosophy: **Make state-of-the-art AI accessible to everyone, regardless of their expertise level.**\n",
    "\n",
    "To achieve this, the library provides **three levels of abstraction**, like an elevator in a building:\n",
    "\n",
    "#### üè¢ Level 1: Pipelines (Top Floor - Easiest)\n",
    "- **Who it's for**: Beginners, rapid prototyping, quick solutions\n",
    "- **What you get**: One-line solutions for common tasks\n",
    "- **Trade-off**: Less control, but incredibly easy\n",
    "- **Example**: `classifier(\"I love this product!\")` ‚Üí `positive`\n",
    "\n",
    "#### üè¢ Level 2: Auto Classes (Middle Floor - Balanced)\n",
    "- **Who it's for**: Intermediate users who need more control\n",
    "- **What you get**: Access to specific models and tokenizers\n",
    "- **Trade-off**: More control, but requires understanding of tokenization\n",
    "- **Example**: Choose exactly which model, customize preprocessing\n",
    "\n",
    "#### üè¢ Level 3: Raw Models (Ground Floor - Most Control)\n",
    "- **Who it's for**: Advanced users, researchers, custom training\n",
    "- **What you get**: Complete control over every aspect\n",
    "- **Trade-off**: Most flexibility, but requires deep knowledge\n",
    "- **Example**: Access raw model weights, modify architecture\n",
    "\n",
    "**Design Principles**:\n",
    "\n",
    "1. **Progressive disclosure**: Start simple, add complexity when needed\n",
    "2. **Consistency**: Similar APIs across all models and tasks\n",
    "3. **Interoperability**: Works with PyTorch, TensorFlow, and JAX\n",
    "4. **Community-first**: Easy to share and reuse models\n",
    "\n",
    "üí° **Key Point**: You can start with pipelines and gradually move to lower levels as your needs become more sophisticated. You don't need to learn everything at once!\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Pipelines: The Easiest Way to Use Models üé¨\n",
    "\n",
    "**Pipelines** are the simplest way to use pre-trained models in Hugging Face. They handle everything for you:\n",
    "\n",
    "- ‚úÖ Downloading the right model\n",
    "- ‚úÖ Tokenizing input text\n",
    "- ‚úÖ Running inference\n",
    "- ‚úÖ Post-processing outputs\n",
    "- ‚úÖ Returning human-readable results\n",
    "\n",
    "All in **one line of code**!\n",
    "\n",
    "#### When to Use Pipelines\n",
    "\n",
    "‚úÖ **Perfect for**:\n",
    "- Quick prototypes and demos\n",
    "- Standard tasks with default settings\n",
    "- When you want results fast\n",
    "- Learning and exploration\n",
    "\n",
    "‚ùå **Not ideal for**:\n",
    "- Fine-grained control over preprocessing\n",
    "- Custom model modifications\n",
    "- Training or fine-tuning\n",
    "- Batch processing with specific performance requirements\n",
    "\n",
    "#### üìã Available Pipeline Tasks\n",
    "\n",
    "Hugging Face provides pipelines for dozens of tasks. Here are the most common:\n",
    "\n",
    "**Text Tasks**:\n",
    "- `text-classification` / `sentiment-analysis`: Classify text into categories\n",
    "- `text-generation`: Generate continuing text\n",
    "- `fill-mask`: Fill in masked words (like BERT)\n",
    "- `question-answering`: Answer questions based on context\n",
    "- `summarization`: Create summaries of long texts\n",
    "- `translation`: Translate between languages\n",
    "- `zero-shot-classification`: Classify without training examples\n",
    "- `ner` (Named Entity Recognition): Extract entities (names, places, etc.)\n",
    "- `conversational`: Multi-turn conversations\n",
    "\n",
    "**Audio Tasks**:\n",
    "- `automatic-speech-recognition`: Transcribe speech to text\n",
    "- `text-to-speech`: Convert text to audio\n",
    "- `audio-classification`: Classify audio clips\n",
    "\n",
    "**Image Tasks**:\n",
    "- `image-classification`: Classify images\n",
    "- `object-detection`: Detect objects in images\n",
    "- `image-segmentation`: Segment images into regions\n",
    "- `image-to-text`: Generate captions for images\n",
    "\n",
    "**Multimodal Tasks**:\n",
    "- `visual-question-answering`: Answer questions about images\n",
    "- `document-question-answering`: Extract info from document images\n",
    "\n",
    "#### Benefits and Limitations\n",
    "\n",
    "**‚úÖ Benefits**:\n",
    "- Extremely easy to use\n",
    "- Handles all complexity automatically\n",
    "- Great defaults chosen by experts\n",
    "- Perfect for rapid prototyping\n",
    "\n",
    "**‚ö†Ô∏è Limitations**:\n",
    "- Less control over preprocessing\n",
    "- May not be optimal for production at scale\n",
    "- Harder to debug when things go wrong\n",
    "- Some advanced features require lower-level APIs\n",
    "\n",
    "Let's see pipelines in action! üé™\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Deep Dive: Sentiment Analysis Pipeline\n",
    "\n",
    "Let's explore our first pipeline: **sentiment analysis**. This is one of the most popular NLP tasks and a great way to understand how pipelines work.\n",
    "\n",
    "#### üìñ What is Sentiment Analysis?\n",
    "\n",
    "**Sentiment analysis** (also called opinion mining) determines the emotional tone of text. It answers the question: \"Is this text positive, negative, or neutral?\"\n",
    "\n",
    "**Real-world use cases**:\n",
    "- Monitoring customer feedback and reviews\n",
    "- Social media monitoring (brand reputation)\n",
    "- Customer support ticket prioritization\n",
    "- Market research and survey analysis\n",
    "- Content moderation\n",
    "\n",
    "Let's create a sentiment analysis pipeline and see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline\n",
    "# This automatically downloads and loads a pre-trained model\n",
    "print(\"üîÑ Loading sentiment analysis model...\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Test it with a simple example\n",
    "result = sentiment_pipeline(\"I absolutely love this product! It's amazing!\")\n",
    "print(\"üìù Input: 'I absolutely love this product! It's amazing!'\")\n",
    "print(f\"üéØ Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "1. The pipeline downloaded a pre-trained sentiment model (stored in cache for reuse)\n",
    "2. It tokenized your text (converted words to numbers the model understands)\n",
    "3. It ran the model to get predictions\n",
    "4. It formatted the output in a human-readable format\n",
    "\n",
    "All in one line! Let's try more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different sentiments\n",
    "test_texts = [\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"The weather today is okay.\",\n",
    "    \"I'm not sure how I feel about this.\",\n",
    "    \"Incredible! Best purchase ever!\",\n",
    "    \"It's fine, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing multiple examples:\\n\")\n",
    "for text in test_texts:\n",
    "    result = sentiment_pipeline(text)[0]  # [0] because result is a list\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    \n",
    "    # Add emoji based on sentiment\n",
    "    emoji = \"üòä\" if label == \"POSITIVE\" else \"üòû\"\n",
    "    \n",
    "    print(f\"{emoji} '{text}'\")\n",
    "    print(f\"   ‚Üí {label} (confidence: {score:.1%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîç Understanding the Model-Tokenizer Relationship\n",
    "\n",
    "Now that you've seen a pipeline in action, let's understand what's happening under the hood. This is crucial for working with models effectively.\n",
    "\n",
    "#### Why Both Model and Tokenizer Are Needed\n",
    "\n",
    "**The fundamental problem**: Neural networks work with numbers, but we give them text. How do we bridge this gap?\n",
    "\n",
    "**The solution**: A two-part system:\n",
    "\n",
    "1. **Tokenizer**: Converts text ‚Üí numbers (encoding)\n",
    "2. **Model**: Processes numbers ‚Üí predictions\n",
    "\n",
    "Think of it like international communication:\n",
    "- **Tokenizer** = Translator (English ‚Üí Binary)\n",
    "- **Model** = Brain that processes the binary information\n",
    "- You need the **same translator** that the brain was trained with, or it won't understand!\n",
    "\n",
    "#### What Tokenizers Do: Text ‚Üí Tokens ‚Üí IDs\n",
    "\n",
    "Tokenization happens in steps:\n",
    "\n",
    "**Step 1: Text ‚Üí Tokens**\n",
    "```\n",
    "\"Hello, world!\" ‚Üí [\"Hello\", \",\", \"world\", \"!\"]\n",
    "```\n",
    "\n",
    "**Step 2: Tokens ‚Üí Token IDs**\n",
    "```\n",
    "[\"Hello\", \",\", \"world\", \"!\"] ‚Üí [7592, 11, 995, 0]\n",
    "```\n",
    "\n",
    "**Step 3: Feed to Model**\n",
    "```\n",
    "[7592, 11, 995, 0] ‚Üí Model ‚Üí Predictions\n",
    "```\n",
    "\n",
    "Let's see this in action by peeking inside the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the tokenizer and model inside the pipeline\n",
    "tokenizer = sentiment_pipeline.tokenizer\n",
    "model = sentiment_pipeline.model\n",
    "\n",
    "print(\"üîç Let's see what the tokenizer does:\\n\")\n",
    "\n",
    "# Example text\n",
    "text = \"I love Hugging Face!\"\n",
    "print(f\"üìù Original text: '{text}'\\n\")\n",
    "\n",
    "# Step 1: Tokenize (text ‚Üí tokens)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"üî§ Tokens: {tokens}\")\n",
    "\n",
    "# Step 2: Convert to IDs (tokens ‚Üí numbers)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"üî¢ Token IDs: {token_ids}\\n\")\n",
    "\n",
    "# We can also do both steps at once\n",
    "encoded = tokenizer(text)\n",
    "print(f\"‚ö° Quick encoding (does both steps): {encoded['input_ids']}\\n\")\n",
    "\n",
    "# And we can decode back to text\n",
    "decoded = tokenizer.decode(encoded['input_ids'])\n",
    "print(f\"üîÑ Decoded back to text: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Brief Overview of Tokenizer Types\n",
    "\n",
    "Different models use different tokenization strategies:\n",
    "\n",
    "**1. Word-based tokenization**\n",
    "- Splits on spaces and punctuation\n",
    "- Example: `\"Hello world\" ‚Üí [\"Hello\", \"world\"]`\n",
    "- Problem: Massive vocabulary, can't handle new words\n",
    "\n",
    "**2. Character-based tokenization**\n",
    "- Each character is a token\n",
    "- Example: `\"Hello\" ‚Üí [\"H\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "- Problem: Very long sequences, loses word meaning\n",
    "\n",
    "**3. Subword tokenization** (Most common - used by BERT, GPT, etc.)\n",
    "- Breaks words into meaningful pieces\n",
    "- Example: `\"unhappiness\" ‚Üí [\"un\", \"happiness\"]`\n",
    "- Advantages: Manageable vocabulary, handles new words, preserves meaning\n",
    "\n",
    "Common subword algorithms:\n",
    "- **BPE (Byte-Pair Encoding)**: Used by GPT, Llama\n",
    "- **WordPiece**: Used by BERT\n",
    "- **SentencePiece**: Used by T5, ALBERT\n",
    "\n",
    "üí° **Key Point**: Each model comes with its own tokenizer trained specifically for that model. You must use matching pairs - you can't mix and match!\n",
    "\n",
    "‚ö†Ô∏è **Common Mistake**: Using a different tokenizer than the model was trained with. This will give nonsensical results!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Hands-on: Sentiment Analysis with Real Data\n",
    "\n",
    "Now let's use our sentiment pipeline on a real dataset from Hugging Face. We'll analyze movie reviews from the IMDb dataset.\n",
    "\n",
    "First, let's load a small sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small sample of movie reviews\n",
    "print(\"üì• Loading IMDb movie review dataset...\")\n",
    "dataset = load_dataset(\"imdb\", split=\"test[:10]\")  # Load first 10 test examples\n",
    "print(f\"‚úÖ Loaded {len(dataset)} reviews\\n\")\n",
    "\n",
    "# Look at the first review\n",
    "print(\"üìù Example review:\")\n",
    "print(f\"Text: {dataset[0]['text'][:200]}...\")  # Show first 200 characters\n",
    "print(f\"Actual label: {dataset[0]['label']} (0=negative, 1=positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's analyze these reviews with our sentiment pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment for each review\n",
    "print(\"üé¨ Analyzing movie reviews:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fix: Iterate correctly over the dataset\n",
    "for i in range(min(5, len(dataset))):  # Analyze first 5\n",
    "    # Get the review text and actual label\n",
    "    review = dataset[i]  # Access each review individually\n",
    "    text = review['text']\n",
    "    actual_label = \"POSITIVE\" if review['label'] == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    # Truncate long reviews (models have max length limits)\n",
    "    text_preview = text[:150] + \"...\" if len(text) > 150 else text\n",
    "    \n",
    "    # Get prediction from pipeline\n",
    "    prediction = sentiment_pipeline(text)[0]\n",
    "    predicted_label = prediction['label']\n",
    "    confidence = prediction['score']\n",
    "    \n",
    "    # Check if prediction matches actual label\n",
    "    is_correct = actual_label == predicted_label\n",
    "    status_emoji = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{status_emoji} Review {i+1}:\")\n",
    "    print(f\"Text: {text_preview}\")\n",
    "    print(f\"Actual: {actual_label} | Predicted: {predicted_label} (confidence: {confidence:.1%})\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé® Customizing Parameters\n",
    "\n",
    "Pipelines allow some customization. Let's explore the options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify which model to use\n",
    "# Default is distilbert-base-uncased-finetuned-sst-2-english\n",
    "# Let's try a different sentiment model\n",
    "\n",
    "print(\"üîÑ Loading a different sentiment model...\\n\")\n",
    "\n",
    "# Using a Twitter-specific sentiment model\n",
    "twitter_sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "# This model has 3 labels: negative, neutral, positive\n",
    "test_tweets = [\n",
    "    \"@HuggingFace is awesome! üéâ\",\n",
    "    \"I don't really care either way.\",\n",
    "    \"This is terrible. Not happy at all.\"\n",
    "]\n",
    "\n",
    "print(\"üê¶ Testing with Twitter-trained model:\\n\")\n",
    "for tweet in test_tweets:\n",
    "    result = twitter_sentiment(tweet)[0]\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.1%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä Understanding Outputs\n",
    "\n",
    "Pipeline outputs are dictionaries with consistent structure:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        'label': 'POSITIVE',    # The predicted class\n",
    "        'score': 0.9998         # Confidence (probability between 0-1)\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "**Key points**:\n",
    "- Output is always a **list** (even for single inputs)\n",
    "- `label`: The predicted category\n",
    "- `score`: Confidence level (closer to 1.0 = more confident)\n",
    "\n",
    "**When to trust predictions**:\n",
    "- `score > 0.9`: Very confident, likely accurate\n",
    "- `score 0.7-0.9`: Confident, usually reliable\n",
    "- `score 0.5-0.7`: Uncertain, consider edge case\n",
    "- `score < 0.5`: Not confident (shouldn't happen with 2 classes)\n",
    "\n",
    "üí° **Key Takeaway**: Always check the confidence score, not just the label. Low confidence predictions may need human review.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Quick Examples of Other Pipelines\n",
    "\n",
    "Before we move deeper, let's quickly see a few other pipeline tasks in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation - continue a prompt\n",
    "print(\"üìù Text Generation Pipeline\\n\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"Artificial intelligence will\"\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,        # Maximum total tokens (input + output)\n",
    "    num_return_sequences=2,  # Generate 2 different completions\n",
    "    temperature=0.7       # Creativity (higher = more creative)\n",
    ")\n",
    "\n",
    "print(f\"üí≠ Prompt: '{prompt}'\\n\")\n",
    "for i, generation in enumerate(result, 1):\n",
    "    print(f\"{i}. {generation['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Zero-Shot Classification\n",
    "\n",
    "This is one of the most powerful pipelines - it classifies text into categories **you define**, without any training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification - classify without training examples!\n",
    "print(\"üéØ Zero-Shot Classification Pipeline\\n\")\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define our custom categories\n",
    "text = \"I need to reset my password because I forgot it.\"\n",
    "candidate_labels = [\"technical support\", \"billing inquiry\", \"product feedback\", \"account issue\"]\n",
    "\n",
    "result = classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"üìù Text: '{text}'\\n\")\n",
    "print(\"üè∑Ô∏è Classification results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    bar = \"‚ñà\" * int(score * 20)  # Visual bar\n",
    "    print(f\"  {label:20s} {bar} {score:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **This is incredibly powerful!** You can create a classifier for any categories you want without training any model!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running Models Locally: Full Control\n",
    "\n",
    "Now that you understand pipelines and the model-tokenizer relationship, let's go deeper. We'll run a powerful language model locally and see exactly how it works under the hood.\n",
    "\n",
    "### 6.1 Why Run Models Locally? üñ•Ô∏è\n",
    "\n",
    "You might wonder: \"If APIs like OpenAI are so convenient, why run models locally?\"\n",
    "\n",
    "#### ‚úÖ Advantages\n",
    "\n",
    "**Privacy and Security**\n",
    "- Your data never leaves your machine\n",
    "- No third-party can access your queries\n",
    "- Critical for sensitive data (medical, legal, financial)\n",
    "\n",
    "**Cost Efficiency**\n",
    "- No per-token charges\n",
    "- Pay once for hardware, use unlimited times\n",
    "- Better for high-volume applications\n",
    "\n",
    "**Offline Capability**\n",
    "- Works without internet connection\n",
    "- No dependency on API availability\n",
    "- Guaranteed uptime\n",
    "\n",
    "**Learning and Understanding**\n",
    "- See exactly how models work\n",
    "- Experiment with parameters\n",
    "- Build deeper intuition\n",
    "\n",
    "**Customization**\n",
    "- Full control over generation parameters\n",
    "- Can modify model architecture\n",
    "- Fine-tune for specific tasks\n",
    "\n",
    "#### ‚ùå Disadvantages\n",
    "\n",
    "**Resource Requirements**\n",
    "- Need powerful hardware (especially GPU)\n",
    "- RAM requirements can be high (8GB+ for good models)\n",
    "- Storage space for model files (2-20GB per model)\n",
    "\n",
    "**Speed Considerations**\n",
    "- Slower than hosted APIs (especially without GPU)\n",
    "- First run downloads large files\n",
    "\n",
    "**Complexity**\n",
    "- More code to write\n",
    "- Need to handle updates manually\n",
    "- Debugging can be harder\n",
    "\n",
    "**Model Quality**\n",
    "- Smaller models than latest commercial offerings\n",
    "- May not match GPT-4 quality\n",
    "\n",
    "#### üéØ When to Use Each Approach\n",
    "\n",
    "**Use APIs** (OpenAI, Anthropic) when:\n",
    "- Building quickly\n",
    "- Need best possible quality\n",
    "- Low to moderate volume\n",
    "- Don't have powerful hardware\n",
    "\n",
    "**Run locally** (Hugging Face) when:\n",
    "- Privacy is critical\n",
    "- High volume usage\n",
    "- Need offline capability\n",
    "- Learning and experimentation\n",
    "- Cost is a major concern\n",
    "\n",
    "üí° **Best practice**: Many companies use a hybrid approach - APIs for production quality, local models for development and experimentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Selection: Mistral-3B üéØ\n",
    "\n",
    "For this tutorial, we'll use **Mistral-3B**, an excellent model that balances quality and resource requirements.\n",
    "\n",
    "#### Why Mistral-3B?\n",
    "\n",
    "**Technical Specifications**:\n",
    "- **Size**: 3 billion parameters\n",
    "- **Memory**: ~6GB RAM minimum\n",
    "- **Context**: 32K tokens (very long conversations)\n",
    "- **Training**: High-quality curated data\n",
    "\n",
    "**Advantages**:\n",
    "‚úÖ Runs on free Colab (with GPU)\n",
    "‚úÖ Excellent quality for its size\n",
    "‚úÖ Fast inference\n",
    "‚úÖ Well-documented and popular\n",
    "‚úÖ Commercial-friendly license\n",
    "\n",
    "**Comparison to other options**:\n",
    "\n",
    "| Model | Parameters | RAM Needed | Quality | Speed |\n",
    "|-------|-----------|------------|---------|-------|\n",
    "| GPT-2 | 1.5B | 3GB | Good | Very Fast |\n",
    "| **Mistral-3B** | **3B** | **6GB** | **Excellent** | **Fast** |\n",
    "| Llama-7B | 7B | 14GB | Excellent | Medium |\n",
    "| Llama-13B | 13B | 26GB | Outstanding | Slow |\n",
    "\n",
    "üí° **Perfect for learning**: Mistral-3B is powerful enough to be impressive, yet small enough to run on modest hardware.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Downloading and Loading the Model üì•\n",
    "\n",
    "Let's download and load Mistral-3B. This will take a few minutes on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"   Note: Running on CPU will be slower\")\n",
    "\n",
    "print(\"\\nüì• Loading Mistral-3B model...\")\n",
    "print(\"   This will download ~6GB on first run (cached for future use)\")\n",
    "print(\"   Please wait 2-3 minutes...\\n\")\n",
    "\n",
    "# Model name on Hugging Face Hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Using 7B as 3B specific version\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üî§ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"   ‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "print(\"\\nü§ñ Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\",           # Automatically place on available device\n",
    "    low_cpu_mem_usage=True       # Optimize memory usage\n",
    ")\n",
    "print(\"   ‚úÖ Model loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ Mistral is ready to use!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìÇ Understanding Model Storage\n",
    "\n",
    "Let's see where the model files are stored and what they contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Models are cached in the Hugging Face cache directory\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "print(\"üìÇ Model Storage Information\\n\")\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "print(\"\\nHow caching works:\")\n",
    "print(\"  1. First time: Model downloads to cache (~6GB)\")\n",
    "print(\"  2. Subsequent times: Loads instantly from cache\")\n",
    "print(\"  3. Shared across projects: One download, use everywhere\")\n",
    "print(\"\\nüí° Tip: To save space, you can clear cache with:\")\n",
    "print(\"   !rm -rf ~/.cache/huggingface/hub\")\n",
    "print(\"   (but then you'll need to re-download models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üóÇÔ∏è Model File Structure\n",
    "\n",
    "A model download contains several important files:\n",
    "\n",
    "```\n",
    "model_directory/\n",
    "‚îú‚îÄ‚îÄ config.json              # Model architecture and settings\n",
    "‚îú‚îÄ‚îÄ tokenizer.json           # Tokenizer vocabulary and rules\n",
    "‚îú‚îÄ‚îÄ tokenizer_config.json    # Tokenizer settings\n",
    "‚îú‚îÄ‚îÄ special_tokens_map.json  # Special tokens (BOS, EOS, etc.)\n",
    "‚îú‚îÄ‚îÄ pytorch_model.bin        # Model weights (the big file!)\n",
    "‚îî‚îÄ‚îÄ README.md                # Model card documentation\n",
    "```\n",
    "\n",
    "**Key files explained**:\n",
    "\n",
    "- **`pytorch_model.bin`**: The actual neural network weights (largest file)\n",
    "- **`config.json`**: Architecture specs (layers, dimensions, etc.)\n",
    "- **`tokenizer.json`**: Vocabulary and tokenization rules\n",
    "- **`special_tokens_map.json`**: Important tokens like `[BOS]`, `[EOS]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Exploring the Model in Depth üî¨\n",
    "\n",
    "Now let's really understand how this model works.\n",
    "\n",
    "#### üèóÔ∏è Architecture Overview\n",
    "\n",
    "Mistral is a **decoder-only transformer** model, similar to GPT. Here's what that means:\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "1. **Token Embeddings**: Converts token IDs to dense vectors\n",
    "2. **Transformer Layers**: Stacked layers that process information\n",
    "   - Self-attention: \"Looks at\" all previous tokens\n",
    "   - Feed-forward: Processes each token independently\n",
    "3. **Output Layer**: Predicts next token probabilities\n",
    "\n",
    "**How it generates text**:\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "  ‚Üì Tokenize\n",
    "[2,1544,8718,525,356,464] \n",
    "  ‚Üì Embed\n",
    "[vector‚ÇÅ, vector‚ÇÇ, ...]\n",
    "  ‚Üì Transformer layers\n",
    "Process and attend to all tokens\n",
    "  ‚Üì Output layer\n",
    "Probabilities: {\"mat\": 0.6, \"chair\": 0.2, \"floor\": 0.15, ...}\n",
    "  ‚Üì Sample\n",
    "\"mat\" (most likely)\n",
    "```\n",
    "\n",
    "Let's see the model's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model architecture\n",
    "print(\"üèóÔ∏è  Model Architecture\\n\")\n",
    "print(f\"Model class: {model.__class__.__name__}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Vocabulary size: {model.config.vocab_size:,}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Number of attention heads: {model.config.num_attention_heads}\")\n",
    "print(f\"  Maximum position embeddings: {model.config.max_position_embeddings:,}\")\n",
    "\n",
    "print(\"\\nüí° What these numbers mean:\")\n",
    "print(\"  - Parameters: More = more capable (but slower and needs more RAM)\")\n",
    "print(\"  - Vocabulary size: Number of unique tokens the model knows\")\n",
    "print(\"  - Hidden size: Dimension of internal representations\")\n",
    "print(\"  - Layers: More = deeper understanding (but slower)\")\n",
    "print(\"  - Max positions: Maximum input length (32K tokens = ~24K words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### üî§ Tokenization Deep Dive\n",
    "\n",
    "Let's explore tokenization in detail with real examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî¨ Tokenization Deep Dive\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example 1: Simple sentence\n",
    "text1 = \"Hello, world!\"\n",
    "print(f\"\\nüìù Text: '{text1}'\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text1)\n",
    "print(f\"üî§ Tokens: {tokens}\")\n",
    "\n",
    "# Convert to IDs\n",
    "token_ids = tokenizer.encode(text1, add_special_tokens=False)\n",
    "print(f\"üî¢ Token IDs: {token_ids}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"üîÑ Decoded: '{decoded}'\")\n",
    "\n",
    "# Example 2: Subword tokenization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "text2 = \"unhappiness\"\n",
    "print(f\"\\nüìù Text: '{text2}'\")\n",
    "tokens2 = tokenizer.tokenize(text2)\n",
    "print(f\"üî§ Tokens: {tokens2}\")\n",
    "print(\"   Notice how 'unhappiness' breaks into meaningful parts!\")\n",
    "\n",
    "# Example 3: Handling unknown words\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "text3 = \"supercalifragilisticexpialidocious\"\n",
    "print(f\"\\nüìù Text: '{text3}'\")\n",
    "tokens3 = tokenizer.tokenize(text3)\n",
    "print(f\"üî§ Tokens: {tokens3}\")\n",
    "print(\"   Even made-up words can be tokenized using subword units!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéØ Special Tokens Explained\n",
    "\n",
    "Special tokens are important markers that give structure to the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Special Tokens\\n\")\n",
    "\n",
    "# Show special tokens\n",
    "print(\"Token          | Purpose\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"BOS (Beginning)| {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS (End)      | {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"PAD (Padding)  | {tokenizer.pad_token} (ID: {tokenizer.pad_token_id if tokenizer.pad_token else 'None'})\")\n",
    "print(f\"UNK (Unknown)  | {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "\n",
    "print(\"\\nüí° What they do:\")\n",
    "print(\"  BOS: Marks the start of a sequence\")\n",
    "print(\"  EOS: Marks the end - tells model to stop generating\")\n",
    "print(\"  PAD: Fills shorter sequences in batches to same length\")\n",
    "print(\"  UNK: Represents truly unknown tokens (rare with subword tokenization)\")\n",
    "\n",
    "# Show encoding with special tokens\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "text = \"Hello!\"\n",
    "print(f\"\\nüìù Original: '{text}'\")\n",
    "\n",
    "ids_without = tokenizer.encode(text, add_special_tokens=False)\n",
    "ids_with = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "print(f\"\\nWithout special tokens: {ids_without}\")\n",
    "print(f\"With special tokens:    {ids_with}\")\n",
    "print(\"\\nNotice the BOS token added at the beginning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìö Viewing the Vocabulary\n",
    "\n",
    "Let's peek at the model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "print(f\"üìö Vocabulary Information\\n\")\n",
    "print(f\"Total vocabulary size: {len(vocab):,} tokens\")\n",
    "\n",
    "# Show some random tokens\n",
    "import random\n",
    "sample_tokens = random.sample(list(vocab.items()), 15)\n",
    "\n",
    "print(\"\\nüé≤ Random sample of tokens:\")\n",
    "print(\"\\nToken          | ID\")\n",
    "print(\"-\" * 30)\n",
    "for token, idx in sorted(sample_tokens, key=lambda x: x[1]):\n",
    "    # Replace special characters for display\n",
    "    display_token = token.replace('‚ñÅ', '_').replace('\\n', '\\\\n')\n",
    "    print(f\"{display_token:15s}| {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### üîÑ The Generation Process: How Models Create Text\n",
    "\n",
    "Now for the most important part: understanding how the model generates text.\n",
    "\n",
    "**Autoregressive Generation** explained:\n",
    "\n",
    "1. **Start with prompt**: \"The weather today is\"\n",
    "2. **Model predicts next token**: Calculates probabilities for every token in vocabulary\n",
    "3. **Sample a token**: Choose one based on probabilities (e.g., \"beautiful\")\n",
    "4. **Append to prompt**: \"The weather today is beautiful\"\n",
    "5. **Repeat**: Use new prompt to predict next token\n",
    "6. **Stop**: When model generates EOS token or max length reached\n",
    "\n",
    "Let's see this step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"üîÑ Step-by-Step Text Generation\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Start with a prompt\n",
    "prompt = \"Artificial intelligence is\"\n",
    "print(f\"üí≠ Prompt: '{prompt}'\\n\")\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "print(f\"üî¢ Input IDs: {input_ids.tolist()[0]}\")\n",
    "\n",
    "# Generate one token at a time (manual generation loop)\n",
    "print(\"\\nüé≤ Generating tokens one at a time:\\n\")\n",
    "\n",
    "generated_ids = input_ids.clone()\n",
    "\n",
    "for step in range(5):  # Generate 5 tokens\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(generated_ids)\n",
    "        # Get logits for the last token\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # Get top 5 most likely tokens\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(f\"Step {step + 1}:\")\n",
    "    print(f\"  Current text: '{tokenizer.decode(generated_ids[0])}'\")\n",
    "    print(f\"  Top 5 next token predictions:\")\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"    '{token}' - {prob.item():.1%}\")\n",
    "    \n",
    "    # Sample the most likely token (greedy)\n",
    "    next_token_id = top_indices[0].unsqueeze(0).unsqueeze(0)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "    \n",
    "    print(f\"  ‚úÖ Chose: '{tokenizer.decode([top_indices[0]])}'\\n\")\n",
    "\n",
    "final_text = tokenizer.decode(generated_ids[0])\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚ú® Final generated text:\\n'{final_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üé® Sampling Strategies Explained\n",
    "\n",
    "There are different ways to choose the next token from the probability distribution:\n",
    "\n",
    "**1. Greedy Decoding** (always pick highest probability)\n",
    "- **Pro**: Deterministic, coherent\n",
    "- **Con**: Repetitive, boring\n",
    "- **Use case**: Factual questions, translations\n",
    "\n",
    "**2. Sampling** (randomly pick based on probabilities)\n",
    "- **Pro**: Diverse, creative\n",
    "- **Con**: Can be incoherent\n",
    "- **Use case**: Creative writing\n",
    "\n",
    "**3. Top-k Sampling** (only consider top k most likely tokens)\n",
    "- **Pro**: Balance between diversity and coherence\n",
    "- **Con**: Fixed k might be too restrictive or too loose\n",
    "- **Use case**: General text generation\n",
    "\n",
    "**4. Top-p (Nucleus) Sampling** (consider tokens until cumulative probability reaches p)\n",
    "- **Pro**: Adaptive to context (more options when uncertain)\n",
    "- **Con**: More complex\n",
    "- **Use case**: High-quality generation (most popular)\n",
    "\n",
    "**5. Temperature Scaling** (adjust probability distribution)\n",
    "- **Low temp (0.1-0.5)**: Confident, focused, deterministic\n",
    "- **Medium temp (0.7-1.0)**: Balanced (default is 1.0)\n",
    "- **High temp (1.5-2.0)**: Creative, diverse, random\n",
    "\n",
    "Let's compare different strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Comparing Sampling Strategies\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prompt = \"Once upon a time, there was a\"\n",
    "print(f\"üí≠ Prompt: '{prompt}'\\n\")\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Strategy 1: Greedy (deterministic)\n",
    "print(\"1Ô∏è‚É£ Greedy Decoding (always pick most likely):\")\n",
    "output_greedy = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=False,  # Greedy\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"   {tokenizer.decode(output_greedy[0], skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# Strategy 2: Temperature sampling (creative)\n",
    "print(\"2Ô∏è‚É£ High Temperature (creative, diverse):\")\n",
    "output_temp = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.5,  # High = more random\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"   {tokenizer.decode(output_temp[0], skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# Strategy 3: Top-p sampling (balanced)\n",
    "print(\"3Ô∏è‚É£ Top-p Sampling (balanced, high quality):\")\n",
    "output_topp = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,  # Consider tokens until 90% probability mass\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"   {tokenizer.decode(output_topp[0], skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# Strategy 4: Top-k sampling\n",
    "print(\"4Ô∏è‚É£ Top-k Sampling (controlled diversity):\")\n",
    "output_topk = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,  # Only consider top 50 tokens\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(f\"   {tokenizer.decode(output_topk[0], skip_special_tokens=True)}\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüí° Notice how different strategies produce different outputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõë Stopping Criteria\n",
    "\n",
    "Generation stops when:\n",
    "\n",
    "1. **EOS token generated**: Model decides it's done\n",
    "2. **Max length reached**: Safety limit to prevent infinite generation\n",
    "3. **Custom stopping criteria**: You can define custom rules\n",
    "\n",
    "üí° **Key Parameters**:\n",
    "- `max_length`: Maximum total tokens (input + output)\n",
    "- `max_new_tokens`: Maximum tokens to generate (clearer)\n",
    "- `min_length`: Force minimum length (prevents premature stopping)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Practical Applications üöÄ\n",
    "\n",
    "Now let's use our model for real applications!\n",
    "\n",
    "#### üí¨ Building a Simple Chatbot\n",
    "\n",
    "Let's create a basic chatbot that maintains conversation context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mistral(message, conversation_history=\"\", max_length=200):\n",
    "    \"\"\"\n",
    "    Simple chatbot using Mistral.\n",
    "    \n",
    "    Args:\n",
    "        message: User's message\n",
    "        conversation_history: Previous conversation (optional)\n",
    "        max_length: Maximum response length\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (response, updated_conversation_history)\n",
    "    \"\"\"\n",
    "    # Format the conversation\n",
    "    prompt = f\"{conversation_history}Human: {message}\\nAssistant:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    # Update conversation history\n",
    "    updated_history = f\"{conversation_history}Human: {message}\\nAssistant: {response}\\n\"\n",
    "    \n",
    "    return response, updated_history\n",
    "\n",
    "# Test the chatbot\n",
    "print(\"ü§ñ Chatbot Demo\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "conversation = \"\"\n",
    "\n",
    "# Turn 1\n",
    "user_msg = \"Hello! What can you help me with?\"\n",
    "print(f\"üë§ Human: {user_msg}\")\n",
    "response, conversation = chat_with_mistral(user_msg, conversation)\n",
    "print(f\"ü§ñ Assistant: {response}\\n\")\n",
    "\n",
    "# Turn 2\n",
    "user_msg = \"Tell me a fun fact about space.\"\n",
    "print(f\"üë§ Human: {user_msg}\")\n",
    "response, conversation = chat_with_mistral(user_msg, conversation)\n",
    "print(f\"ü§ñ Assistant: {response}\\n\")\n",
    "\n",
    "# Turn 3\n",
    "user_msg = \"That's interesting! Tell me more.\"\n",
    "print(f\"üë§ Human: {user_msg}\")\n",
    "response, conversation = chat_with_mistral(user_msg, conversation)\n",
    "print(f\"ü§ñ Assistant: {response}\\n\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úçÔ∏è Interactive Chatbot Loop\n",
    "\n",
    "Now let's create an interactive version you can chat with:\n",
    "\n",
    "‚ö†Ô∏è **Note**: In Colab, you'll need to type your messages and press Enter. Type 'quit' to exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Interactive Chatbot\")\n",
    "print(\"=\"*70)\n",
    "print(\"Chat with Mistral! Type your messages and press Enter.\")\n",
    "print(\"Type 'quit' or 'exit' to end the conversation.\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "conversation = \"\"\n",
    "\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_message = input(\"üë§ You: \").strip()\n",
    "    \n",
    "    # Check for exit commands\n",
    "    if user_message.lower() in ['quit', 'exit', 'bye']:\n",
    "        print(\"\\nü§ñ Assistant: Goodbye! Have a great day!\")\n",
    "        break\n",
    "    \n",
    "    # Skip empty messages\n",
    "    if not user_message:\n",
    "        continue\n",
    "    \n",
    "    # Generate response\n",
    "    print(\"ü§ñ Assistant: \", end=\"\", flush=True)\n",
    "    response, conversation = chat_with_mistral(user_message, conversation, max_length=150)\n",
    "    print(response + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **What we built**:\n",
    "- Maintains conversation context\n",
    "- Uses appropriate sampling for natural responses\n",
    "- Handles multi-turn conversations\n",
    "- Interactive input/output\n",
    "\n",
    "‚ö†Ô∏è **Limitations**:\n",
    "- Context window limit (~32K tokens)\n",
    "- No memory beyond current conversation\n",
    "- May generate incorrect information (hallucinations)\n",
    "- Not as sophisticated as ChatGPT\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercise üìù\n",
    "\n",
    "Time to apply what you've learned!\n",
    "\n",
    "### Exercise: Model Exploration\n",
    "\n",
    "**Your task**: Find a model on Hugging Face for a specific task and use it.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "1. **Choose a task** from the list:\n",
    "   - Text summarization\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Question Answering\n",
    "   - Translation (any language pair)\n",
    "\n",
    "2. **Find a model** on the Hugging Face Hub:\n",
    "   - Go to [huggingface.co/models](https://huggingface.co/models)\n",
    "   - Filter by your chosen task\n",
    "   - Pick a model with good downloads/likes\n",
    "   - Read the model card\n",
    "\n",
    "3. **Implement it** using a pipeline:\n",
    "   - Load the model with `pipeline()`\n",
    "   - Test it with at least 3 different inputs\n",
    "   - Print the results\n",
    "\n",
    "4. **Document your choice**:\n",
    "   - Why did you choose this model?\n",
    "   - What does the model card say about its capabilities?\n",
    "   - What are its limitations?\n",
    "\n",
    "#### Template Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Document your choice\n",
    "print(\"üìã Model Information\")\n",
    "print(\"=\"*70)\n",
    "print(\"Task chosen: [YOUR TASK HERE]\")\n",
    "print(\"Model name: [MODEL NAME FROM HUB]\")\n",
    "print(\"Why I chose it: [YOUR REASONING]\")\n",
    "print(\"Key capabilities: [FROM MODEL CARD]\")\n",
    "print(\"Known limitations: [FROM MODEL CARD]\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 2. Load the model\n",
    "# Replace 'task' and 'model' with your choices\n",
    "my_pipeline = pipeline(\n",
    "    task=\"YOUR-TASK\",\n",
    "    model=\"YOUR-MODEL-NAME\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# 3. Test with multiple inputs\n",
    "test_inputs = [\n",
    "    \"[YOUR FIRST TEST INPUT]\",\n",
    "    \"[YOUR SECOND TEST INPUT]\",\n",
    "    \"[YOUR THIRD TEST INPUT]\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the model:\\n\")\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Input: {test_input}\")\n",
    "    \n",
    "    result = my_pipeline(test_input)\n",
    "    print(f\"Output: {result}\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Solution (Summarization)\n",
    "\n",
    "Here's an example of a completed exercise using text summarization:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Document your choice\n",
    "print(\"üìã Model Information\")\n",
    "print(\"=\"*70)\n",
    "print(\"Task chosen: Text Summarization\")\n",
    "print(\"Model name: facebook/bart-large-cnn\")\n",
    "print(\"Why I chose it: High downloads, specifically trained on news articles\")\n",
    "print(\"Key capabilities: Generates concise summaries of long articles\")\n",
    "print(\"Known limitations: Best for news-style text, may struggle with technical content\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 2. Load the model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# 3. Test with multiple inputs\n",
    "article = \"\"\"\n",
    "Artificial intelligence has made significant progress in recent years.\n",
    "Machine learning models can now perform tasks that were once thought to \n",
    "require human intelligence. Deep learning, a subset of machine learning,\n",
    "uses neural networks with multiple layers to learn from large amounts of data.\n",
    "These models have achieved remarkable results in image recognition, natural\n",
    "language processing, and game playing.\n",
    "\"\"\"\n",
    "\n",
    "result = summarizer(article, max_length=50, min_length=20)\n",
    "print(f\"Summary: {result[0]['summary_text']}\")\n",
    "```\n",
    "\n",
    "üí° **Tips**:\n",
    "- Start with popular models (high downloads)\n",
    "- Read the model card carefully for usage examples\n",
    "- Don't worry if it takes time to load - first run always downloads\n",
    "- Experiment with different inputs to see strengths and weaknesses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways üéì\n",
    "\n",
    "Congratulations! You've completed the Hugging Face and Transformers tutorial. Let's recap what you've learned.\n",
    "\n",
    "---\n",
    "\n",
    "### üìö What We Covered\n",
    "\n",
    "#### 1. **Transfer Learning Fundamentals**\n",
    "- Why pre-trained models are powerful\n",
    "- How they save time, money, and resources\n",
    "- When to use them vs. training from scratch\n",
    "\n",
    "#### 2. **Hugging Face Ecosystem**\n",
    "- The Hub: Models, datasets, spaces, documentation\n",
    "- Model cards and how to evaluate models\n",
    "- Account setup and authentication\n",
    "- Security best practices\n",
    "\n",
    "#### 3. **Pipelines: Easy AI**\n",
    "- One-line solutions for common tasks\n",
    "- 20+ different pipeline tasks available\n",
    "- Sentiment analysis in depth\n",
    "- Zero-shot classification\n",
    "- Text generation\n",
    "\n",
    "#### 4. **Tokenization Deep Dive**\n",
    "- The model-tokenizer relationship\n",
    "- How text becomes numbers\n",
    "- Subword tokenization\n",
    "- Special tokens and their purposes\n",
    "\n",
    "#### 5. **Running Models Locally**\n",
    "- Loading Mistral-3B (or similar models)\n",
    "- Understanding model architecture\n",
    "- The generation process step-by-step\n",
    "- Sampling strategies (greedy, temperature, top-k, top-p)\n",
    "\n",
    "#### 6. **Practical Applications**\n",
    "- Building a chatbot\n",
    "- Interactive conversation systems\n",
    "- Real-world use cases\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Skills Gained\n",
    "\n",
    "You can now:\n",
    "\n",
    "‚úÖ Navigate the Hugging Face Hub and find appropriate models\n",
    "\n",
    "‚úÖ Read and understand model cards\n",
    "\n",
    "‚úÖ Use pipelines for instant AI capabilities\n",
    "\n",
    "‚úÖ Understand tokenization and the model-tokenizer relationship\n",
    "\n",
    "‚úÖ Load and run models locally with full control\n",
    "\n",
    "‚úÖ Customize generation parameters for different use cases\n",
    "\n",
    "‚úÖ Build practical applications like chatbots\n",
    "\n",
    "‚úÖ Work with datasets from the Hub\n",
    "\n",
    "‚úÖ Implement security best practices\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Important Reminders\n",
    "\n",
    "**Security**:\n",
    "- Never commit access tokens to repositories\n",
    "- Use environment variables for sensitive data\n",
    "- Rotate tokens periodically\n",
    "\n",
    "**Model Selection**:\n",
    "- Check licenses before commercial use\n",
    "- Read model cards to understand limitations\n",
    "- Consider model size vs. your hardware\n",
    "- Popular ‚â† always best for your use case\n",
    "\n",
    "**Generation Quality**:\n",
    "- Always check confidence scores\n",
    "- Models can hallucinate (generate false information)\n",
    "- Test thoroughly with diverse inputs\n",
    "- Have human review for critical applications\n",
    "\n",
    "**Performance**:\n",
    "- First run downloads models (can be slow)\n",
    "- Use GPU when available (much faster)\n",
    "- Consider batch processing for many inputs\n",
    "- Cache is your friend - don't delete unnecessarily\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "Ready to go further? Here are recommended next steps:\n",
    "\n",
    "**Immediate Practice**:\n",
    "1. Complete the exercise in section 8\n",
    "2. Try different models for the same task\n",
    "3. Experiment with generation parameters\n",
    "4. Build a small project using what you learned\n",
    "\n",
    "**Advanced Topics to Explore**:\n",
    "- **Fine-tuning**: Customize models for your specific data\n",
    "- **Quantization**: Run larger models with less memory\n",
    "- **Multi-modal models**: Work with images and text together\n",
    "- **RAG (Retrieval Augmented Generation)**: Combine models with databases\n",
    "- **Model deployment**: Put your models in production\n",
    "\n",
    "**Resources**:\n",
    "- üìñ [Hugging Face Course](https://huggingface.co/course) - Free comprehensive course\n",
    "- üìö [Transformers Documentation](https://huggingface.co/docs/transformers) - Official docs\n",
    "- üí¨ [Hugging Face Forums](https://discuss.huggingface.co) - Community help\n",
    "- üéì [Hugging Face YouTube](https://www.youtube.com/@HuggingFace) - Video tutorials\n",
    "- üìù [Papers with Code](https://paperswithcode.com) - Research papers and implementations\n",
    "\n",
    "---\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You've taken a significant step in your AI journey. You now have the knowledge and tools to:\n",
    "\n",
    "- Leverage state-of-the-art pre-trained models\n",
    "- Build practical AI applications\n",
    "- Understand how modern language models work\n",
    "- Continue learning and exploring on your own\n",
    "\n",
    "**Remember**: The best way to learn is by doing. Pick a project that interests you and start building. Don't be afraid to experiment, make mistakes, and ask questions in the community.\n",
    "\n",
    "The field of AI is evolving rapidly, and Hugging Face is at the forefront. Stay curious, keep learning, and most importantly - have fun building amazing things!\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Feedback\n",
    "\n",
    "If you found this tutorial helpful or have suggestions for improvement, consider:\n",
    "- Starring models you find useful on the Hub\n",
    "- Contributing to the community\n",
    "- Sharing your projects and learnings\n",
    "\n",
    "**Happy coding! üöÄü§ó**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
