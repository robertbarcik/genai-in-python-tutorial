{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjsBvxmPQ0xx"
   },
   "source": [
    "# Natural Language to SQL with OpenAI API\n",
    "## Enabling IT Managers to Query Support Data Without SQL Knowledge\n",
    "\n",
    "---\n",
    "\n",
    "### üìö What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to build a practical NLP-to-SQL pipeline that converts natural language questions into executable SQL queries. By the end, you'll have a working system that allows non-technical users to query IT support data using plain English.\n",
    "\n",
    "**End Goal:** Build a system that converts questions like *\"How many critical tickets are open?\"* into SQL queries, executes them against a database, and returns results in an understandable format.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ The Business Problem\n",
    "\n",
    "In IT support organizations, data-driven decision making is critical for operational efficiency and service quality. Managers and stakeholders constantly need to answer important questions about their service desk performance:\n",
    "\n",
    "- \"What's our current ticket backlog?\"\n",
    "- \"Which technician is most productive this month?\"\n",
    "- \"Are we meeting our SLA targets?\"\n",
    "- \"How many critical issues are unresolved?\"\n",
    "- \"What categories of problems are most common?\"\n",
    "\n",
    "**The Challenge:** Most managers and stakeholders don't know SQL or how to query databases directly. This creates a significant bottleneck in organizations.\n",
    "\n",
    "**Current Reality:**\n",
    "\n",
    "Organizations typically handle this in three ways, all with drawbacks:\n",
    "\n",
    "1. **Wait for technical staff to run queries** ‚Üí Slow, creates bottlenecks, technical staff become overwhelmed with reporting requests\n",
    "2. **Use pre-built dashboards** ‚Üí Limited flexibility, can't answer ad-hoc questions, doesn't adapt to changing business needs\n",
    "3. **Export data to Excel manually** ‚Üí Time-consuming, error-prone, data quickly becomes stale, doesn't scale\n",
    "\n",
    "These approaches create delays in decision-making, increase workload on technical staff, and prevent agile responses to emerging issues.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° The Solution: NLP-to-SQL Pipeline\n",
    "\n",
    "Large Language Models (LLMs) have a remarkable capability: they can understand natural language AND generate structured code, including SQL queries. We can leverage this to build an intelligent system that bridges the gap between business users and databases.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "Our system follows a multi-phase pipeline:\n",
    "\n",
    "1. **Schema Understanding** ‚Üí The LLM learns what tables and columns exist in our database\n",
    "2. **Question Analysis** ‚Üí A manager asks a question in plain English\n",
    "3. **SQL Generation** ‚Üí The LLM converts the question into a valid SQL query\n",
    "4. **Validation** ‚Üí We ensure the generated query is safe and correct\n",
    "5. **Execution** ‚Üí The query runs against our database\n",
    "6. **Results Presentation** ‚Üí Data is returned in an easy-to-understand format\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This technology has immediate practical applications:\n",
    "\n",
    "- ‚úÖ **Manager self-service reporting** without requiring SQL knowledge or technical training\n",
    "- ‚úÖ **Ad-hoc queries during incident reviews** to quickly investigate patterns or anomalies\n",
    "- ‚úÖ **Quick status checks for stakeholder meetings** to provide up-to-date metrics on demand\n",
    "- ‚úÖ **Data exploration for process improvement** to identify bottlenecks and optimization opportunities\n",
    "- ‚úÖ **Training tool for learning SQL patterns** by seeing how natural language maps to queries\n",
    "- ‚úÖ **Automated reporting workflows** that can be triggered by natural language commands\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin building this system step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwC_2pKkQ0x0"
   },
   "source": [
    "---\n",
    "\n",
    "## üîß SETUP SECTION\n",
    "\n",
    "### Step 1: Install Required Dependencies\n",
    "\n",
    "We need three main libraries:\n",
    "- **openai** ‚Üí To communicate with OpenAI's API\n",
    "- **pandas** ‚Üí For data manipulation and analysis\n",
    "- **sqlalchemy** ‚Üí For database connections and SQL execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3up5R1xQ0x1"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhTv5MiwQ0x2"
   },
   "source": [
    "### Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CMMQGI_Q0x2"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng6uUhiFQ0x2"
   },
   "source": [
    "### Step 3: Configure OpenAI API Key\n",
    "\n",
    "We'll try to load the API key from Google Colab secrets first (recommended for security), with a fallback to manual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVMSR-eLQ0x2"
   },
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"‚úÖ OpenAI client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duYYID1nQ0x2"
   },
   "source": [
    "---\n",
    "\n",
    "## üìä DATA PREPARATION SECTION\n",
    "\n",
    "### Understanding Our IT Service Desk Data\n",
    "\n",
    "In IT support environments, we typically track two primary entities: **tickets** (service requests, incidents, and problems) and **technicians** (the staff who resolve these tickets). Understanding this data structure is critical for both humans and the LLM.\n",
    "\n",
    "The LLM needs to know:\n",
    "- What tables exist in our database\n",
    "- What columns each table contains\n",
    "- What type of data each column holds (text, numbers, dates, etc.)\n",
    "- What values are typical for each column\n",
    "\n",
    "This context enables the LLM to generate accurate SQL queries. Without it, the LLM would be guessing at table and column names, leading to errors.\n",
    "\n",
    "Let's create realistic sample data that mirrors a real IT service desk system.\n",
    "\n",
    "### Dataset 1: Support Tickets\n",
    "\n",
    "Our tickets table represents individual support requests from customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N93zxWUQQ0x2"
   },
   "outputs": [],
   "source": [
    "# Generate sample IT support ticket data\n",
    "# Using dates relative to today for realistic time-based queries\n",
    "today = datetime.now()\n",
    "\n",
    "tickets_data = [\n",
    "    # Recent critical tickets (last week)\n",
    "    {'ticket_id': 'T001', 'title': 'Server outage - production environment down', 'category': 'Network Problem', 'priority': 'Critical', 'status': 'In Progress', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T002', 'title': 'Cannot access shared drive - entire department affected', 'category': 'Access Request', 'priority': 'Critical', 'status': 'Open', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Mike Chen', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T003', 'title': 'Email server not responding', 'category': 'Email Issue', 'priority': 'Critical', 'status': 'Resolved', 'created_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'assigned_to': 'Sarah Johnson', 'customer_company': 'DataSystems'},\n",
    "\n",
    "    # High priority tickets\n",
    "    {'ticket_id': 'T004', 'title': 'VPN connection keeps dropping', 'category': 'Network Problem', 'priority': 'High', 'status': 'In Progress', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Alex Rodriguez', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T005', 'title': 'Laptop won\\'t boot - blue screen error', 'category': 'Hardware Issue', 'priority': 'High', 'status': 'Open', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'HealthPlus'},\n",
    "    {'ticket_id': 'T006', 'title': 'Database connection timeout errors', 'category': 'Software Installation', 'priority': 'High', 'status': 'Resolved', 'created_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'TechCorp'},\n",
    "\n",
    "    # Medium priority tickets\n",
    "    {'ticket_id': 'T007', 'title': 'Need admin rights for new software installation', 'category': 'Access Request', 'priority': 'Medium', 'status': 'Open', 'created_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "    {'ticket_id': 'T008', 'title': 'Email not syncing to mobile device', 'category': 'Email Issue', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Mike Chen', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T009', 'title': 'Printer offline in conference room', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T010', 'title': 'Software license expired for Adobe Creative Suite', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'HealthPlus'},\n",
    "\n",
    "    # Low priority tickets\n",
    "    {'ticket_id': 'T011', 'title': 'Request to change desktop wallpaper policy', 'category': 'Access Request', 'priority': 'Low', 'status': 'Open', 'created_date': (today - timedelta(days=10)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Alex Rodriguez', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T012', 'title': 'Keyboard spacebar sticking occasionally', 'category': 'Hardware Issue', 'priority': 'Low', 'status': 'Closed', 'created_date': (today - timedelta(days=15)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=12)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "\n",
    "    # Password reset requests (common category)\n",
    "    {'ticket_id': 'T013', 'title': 'Password reset - forgot domain password', 'category': 'Password Reset', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T014', 'title': 'Account locked after too many failed login attempts', 'category': 'Password Reset', 'priority': 'High', 'status': 'Closed', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'assigned_to': 'Sarah Johnson', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T015', 'title': 'Need to reset multi-factor authentication', 'category': 'Password Reset', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'HealthPlus'},\n",
    "\n",
    "    # Network issues\n",
    "    {'ticket_id': 'T016', 'title': 'Slow internet connection in office', 'category': 'Network Problem', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T017', 'title': 'Cannot connect to Wi-Fi network', 'category': 'Network Problem', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=9)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'DataSystems'},\n",
    "\n",
    "    # Software installation requests\n",
    "    {'ticket_id': 'T018', 'title': 'Install Zoom for remote meetings', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=11)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=10)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T019', 'title': 'Microsoft Office needs to be updated', 'category': 'Software Installation', 'priority': 'Low', 'status': 'Closed', 'created_date': (today - timedelta(days=14)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=13)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T020', 'title': 'Install Python development environment', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Open', 'created_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Emma Davis', 'customer_company': 'TechCorp'},\n",
    "\n",
    "    # Additional variety\n",
    "    {'ticket_id': 'T021', 'title': 'Laptop running very slow - needs optimization', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'HealthPlus'},\n",
    "    {'ticket_id': 'T022', 'title': 'External monitor not detected', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=12)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=11)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "    {'ticket_id': 'T023', 'title': 'Need access to finance folder on SharePoint', 'category': 'Access Request', 'priority': 'High', 'status': 'Closed', 'created_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T024', 'title': 'Email attachments not downloading', 'category': 'Email Issue', 'priority': 'High', 'status': 'Resolved', 'created_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T025', 'title': 'Spam emails getting through filter', 'category': 'Email Issue', 'priority': 'Low', 'status': 'Open', 'created_date': (today - timedelta(days=13)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'HealthPlus'},\n",
    "\n",
    "    # Recent unassigned tickets\n",
    "    {'ticket_id': 'T026', 'title': 'New hire needs laptop setup', 'category': 'Hardware Issue', 'priority': 'High', 'status': 'Open', 'created_date': today.strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': None, 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T027', 'title': 'Conference room TV not working', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Open', 'created_date': today.strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': None, 'customer_company': 'DataSystems'},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "tickets_df = pd.DataFrame(tickets_data)\n",
    "\n",
    "print(\"üìã SUPPORT TICKETS DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total tickets: {len(tickets_df)}\")\n",
    "print(f\"Date range: {tickets_df['created_date'].min()} to {tickets_df['created_date'].max()}\")\n",
    "print(\"\\nFirst 10 tickets:\")\n",
    "display(tickets_df.head(10))\n",
    "\n",
    "print(\"\\nüìä Ticket Statistics:\")\n",
    "print(f\"  Status breakdown: {tickets_df['status'].value_counts().to_dict()}\")\n",
    "print(f\"  Priority breakdown: {tickets_df['priority'].value_counts().to_dict()}\")\n",
    "print(f\"  Category breakdown: {tickets_df['category'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uoaPgOtQ0x2"
   },
   "source": [
    "### Dataset 2: Technicians\n",
    "\n",
    "Our technicians table represents the IT support staff who handle tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5Fjy0NxQ0x3"
   },
   "outputs": [],
   "source": [
    "# Generate technician data\n",
    "technicians_data = [\n",
    "    {'tech_id': 1, 'name': 'John Smith', 'specialization': 'Hardware', 'hire_date': '2020-03-15', 'availability': 'Full-time'},\n",
    "    {'tech_id': 2, 'name': 'Sarah Johnson', 'specialization': 'Network', 'hire_date': '2019-07-22', 'availability': 'Full-time'},\n",
    "    {'tech_id': 3, 'name': 'Mike Chen', 'specialization': 'Software', 'hire_date': '2021-01-10', 'availability': 'Full-time'},\n",
    "    {'tech_id': 4, 'name': 'Emma Davis', 'specialization': 'Software', 'hire_date': '2021-09-05', 'availability': 'Full-time'},\n",
    "    {'tech_id': 5, 'name': 'Alex Rodriguez', 'specialization': 'General', 'hire_date': '2022-05-18', 'availability': 'Part-time'},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "technicians_df = pd.DataFrame(technicians_data)\n",
    "\n",
    "print(\"üë• TECHNICIANS DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total technicians: {len(technicians_df)}\")\n",
    "display(technicians_df)\n",
    "\n",
    "print(\"\\nüìä Technician Statistics:\")\n",
    "print(f\"  Specializations: {technicians_df['specialization'].value_counts().to_dict()}\")\n",
    "print(f\"  Availability: {technicians_df['availability'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz5f7zIFQ0x3"
   },
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è DATABASE SETUP SECTION\n",
    "\n",
    "\n",
    "In production environments, we typically use enterprise databases like PostgreSQL, MySQL, or Microsoft SQL Server. However, for learning and demonstration purposes, we'll use SQLite.\n",
    "\n",
    "The NLP-to-SQL pipeline we build here will work with any SQL database with minimal modifications.\n",
    "\n",
    "### Create SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfYMK7O2Q0x3"
   },
   "outputs": [],
   "source": [
    "# Create in-memory SQLite database\n",
    "# Note: Using in-memory database for this demo (data lost when kernel restarts)\n",
    "# For persistence, use: create_engine('sqlite:///service_desk.db')\n",
    "temp_db = create_engine('sqlite:///:memory:')\n",
    "\n",
    "print(\"‚úÖ SQLite database created in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK5swkk3Q0x3"
   },
   "source": [
    "### Load Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-MIEqZYQ0x3"
   },
   "outputs": [],
   "source": [
    "# Load DataFrames into SQLite database as tables\n",
    "tickets_df.to_sql('tickets', temp_db, index=False, if_exists='replace')\n",
    "technicians_df.to_sql('technicians', temp_db, index=False, if_exists='replace')\n",
    "\n",
    "print(\"‚úÖ Data loaded into database\")\n",
    "print(\"  - tickets table created with {} records\".format(len(tickets_df)))\n",
    "print(\"  - technicians table created with {} records\".format(len(technicians_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnnDAqPvQ0x3"
   },
   "source": [
    "### Verify Database Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZS_neD-3Q0x3"
   },
   "outputs": [],
   "source": [
    "# Query to verify tables exist\n",
    "with temp_db.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT name FROM sqlite_master WHERE type='table'\"))\n",
    "    tables = result.fetchall()\n",
    "    print(\"\\nüìä Tables in database:\")\n",
    "    for table in tables:\n",
    "        print(f\"  - {table[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlRGRSkEQ0x3"
   },
   "source": [
    "### Test Database with Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpB4rNcUQ0x3"
   },
   "outputs": [],
   "source": [
    "# Example: Get first 5 tickets from database\n",
    "with temp_db.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT ticket_id, title, priority, status FROM tickets LIMIT 5\"))\n",
    "    rows = result.fetchall()\n",
    "\n",
    "print(\"\\nüîç Sample Query: First 5 tickets\")\n",
    "print(\"=\" * 80)\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n‚úÖ Database is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBdHDxZcQ0x3"
   },
   "source": [
    "---\n",
    "\n",
    "## üîÑ UNDERSTANDING THE WORKFLOW\n",
    "\n",
    "\n",
    "\n",
    "#### **Phase 1: Schema Context Building** üèóÔ∏è\n",
    "- Generate a comprehensive description of our database structure\n",
    "\n",
    "\n",
    "#### **Phase 2: Natural Language Input** üí¨\n",
    "- User asks a question in plain English\n",
    "\n",
    "#### **Phase 3: SQL Generation** ü§ñ\n",
    "- Send the schema context + user question to gpt-5-nano\n",
    "\n",
    "\n",
    "#### **Phase 4: Query Validation & Cleaning** üõ°Ô∏è\n",
    "- Extract just the SQL from the LLM response (remove explanatory text)\n",
    "\n",
    "\n",
    "#### **Phase 5: Execution** ‚öôÔ∏è\n",
    "- Execute the validated query against our SQLite database\n",
    "\n",
    "\n",
    "#### **Phase 6: Results Presentation** üìä\n",
    "- Display results in a user-friendly format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now let's implement each phase step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-MvetIhQ0x3"
   },
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è PHASE 1: SCHEMA CONTEXT BUILDING\n",
    "\n",
    "### Teaching the LLM About Our Database\n",
    "\n",
    "Large Language Models don't have built-in knowledge of your specific database. They know SQL syntax and general database concepts, but they don't know what tables YOU have or what columns exist in YOUR database.\n",
    "\n",
    "We must explicitly tell the LLM:\n",
    "- **What tables we have** ‚Üí \"tickets\" and \"technicians\"\n",
    "- **What columns each table contains** ‚Üí column names and their purposes\n",
    "- **What type of data each column holds** ‚Üí text, integers, dates, etc.\n",
    "- **Example values** ‚Üí Show the LLM what actual data looks like\n",
    "\n",
    "This is called **schema context** or **table definitions**. The more detailed and accurate our description, the better the SQL generation will be.\n",
    "\n",
    "\n",
    "### Build Schema Context Function\n",
    "\n",
    "**What this function does:** This function creates a structured text description of a database table that the LLM can understand. It takes a table name and a pandas DataFrame, then generates a detailed summary including all column names with their data types and a few sample rows showing actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoQvEqsfQ0x3"
   },
   "outputs": [],
   "source": [
    "def generate_table_context(table_name, df, num_samples=3):\n",
    "    \"\"\"\n",
    "    Generate schema context for a single table\n",
    "\n",
    "    This creates a detailed description of the table structure that the LLM can understand.\n",
    "\n",
    "    Args:\n",
    "        table_name: Name of the database table\n",
    "        df: Pandas DataFrame containing the table data\n",
    "        num_samples: How many sample rows to include\n",
    "\n",
    "    Returns:\n",
    "        String containing formatted table description\n",
    "    \"\"\"\n",
    "\n",
    "    context = f\"\\nTable: {table_name}\\n\"\n",
    "    context += \"Columns:\\n\"\n",
    "\n",
    "    # Add column information with data types\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        context += f\"  - {col} ({dtype})\\n\"\n",
    "\n",
    "    # Add sample values to show the LLM what data looks like\n",
    "    context += f\"\\nSample rows (first {num_samples}):\\n\"\n",
    "    sample_rows = df.head(num_samples).to_dict('records')\n",
    "    for i, row in enumerate(sample_rows, 1):\n",
    "        context += f\"  Row {i}: {row}\\n\"\n",
    "\n",
    "    return context\n",
    "\n",
    "print(\"‚úÖ Schema context function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvcQJaH1Q0x3"
   },
   "source": [
    "### Generate Schema Context for Both Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkKJFXQ9Q0x3"
   },
   "outputs": [],
   "source": [
    "# Generate context for tickets table\n",
    "tickets_context = generate_table_context('tickets', tickets_df, num_samples=3)\n",
    "\n",
    "# Generate context for technicians table\n",
    "technicians_context = generate_table_context('technicians', technicians_df, num_samples=3)\n",
    "\n",
    "# Combine into complete database schema context\n",
    "database_schema = tickets_context + \"\\n\" + technicians_context\n",
    "\n",
    "print(\"üìã DATABASE SCHEMA CONTEXT\")\n",
    "print(\"=\" * 80)\n",
    "print(database_schema)\n",
    "print(\"\\n‚úÖ Schema context generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnxN7fyLQ0x4"
   },
   "source": [
    "### Build Complete System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhCLe7K-Q0x4"
   },
   "outputs": [],
   "source": [
    "# Create the system prompt that will be sent to the LLM\n",
    "# This prompt gives the LLM its role, context, and instructions\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert SQL query generator for an IT Service Desk database.\n",
    "\n",
    "Your task is to convert natural language questions into valid SQL queries.\n",
    "\n",
    "DATABASE SCHEMA:\n",
    "{database_schema}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Generate ONLY SELECT queries (no INSERT, UPDATE, DELETE, DROP)\n",
    "2. Use proper SQL syntax for SQLite\n",
    "3. Return ONLY the SQL query, nothing else\n",
    "4. End the query with a semicolon\n",
    "5. Use appropriate JOINs when data from multiple tables is needed\n",
    "6. Use COUNT, SUM, AVG, etc. for aggregation questions\n",
    "7. Use WHERE clauses to filter data based on conditions\n",
    "8. Use ORDER BY and LIMIT when appropriate\n",
    "\n",
    "IMPORTANT:\n",
    "- Return ONLY the SQL query\n",
    "- Do not include explanations or markdown formatting\n",
    "- Do not use code blocks (```)\n",
    "- The query should be executable as-is\n",
    "\n",
    "EXAMPLE:\n",
    "Question: \"How many open tickets are there?\"\n",
    "Response: SELECT COUNT(*) FROM tickets WHERE status = 'Open';\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ System prompt created\")\n",
    "print(\"\\nThis prompt gives the LLM everything it needs to generate accurate SQL queries for our IT service desk database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZKevgcwQ0x4"
   },
   "source": [
    "---\n",
    "\n",
    "## ü§ñ PHASE 2 & 3: NATURAL LANGUAGE INPUT AND SQL GENERATION\n",
    "\n",
    "### Converting Questions to SQL\n",
    "\n",
    "Now that the LLM understands our database schema, it can generate SQL queries from natural language questions. The process works like this:\n",
    "\n",
    "1. We send the **system prompt** (which contains the schema) to establish context\n",
    "2. We send the **user's question** as a message\n",
    "3. The LLM analyzes the question and generates appropriate SQL\n",
    "4. The LLM returns the query as a response\n",
    "\n",
    "\n",
    "### Build Query Generation Function\n",
    "\n",
    "This is the core function that connects to OpenAI's API and transforms a natural language question into a SQL query. It sends both the system prompt (containing our database schema) and the user's question to gpt-5-nano, then retrieves the generated SQL. The function uses `temperature=0` to ensure consistent, deterministic outputs (the same question always produces the same query), which is important for reliability. It also includes error handling to catch API failures gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQUJZAakQ0x4"
   },
   "outputs": [],
   "source": [
    "def generate_sql_query(question, system_prompt):\n",
    "    \"\"\"\n",
    "    Use OpenAI API to convert natural language to SQL\n",
    "\n",
    "    Args:\n",
    "        question: Natural language question from user\n",
    "        system_prompt: System prompt with database schema\n",
    "\n",
    "    Returns:\n",
    "        Generated SQL query as string\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüí¨ Question: {question}\")\n",
    "    print(\"ü§ñ Generating SQL query...\")\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI API using responses.create\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=f\"{system_prompt}\\n\\nUser Question: {question}\"\n",
    "        )\n",
    "\n",
    "        # Extract the generated SQL\n",
    "        sql_query = response.output_text.strip()\n",
    "\n",
    "        print(\"‚úÖ SQL query generated\")\n",
    "        return sql_query\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating SQL: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Query generation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n47_1izHQ0x4"
   },
   "source": [
    "### Test with First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7OCDOp6Q0x4"
   },
   "outputs": [],
   "source": [
    "# Test the SQL generation with a simple question\n",
    "test_question = \"How many critical tickets are currently open?\"\n",
    "\n",
    "generated_sql = generate_sql_query(test_question, system_prompt)\n",
    "\n",
    "if generated_sql:\n",
    "    print(\"\\nüìù Generated SQL:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(generated_sql)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFJkHyRQQ0x4"
   },
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è PHASE 4: QUERY VALIDATION AND CLEANING\n",
    "\n",
    "### Security and Safety Considerations\n",
    "\n",
    "Large Language Models can generate code, but we should **never blindly execute it**. This is a critical security principle. Even though we've instructed the LLM to only generate SELECT queries, we must validate this programmatically.\n",
    "\n",
    "**Security Risks:**\n",
    "\n",
    "1. **SQL Injection** ‚Üí Malicious users might try to craft questions that generate harmful queries\n",
    "   - Example: A question designed to drop tables or access sensitive data\n",
    "   - We must validate that only safe operations are allowed\n",
    "\n",
    "2. **Incorrect Queries** ‚Üí The LLM might generate invalid or malformed SQL\n",
    "   - Syntax errors could crash the application\n",
    "   - We need to catch these before execution\n",
    "\n",
    "3. **Harmful Operations** ‚Üí We only want SELECT queries, not DELETE/UPDATE/DROP\n",
    "   - Even accidental data modification could be disastrous\n",
    "   - Strict validation prevents this\n",
    "\n",
    "**Our Solution: Two-Step Validation Process**\n",
    "\n",
    "1. **Extract the SQL query** ‚Üí Remove any explanatory text the LLM might have included\n",
    "2. **Validate it's safe** ‚Üí Ensure it's a SELECT query with no forbidden keywords\n",
    "\n",
    "\n",
    "### Build SQL Extraction Function\n",
    "\n",
    "This function acts as a \"cleaning service\" for the LLM's response. Sometimes the LLM might return extra text like \"Here's the query you requested:\" or wrap the SQL in markdown code blocks. This function uses regex pattern matching to locate and extract ONLY the actual SQL SELECT statement from the response, stripping away any formatting or explanation. It searches for patterns that match SELECT queries ending with semicolons, ensuring we get clean, executable SQL code ready for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5mEpnsmQ0x5"
   },
   "outputs": [],
   "source": [
    "def extract_sql_query(text):\n",
    "    \"\"\"\n",
    "    Extracts SQL SELECT query from LLM response text\n",
    "\n",
    "    Handles cases where LLM includes:\n",
    "    - Explanation before/after the query\n",
    "    - SQL wrapped in markdown code blocks\n",
    "    - Multiple queries (takes the first)\n",
    "\n",
    "    Args:\n",
    "        text: Raw response from LLM\n",
    "\n",
    "    Returns:\n",
    "        Cleaned SQL query string or None if no valid query found\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove markdown code blocks if present\n",
    "    text = re.sub(r'```sql\\n?', '', text)\n",
    "    text = re.sub(r'```\\n?', '', text)\n",
    "\n",
    "    # Pattern to match SELECT queries\n",
    "    # Matches: SELECT ... ; (case insensitive, multiline)\n",
    "    pattern = r'SELECT\\s+.*?;'\n",
    "\n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        query = match.group(0).strip()\n",
    "        return query\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid SELECT query found in response\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ SQL extraction function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1ru8FE6Q0x5"
   },
   "source": [
    "### Build SQL Validation Function\n",
    "\n",
    "This function performs multiple safety checks on the extracted SQL query before allowing it to execute. First, it verifies the query starts with SELECT (read-only operation). Then it scans for dangerous keywords like DELETE, DROP, UPDATE, INSERT, or ALTER that could modify or destroy data. The function uses regex word boundaries (`\\b`) to match these keywords as complete words only - this prevents false positives where \"CREATE\" might appear as part of column names like \"created_date\". This function implements the principle of \"whitelist validation\" - only allowing explicitly safe operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOK32odrQ0x5"
   },
   "outputs": [],
   "source": [
    "def validate_sql_query(query):\n",
    "    \"\"\"\n",
    "    Validates that SQL query is safe to execute\n",
    "\n",
    "    Security checks:\n",
    "    - Must be a SELECT statement\n",
    "    - Cannot contain DELETE, DROP, UPDATE, INSERT, ALTER\n",
    "    - Must use only allowed tables (tickets, technicians)\n",
    "\n",
    "    Args:\n",
    "        query: SQL query string to validate\n",
    "\n",
    "    Returns:\n",
    "        Boolean: True if safe, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    if not query:\n",
    "        return False\n",
    "\n",
    "    # Convert to uppercase for checking\n",
    "    query_upper = query.upper()\n",
    "\n",
    "    # Must start with SELECT\n",
    "    if not query_upper.strip().startswith('SELECT'):\n",
    "        print(\"‚ùå Query must start with SELECT\")\n",
    "        return False\n",
    "\n",
    "    # Forbidden operations - check for whole words only using word boundaries\n",
    "    forbidden_keywords = ['DELETE', 'DROP', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE', 'EXEC', 'EXECUTE']\n",
    "    for keyword in forbidden_keywords:\n",
    "        # Use regex with word boundaries to match whole words only\n",
    "        # This prevents matching \"CREATE\" in \"created_date\"\n",
    "        pattern = r'\\b' + keyword + r'\\b'\n",
    "        if re.search(pattern, query_upper):\n",
    "            print(f\"‚ùå Forbidden keyword detected: {keyword}\")\n",
    "            return False\n",
    "\n",
    "    # Allowed tables only (basic check)\n",
    "    allowed_tables = ['tickets', 'technicians']\n",
    "\n",
    "    print(\"‚úÖ Query passed validation\")\n",
    "    return True\n",
    "\n",
    "print(\"‚úÖ SQL validation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7hVW1ggQ0x5"
   },
   "source": [
    "### Build Combined Processing Function\n",
    "\n",
    "This function combines the previous two steps (extraction and validation) into one convenient workflow. It takes the raw text response from the LLM and runs it through both the cleaning process (extract_sql_query) and the security checks (validate_sql_query). Only if both steps succeed does the function return the validated query. If either step fails, it returns None, preventing any unsafe SQL from reaching the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odbv9EXMQ0x5"
   },
   "outputs": [],
   "source": [
    "def process_generated_sql(raw_sql):\n",
    "    \"\"\"\n",
    "    Process raw LLM output: extract and validate SQL query\n",
    "\n",
    "    Args:\n",
    "        raw_sql: Raw text from LLM\n",
    "\n",
    "    Returns:\n",
    "        Validated SQL query or None if invalid\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüõ°Ô∏è Processing and validating SQL...\")\n",
    "\n",
    "    # Step 1: Extract SQL\n",
    "    query = extract_sql_query(raw_sql)\n",
    "\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    print(f\"üìù Extracted query: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "\n",
    "    # Step 2: Validate\n",
    "    if validate_sql_query(query):\n",
    "        return query\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Combined processing function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eb-QMQfQ0x5"
   },
   "source": [
    "### Test the Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLbDPI8pQ0x5"
   },
   "outputs": [],
   "source": [
    "# Test with our previously generated SQL\n",
    "if generated_sql:\n",
    "    validated_query = process_generated_sql(generated_sql)\n",
    "\n",
    "    if validated_query:\n",
    "        print(\"\\n‚úÖ VALIDATION SUCCESSFUL\")\n",
    "        print(\"Query is safe to execute:\")\n",
    "        print(validated_query)\n",
    "    else:\n",
    "        print(\"\\n‚ùå VALIDATION FAILED\")\n",
    "        print(\"Query was rejected by security checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXkppCd7Q0x6"
   },
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è PHASE 5: QUERY EXECUTION\n",
    "\n",
    "### Running SQL Against the Database\n",
    "\n",
    "Now we have a validated, safe SQL query that we're confident is secure to execute. The next phase is to actually run it against our SQLite database and retrieve the results.\n",
    "\n",
    "\n",
    "### Build Query Execution Function\n",
    "\n",
    "It uses SQLAlchemy's connection context manager (`with` statement) which automatically handles opening and closing the database connection safely, even if errors occur. The function executes the SQL using `conn.execute(text(query))` and fetches all result rows as tuples. It includes comprehensive error handling to catch any database errors (like syntax issues or non-existent columns) and reports them gracefully instead of crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en0VfBowQ0x6"
   },
   "outputs": [],
   "source": [
    "def execute_sql_query(query, database_engine):\n",
    "    \"\"\"\n",
    "    Execute SQL query against the database\n",
    "\n",
    "    Args:\n",
    "        query: Validated SQL query string\n",
    "        database_engine: SQLAlchemy engine connected to database\n",
    "\n",
    "    Returns:\n",
    "        List of result rows (tuples) or None if error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n‚öôÔ∏è Executing query...\")\n",
    "\n",
    "    try:\n",
    "        # Use context manager for safe connection handling\n",
    "        with database_engine.connect() as conn:\n",
    "            # Execute the query\n",
    "            result = conn.execute(text(query))\n",
    "\n",
    "            # Fetch all rows\n",
    "            rows = result.fetchall()\n",
    "\n",
    "            print(f\"‚úÖ Query executed successfully\")\n",
    "            print(f\"üìä Returned {len(rows)} row(s)\")\n",
    "\n",
    "            return rows\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Query execution function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4TIpQFQ0x6"
   },
   "source": [
    "### Execute Our First Complete Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoseaL79Q0x6"
   },
   "outputs": [],
   "source": [
    "# Execute the validated query we generated earlier\n",
    "if validated_query:\n",
    "    results = execute_sql_query(validated_query, temp_db)\n",
    "\n",
    "    if results is not None:\n",
    "        print(\"\\nüìä QUERY RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Question: {test_question}\")\n",
    "        print(f\"SQL Query: {validated_query}\")\n",
    "        print(f\"\\nResults:\")\n",
    "        for row in results:\n",
    "            print(f\"  {row}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n‚úÖ First complete NLP-to-SQL pipeline execution successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1iRVytKQ0x7"
   },
   "source": [
    "---\n",
    "\n",
    "## üéØ PHASE 6: COMPLETE PIPELINE FUNCTION\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "We've successfully built and tested all the individual pieces of our NLP-to-SQL pipeline:\n",
    "\n",
    "1. ‚úÖ Schema context generation\n",
    "2. ‚úÖ SQL query generation with LLM\n",
    "3. ‚úÖ Query validation and security checks\n",
    "4. ‚úÖ Query execution against database\n",
    "\n",
    "Now we'll combine these components into a single, easy-to-use function that handles the entire pipeline automatically. This function will:\n",
    "\n",
    "- Take a natural language question as input\n",
    "- Process it through all phases\n",
    "- Return both the generated SQL and the results\n",
    "- Handle errors gracefully at each step\n",
    "- Provide progress feedback (optional verbose mode)\n",
    "\n",
    "This makes it simple to query the database: just call one function with a question!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJb-MYTzQ0x7"
   },
   "outputs": [],
   "source": [
    "def nlp_to_sql_pipeline(question, verbose=False):\n",
    "    \"\"\"\n",
    "    Complete NLP-to-SQL pipeline\n",
    "\n",
    "    Takes a natural language question and returns database results.\n",
    "    Handles all phases: generation, validation, execution.\n",
    "\n",
    "    Args:\n",
    "        question: Natural language question (string)\n",
    "        verbose: If True, show detailed progress. If False, only show minimal output (default: False)\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (sql_query, results) or (None, None) if any phase fails\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üöÄ STARTING NLP-TO-SQL PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    # Phase 1: Generate SQL query\n",
    "    if verbose:\n",
    "        print(f\"\\nüí¨ Question: {question}\")\n",
    "        print(\"ü§ñ Generating SQL query...\")\n",
    "\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            input=f\"{system_prompt}\\n\\nUser Question: {question}\"\n",
    "        )\n",
    "        sql_query = response.output_text.strip()\n",
    "        if verbose:\n",
    "            print(\"‚úÖ SQL query generated\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå Error generating SQL: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    if not sql_query:\n",
    "        if verbose:\n",
    "            print(\"‚ùå Pipeline failed: Could not generate SQL\")\n",
    "        return None, None\n",
    "\n",
    "    # Phase 2: Validate and clean SQL\n",
    "    if verbose:\n",
    "        print(\"\\nüõ°Ô∏è Processing and validating SQL...\")\n",
    "\n",
    "    validated_sql = process_generated_sql(sql_query) if verbose else process_generated_sql_quiet(sql_query)\n",
    "    if not validated_sql:\n",
    "        if verbose:\n",
    "            print(\"‚ùå Pipeline failed: Query validation failed\")\n",
    "        return None, None\n",
    "\n",
    "    # Phase 3: Execute query\n",
    "    if verbose:\n",
    "        print(\"\\n‚öôÔ∏è Executing query...\")\n",
    "\n",
    "    try:\n",
    "        with temp_db.connect() as conn:\n",
    "            result = conn.execute(text(validated_sql))\n",
    "            rows = result.fetchall()\n",
    "            if verbose:\n",
    "                print(f\"‚úÖ Query executed successfully\")\n",
    "                print(f\"üìä Returned {len(rows)} row(s)\")\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"‚ùå Error executing query: {e}\")\n",
    "        return validated_sql, None\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    return validated_sql, rows\n",
    "\n",
    "# Helper function for quiet validation\n",
    "def process_generated_sql_quiet(raw_sql):\n",
    "    \"\"\"Process and validate SQL without printing messages\"\"\"\n",
    "    query = extract_sql_query(raw_sql)\n",
    "    if not query:\n",
    "        return None\n",
    "    if validate_sql_query_quiet(query):\n",
    "        return query\n",
    "    return None\n",
    "\n",
    "def validate_sql_query_quiet(query):\n",
    "    \"\"\"Validate SQL query without printing messages\"\"\"\n",
    "    if not query:\n",
    "        return False\n",
    "    query_upper = query.upper()\n",
    "    if not query_upper.strip().startswith('SELECT'):\n",
    "        return False\n",
    "    forbidden_keywords = ['DELETE', 'DROP', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE', 'EXEC', 'EXECUTE']\n",
    "    for keyword in forbidden_keywords:\n",
    "        pattern = r'\\b' + keyword + r'\\b'\n",
    "        if re.search(pattern, query_upper):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(\"‚úÖ Complete pipeline function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHwLjdjkQ0x7"
   },
   "source": [
    "### Build Results Display Function\n",
    "\n",
    "This function takes the raw query results (which are just tuples of data) and formats them in a clean, manager-friendly way that's easy to read and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Kq76KpnQ0x7"
   },
   "outputs": [],
   "source": [
    "def display_results(question, query, results):\n",
    "    \"\"\"\n",
    "    Display query results in a clean, manager-friendly format\n",
    "\n",
    "    Args:\n",
    "        question: Original natural language question\n",
    "        query: Generated SQL query (optional, for reference)\n",
    "        results: Query results (list of tuples)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚ùì {question}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Format results based on type\n",
    "    if len(results) == 0:\n",
    "        print(\"üìä No results found.\")\n",
    "    elif len(results) == 1 and len(results[0]) == 1:\n",
    "        # Single value result (like COUNT)\n",
    "        print(f\"üìä Answer: {results[0][0]}\")\n",
    "    else:\n",
    "        # Multiple rows/columns - display as numbered list\n",
    "        print(f\"üìä Found {len(results)} result(s):\\n\")\n",
    "        for i, row in enumerate(results, 1):\n",
    "            # Format each row more readably\n",
    "            if len(row) == 1:\n",
    "                print(f\"   {i}. {row[0]}\")\n",
    "            else:\n",
    "                # For multiple columns, format as key-value style\n",
    "                formatted_row = \" | \".join(str(val) for val in row)\n",
    "                print(f\"   {i}. {formatted_row}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"‚úÖ Display function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKXTKGePQ0x7"
   },
   "source": [
    "---\n",
    "\n",
    "## üíº PRACTICAL EXAMPLES SECTION\n",
    "\n",
    "Now let's test our complete NLP-to-SQL pipeline with various real-world questions that IT managers might ask. These examples demonstrate different SQL operations: counting, filtering, aggregation, joins, and time-based queries.\n",
    "\n",
    "**Note:** By default, the pipeline runs in \"clean mode\" showing only the question and answer. If you want to see technical details (SQL queries, validation steps, etc.), add `verbose=True` to the pipeline call:\n",
    "```python\n",
    "query, results = nlp_to_sql_pipeline(question, verbose=True)\n",
    "```\n",
    "\n",
    "### Example 1: Simple Count Query\n",
    "\n",
    "**Scenario:** Manager wants to know current workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf3tuuDVQ0x8"
   },
   "outputs": [],
   "source": [
    "# Question 1: Simple count (clean output)\n",
    "question = \"How many tickets are currently open?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)\n",
    "\n",
    "# If you want to see technical details, uncomment the line below:\n",
    "# query, results = nlp_to_sql_pipeline(question, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9euMkaEQ0x8"
   },
   "source": [
    "### Example 2: Filtering with Conditions\n",
    "\n",
    "**Scenario:** Manager needs to know about urgent issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfjuZ6rIQ0x8"
   },
   "outputs": [],
   "source": [
    "# Question 2: Filtering by priority\n",
    "question = \"Show me all critical tickets that are not yet resolved\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMYdR5KpQ0x8"
   },
   "source": [
    "### Example 3: Technician Performance\n",
    "\n",
    "**Scenario:** Manager evaluating team productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apNHb55bQ0x8"
   },
   "outputs": [],
   "source": [
    "# Question 3: Technician workload\n",
    "question = \"How many tickets has each technician resolved?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2XzVnsCQ0x8"
   },
   "source": [
    "### Example 4: Time-Based Analysis\n",
    "\n",
    "**Scenario:** Manager tracking recent activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QrrNd2FQ0x8"
   },
   "outputs": [],
   "source": [
    "# Question 4: Recent tickets\n",
    "question = \"Show me tickets created in the last 7 days\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V71Glz1xQ0x8"
   },
   "source": [
    "### Example 5: Category Analysis\n",
    "\n",
    "**Scenario:** Manager identifying common problem types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pzcT7ZyQ0x8"
   },
   "outputs": [],
   "source": [
    "# Question 5: Most common categories\n",
    "question = \"What are the top 3 most common ticket categories?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP8aLY0AQ0x8"
   },
   "source": [
    "### Example 6: JOIN Query\n",
    "\n",
    "**Scenario:** Manager wants to see technician specializations with tickets\n",
    "\n",
    "**Note:** This demonstrates the LLM's ability to generate JOIN queries when data from multiple tables is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPQsbf29Q0x8"
   },
   "outputs": [],
   "source": [
    "# Question 6: JOIN query across tables\n",
    "question = \"Show me open tickets along with the specialization of the assigned technician\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N-6Qm2aQ0x8"
   },
   "source": [
    "---\n",
    "\n",
    "## üß™ TESTING & EDGE CASES SECTION\n",
    "\n",
    "\n",
    "\n",
    "Even with excellent prompts and validation, Large Language Models can make mistakes. It's important to understand the system's boundaries and limitations:\n",
    "\n",
    "\n",
    "1. **LLMs are probabilistic** ‚Üí They don't always generate perfect queries\n",
    "2. **Ambiguous questions** ‚Üí Some questions need clarification from users\n",
    "3. **Edge cases** ‚Üí Unusual questions might produce unexpected SQL\n",
    "4. **Business logic complexity** ‚Üí Complex requirements may not translate well\n",
    "\n",
    "\n",
    "\n",
    "### Run Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiAGq36uQ0x8"
   },
   "outputs": [],
   "source": [
    "# Define test questions covering different scenarios\n",
    "test_questions = [\n",
    "    # Simple queries\n",
    "    \"How many tickets are open?\",\n",
    "    \"Count all critical tickets\",\n",
    "\n",
    "    # Aggregations\n",
    "    \"What's the most common ticket category?\",\n",
    "    \"Average tickets per technician\",\n",
    "\n",
    "    # Filtering\n",
    "    \"Show me hardware issues\",\n",
    "    \"Which tickets are assigned to Sarah Johnson?\",\n",
    "\n",
    "    # Complex\n",
    "    \"Which technician has resolved the most critical tickets?\",\n",
    "    \"Show me tickets that took more than 5 days to resolve\",\n",
    "\n",
    "    # Edge cases\n",
    "    \"Find tickets with no assigned technician\",\n",
    "    \"What tickets are from TechCorp?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ RUNNING TEST SUITE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_queries = 0\n",
    "failed_queries = 0\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n\\n--- TEST {i}/{len(test_questions)} ---\")\n",
    "    query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "    if query and results is not None:\n",
    "        successful_queries += 1\n",
    "        print(f\"‚úÖ SUCCESS: {question}\")\n",
    "        print(f\"   SQL: {query}\")\n",
    "        print(f\"   Results: {len(results)} rows\")\n",
    "    else:\n",
    "        failed_queries += 1\n",
    "        print(f\"‚ùå FAILED: {question}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä TEST SUITE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Successful: {successful_queries}/{len(test_questions)}\")\n",
    "print(f\"‚ùå Failed: {failed_queries}/{len(test_questions)}\")\n",
    "print(f\"üìà Success Rate: {(successful_queries/len(test_questions))*100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edlhjJuLQ0x8"
   },
   "source": [
    "---\n",
    "\n",
    "## üéì BEST PRACTICES & SECURITY CONSIDERATIONS\n",
    "\n",
    "### For Production Implementation:\n",
    "\n",
    "**‚úÖ MUST DO:**\n",
    "\n",
    "1. **Always validate generated SQL before execution** ‚Üí Never trust LLM output blindly\n",
    "2. **Use read-only database connections** ‚Üí Prevent accidental data modification\n",
    "3. **Log all queries for auditing** ‚Üí Track who asked what and when\n",
    "4. **Set appropriate token limits** ‚Üí Control API costs (SQL queries are typically short)\n",
    "5. **Use temperature=0 for consistency** ‚Üí Deterministic outputs for the same question\n",
    "6. **Provide comprehensive schema context** ‚Üí Include sample data and relationships\n",
    "7. **Implement rate limiting** ‚Üí Prevent API abuse and cost overruns\n",
    "8. **Add query timeout limits** ‚Üí Prevent long-running expensive queries\n",
    "9. **Test extensively** ‚Üí Validate behavior across many question types\n",
    "10. **Monitor API costs** ‚Üí Track tokens per query and set budget alerts\n",
    "\n",
    "**‚ö†Ô∏è WARNINGS:**\n",
    "\n",
    "1. **Never expose this system directly to end users without human review** ‚Üí At least initially, have technical staff verify queries\n",
    "2. **Don't use for sensitive production databases** ‚Üí Start with non-critical data\n",
    "3. **Be aware of hallucinations** ‚Üí LLM might generate plausible but incorrect SQL\n",
    "4. **Don't assume 100% accuracy** ‚Üí Always have a fallback to manual querying\n",
    "5. **Watch for prompt injection** ‚Üí Malicious users might try to manipulate the system\n",
    "\n",
    "\n",
    "\n",
    "**Remember:** This is a learning prototype. Production deployment requires additional security, monitoring, and testing, but you now have the foundational knowledge to build sophisticated NLP-to-SQL systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMvx79I2UKO2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
