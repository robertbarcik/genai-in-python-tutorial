{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language to SQL with OpenAI API\n",
    "## Enabling IT Managers to Query Support Data Without SQL Knowledge\n",
    "\n",
    "---\n",
    "\n",
    "### üìö What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn how to build a practical NLP-to-SQL pipeline that converts natural language questions into executable SQL queries. By the end, you'll have a working system that allows non-technical users to query IT support data using plain English.\n",
    "\n",
    "**End Goal:** Build a system that converts questions like *\"How many critical tickets are open?\"* into SQL queries, executes them against a database, and returns results in an understandable format.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ The Business Problem\n",
    "\n",
    "In IT support organizations, data-driven decision making is critical for operational efficiency and service quality. Managers and stakeholders constantly need to answer important questions about their service desk performance:\n",
    "\n",
    "- \"What's our current ticket backlog?\"\n",
    "- \"Which technician is most productive this month?\"\n",
    "- \"Are we meeting our SLA targets?\"\n",
    "- \"How many critical issues are unresolved?\"\n",
    "- \"What categories of problems are most common?\"\n",
    "\n",
    "**The Challenge:** Most managers and stakeholders don't know SQL or how to query databases directly. This creates a significant bottleneck in organizations.\n",
    "\n",
    "**Current Reality:**\n",
    "\n",
    "Organizations typically handle this in three ways, all with drawbacks:\n",
    "\n",
    "1. **Wait for technical staff to run queries** ‚Üí Slow, creates bottlenecks, technical staff become overwhelmed with reporting requests\n",
    "2. **Use pre-built dashboards** ‚Üí Limited flexibility, can't answer ad-hoc questions, doesn't adapt to changing business needs\n",
    "3. **Export data to Excel manually** ‚Üí Time-consuming, error-prone, data quickly becomes stale, doesn't scale\n",
    "\n",
    "These approaches create delays in decision-making, increase workload on technical staff, and prevent agile responses to emerging issues.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° The Solution: NLP-to-SQL Pipeline\n",
    "\n",
    "Large Language Models (LLMs) like GPT-4 have a remarkable capability: they can understand natural language AND generate structured code, including SQL queries. We can leverage this to build an intelligent system that bridges the gap between business users and databases.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "Our system follows a multi-phase pipeline:\n",
    "\n",
    "1. **Schema Understanding** ‚Üí The LLM learns what tables and columns exist in our database\n",
    "2. **Question Analysis** ‚Üí A manager asks a question in plain English\n",
    "3. **SQL Generation** ‚Üí The LLM converts the question into a valid SQL query\n",
    "4. **Validation** ‚Üí We ensure the generated query is safe and correct\n",
    "5. **Execution** ‚Üí The query runs against our database\n",
    "6. **Results Presentation** ‚Üí Data is returned in an easy-to-understand format\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "- ‚úÖ **Democratizes data access** ‚Üí Anyone can query the database without SQL knowledge\n",
    "- ‚úÖ **Eliminates bottlenecks** ‚Üí No waiting for technical staff to run reports\n",
    "- ‚úÖ **Enables self-service analytics** ‚Üí Stakeholders get answers immediately\n",
    "- ‚úÖ **Supports ad-hoc queries** ‚Üí Not limited to pre-built dashboards\n",
    "- ‚úÖ **Accelerates decision-making** ‚Üí Real-time insights when they're needed\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Key Concepts Covered\n",
    "\n",
    "Throughout this notebook, we'll explore:\n",
    "\n",
    "- **Natural Language Processing (NLP)** ‚Üí How LLMs understand human questions\n",
    "- **SQL Query Generation** ‚Üí Teaching LLMs to write database queries\n",
    "- **Database Schema Understanding** ‚Üí Providing context about data structure\n",
    "- **Prompt Engineering** ‚Üí Crafting instructions that produce reliable outputs\n",
    "- **Query Validation** ‚Üí Security considerations and SQL injection prevention\n",
    "- **SQLite Database Interaction** ‚Üí Using Python to work with databases\n",
    "- **Error Handling** ‚Üí Building robust systems that handle edge cases\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Real-World Applications in IT Support\n",
    "\n",
    "This technology has immediate practical applications:\n",
    "\n",
    "- ‚úÖ **Manager self-service reporting** without requiring SQL knowledge or technical training\n",
    "- ‚úÖ **Ad-hoc queries during incident reviews** to quickly investigate patterns or anomalies\n",
    "- ‚úÖ **Quick status checks for stakeholder meetings** to provide up-to-date metrics on demand\n",
    "- ‚úÖ **Data exploration for process improvement** to identify bottlenecks and optimization opportunities\n",
    "- ‚úÖ **Training tool for learning SQL patterns** by seeing how natural language maps to queries\n",
    "- ‚úÖ **Automated reporting workflows** that can be triggered by natural language commands\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin building this system step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß SETUP SECTION\n",
    "\n",
    "### Step 1: Install Required Dependencies\n",
    "\n",
    "We need three main libraries:\n",
    "- **openai** ‚Üí To communicate with OpenAI's API\n",
    "- **pandas** ‚Üí For data manipulation and analysis\n",
    "- **sqlalchemy** ‚Üí For database connections and SQL execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Configure OpenAI API Key\n",
    "\n",
    "We'll try to load the API key from Google Colab secrets first (recommended for security), with a fallback to manual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"‚úÖ OpenAI client initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä DATA PREPARATION SECTION\n",
    "\n",
    "### Understanding Our IT Service Desk Data\n",
    "\n",
    "In IT support environments, we typically track two primary entities: **tickets** (service requests, incidents, and problems) and **technicians** (the staff who resolve these tickets). Understanding this data structure is critical for both humans and the LLM.\n",
    "\n",
    "The LLM needs to know:\n",
    "- What tables exist in our database\n",
    "- What columns each table contains\n",
    "- What type of data each column holds (text, numbers, dates, etc.)\n",
    "- What values are typical for each column\n",
    "\n",
    "This context enables the LLM to generate accurate SQL queries. Without it, the LLM would be guessing at table and column names, leading to errors.\n",
    "\n",
    "Let's create realistic sample data that mirrors a real IT service desk system.\n",
    "\n",
    "### Dataset 1: Support Tickets\n",
    "\n",
    "Our tickets table represents individual support requests from customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample IT support ticket data\n",
    "# Using dates relative to today for realistic time-based queries\n",
    "today = datetime.now()\n",
    "\n",
    "tickets_data = [\n",
    "    # Recent critical tickets (last week)\n",
    "    {'ticket_id': 'T001', 'title': 'Server outage - production environment down', 'category': 'Network Problem', 'priority': 'Critical', 'status': 'In Progress', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T002', 'title': 'Cannot access shared drive - entire department affected', 'category': 'Access Request', 'priority': 'Critical', 'status': 'Open', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Mike Chen', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T003', 'title': 'Email server not responding', 'category': 'Email Issue', 'priority': 'Critical', 'status': 'Resolved', 'created_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'assigned_to': 'Sarah Johnson', 'customer_company': 'DataSystems'},\n",
    "    \n",
    "    # High priority tickets\n",
    "    {'ticket_id': 'T004', 'title': 'VPN connection keeps dropping', 'category': 'Network Problem', 'priority': 'High', 'status': 'In Progress', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Alex Rodriguez', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T005', 'title': 'Laptop won\\'t boot - blue screen error', 'category': 'Hardware Issue', 'priority': 'High', 'status': 'Open', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'HealthPlus'},\n",
    "    {'ticket_id': 'T006', 'title': 'Database connection timeout errors', 'category': 'Software Installation', 'priority': 'High', 'status': 'Resolved', 'created_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'TechCorp'},\n",
    "    \n",
    "    # Medium priority tickets\n",
    "    {'ticket_id': 'T007', 'title': 'Need admin rights for new software installation', 'category': 'Access Request', 'priority': 'Medium', 'status': 'Open', 'created_date': (today - timedelta(days=3)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "    {'ticket_id': 'T008', 'title': 'Email not syncing to mobile device', 'category': 'Email Issue', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Mike Chen', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T009', 'title': 'Printer offline in conference room', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T010', 'title': 'Software license expired for Adobe Creative Suite', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'HealthPlus'},\n",
    "    \n",
    "    # Low priority tickets\n",
    "    {'ticket_id': 'T011', 'title': 'Request to change desktop wallpaper policy', 'category': 'Access Request', 'priority': 'Low', 'status': 'Open', 'created_date': (today - timedelta(days=10)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Alex Rodriguez', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T012', 'title': 'Keyboard spacebar sticking occasionally', 'category': 'Hardware Issue', 'priority': 'Low', 'status': 'Closed', 'created_date': (today - timedelta(days=15)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=12)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "    \n",
    "    # Password reset requests (common category)\n",
    "    {'ticket_id': 'T013', 'title': 'Password reset - forgot domain password', 'category': 'Password Reset', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=1)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T014', 'title': 'Account locked after too many failed login attempts', 'category': 'Password Reset', 'priority': 'High', 'status': 'Closed', 'created_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=2)).strftime('%Y-%m-%d'), 'assigned_to': 'Sarah Johnson', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T015', 'title': 'Need to reset multi-factor authentication', 'category': 'Password Reset', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'HealthPlus'},\n",
    "    \n",
    "    # Network issues\n",
    "    {'ticket_id': 'T016', 'title': 'Slow internet connection in office', 'category': 'Network Problem', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=5)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T017', 'title': 'Cannot connect to Wi-Fi network', 'category': 'Network Problem', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=9)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'DataSystems'},\n",
    "    \n",
    "    # Software installation requests\n",
    "    {'ticket_id': 'T018', 'title': 'Install Zoom for remote meetings', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Closed', 'created_date': (today - timedelta(days=11)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=10)).strftime('%Y-%m-%d'), 'assigned_to': 'Emma Davis', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T019', 'title': 'Microsoft Office needs to be updated', 'category': 'Software Installation', 'priority': 'Low', 'status': 'Closed', 'created_date': (today - timedelta(days=14)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=13)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T020', 'title': 'Install Python development environment', 'category': 'Software Installation', 'priority': 'Medium', 'status': 'Open', 'created_date': (today - timedelta(days=4)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Emma Davis', 'customer_company': 'TechCorp'},\n",
    "    \n",
    "    # Additional variety\n",
    "    {'ticket_id': 'T021', 'title': 'Laptop running very slow - needs optimization', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'In Progress', 'created_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'John Smith', 'customer_company': 'HealthPlus'},\n",
    "    {'ticket_id': 'T022', 'title': 'External monitor not detected', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Resolved', 'created_date': (today - timedelta(days=12)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=11)).strftime('%Y-%m-%d'), 'assigned_to': 'John Smith', 'customer_company': 'DataSystems'},\n",
    "    {'ticket_id': 'T023', 'title': 'Need access to finance folder on SharePoint', 'category': 'Access Request', 'priority': 'High', 'status': 'Closed', 'created_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=6)).strftime('%Y-%m-%d'), 'assigned_to': 'Alex Rodriguez', 'customer_company': 'SecureBank'},\n",
    "    {'ticket_id': 'T024', 'title': 'Email attachments not downloading', 'category': 'Email Issue', 'priority': 'High', 'status': 'Resolved', 'created_date': (today - timedelta(days=8)).strftime('%Y-%m-%d'), 'resolved_date': (today - timedelta(days=7)).strftime('%Y-%m-%d'), 'assigned_to': 'Mike Chen', 'customer_company': 'CloudNet'},\n",
    "    {'ticket_id': 'T025', 'title': 'Spam emails getting through filter', 'category': 'Email Issue', 'priority': 'Low', 'status': 'Open', 'created_date': (today - timedelta(days=13)).strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': 'Sarah Johnson', 'customer_company': 'HealthPlus'},\n",
    "    \n",
    "    # Recent unassigned tickets\n",
    "    {'ticket_id': 'T026', 'title': 'New hire needs laptop setup', 'category': 'Hardware Issue', 'priority': 'High', 'status': 'Open', 'created_date': today.strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': None, 'customer_company': 'TechCorp'},\n",
    "    {'ticket_id': 'T027', 'title': 'Conference room TV not working', 'category': 'Hardware Issue', 'priority': 'Medium', 'status': 'Open', 'created_date': today.strftime('%Y-%m-%d'), 'resolved_date': None, 'assigned_to': None, 'customer_company': 'DataSystems'},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "tickets_df = pd.DataFrame(tickets_data)\n",
    "\n",
    "print(\"üìã SUPPORT TICKETS DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total tickets: {len(tickets_df)}\")\n",
    "print(f\"Date range: {tickets_df['created_date'].min()} to {tickets_df['created_date'].max()}\")\n",
    "print(\"\\nFirst 10 tickets:\")\n",
    "display(tickets_df.head(10))\n",
    "\n",
    "print(\"\\nüìä Ticket Statistics:\")\n",
    "print(f\"  Status breakdown: {tickets_df['status'].value_counts().to_dict()}\")\n",
    "print(f\"  Priority breakdown: {tickets_df['priority'].value_counts().to_dict()}\")\n",
    "print(f\"  Category breakdown: {tickets_df['category'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2: Technicians\n",
    "\n",
    "Our technicians table represents the IT support staff who handle tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate technician data\n",
    "technicians_data = [\n",
    "    {'tech_id': 1, 'name': 'John Smith', 'specialization': 'Hardware', 'hire_date': '2020-03-15', 'availability': 'Full-time'},\n",
    "    {'tech_id': 2, 'name': 'Sarah Johnson', 'specialization': 'Network', 'hire_date': '2019-07-22', 'availability': 'Full-time'},\n",
    "    {'tech_id': 3, 'name': 'Mike Chen', 'specialization': 'Software', 'hire_date': '2021-01-10', 'availability': 'Full-time'},\n",
    "    {'tech_id': 4, 'name': 'Emma Davis', 'specialization': 'Software', 'hire_date': '2021-09-05', 'availability': 'Full-time'},\n",
    "    {'tech_id': 5, 'name': 'Alex Rodriguez', 'specialization': 'General', 'hire_date': '2022-05-18', 'availability': 'Part-time'},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "technicians_df = pd.DataFrame(technicians_data)\n",
    "\n",
    "print(\"üë• TECHNICIANS DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total technicians: {len(technicians_df)}\")\n",
    "display(technicians_df)\n",
    "\n",
    "print(\"\\nüìä Technician Statistics:\")\n",
    "print(f\"  Specializations: {technicians_df['specialization'].value_counts().to_dict()}\")\n",
    "print(f\"  Availability: {technicians_df['availability'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóÑÔ∏è DATABASE SETUP SECTION\n",
    "\n",
    "### Why SQLite for This Demo?\n",
    "\n",
    "In production environments, IT service desks typically use enterprise databases like PostgreSQL, MySQL, or Microsoft SQL Server. However, for learning and demonstration purposes, we'll use SQLite because:\n",
    "\n",
    "- **Lightweight** ‚Üí Runs entirely in memory or as a single file\n",
    "- **Zero configuration** ‚Üí No server setup required\n",
    "- **Perfect for prototyping** ‚Üí Fast iteration and testing\n",
    "- **SQL compatible** ‚Üí Queries work similarly across database systems\n",
    "\n",
    "The NLP-to-SQL pipeline we build here will work with any SQL database with minimal modifications.\n",
    "\n",
    "### Create SQLite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory SQLite database\n",
    "# Note: Using in-memory database for this demo (data lost when kernel restarts)\n",
    "# For persistence, use: create_engine('sqlite:///service_desk.db')\n",
    "temp_db = create_engine('sqlite:///:memory:')\n",
    "\n",
    "print(\"‚úÖ SQLite database created in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrames into SQLite database as tables\n",
    "tickets_df.to_sql('tickets', temp_db, index=False, if_exists='replace')\n",
    "technicians_df.to_sql('technicians', temp_db, index=False, if_exists='replace')\n",
    "\n",
    "print(\"‚úÖ Data loaded into database\")\n",
    "print(\"  - tickets table created with {} records\".format(len(tickets_df)))\n",
    "print(\"  - technicians table created with {} records\".format(len(technicians_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Database Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to verify tables exist\n",
    "with temp_db.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT name FROM sqlite_master WHERE type='table'\"))\n",
    "    tables = result.fetchall()\n",
    "    print(\"\\nüìä Tables in database:\")\n",
    "    for table in tables:\n",
    "        print(f\"  - {table[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Database with Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get first 5 tickets from database\n",
    "with temp_db.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT ticket_id, title, priority, status FROM tickets LIMIT 5\"))\n",
    "    rows = result.fetchall()\n",
    "    \n",
    "print(\"\\nüîç Sample Query: First 5 tickets\")\n",
    "print(\"=\" * 80)\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\n‚úÖ Database is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ UNDERSTANDING THE WORKFLOW\n",
    "\n",
    "### NLP-to-SQL Pipeline Overview\n",
    "\n",
    "Our system follows a multi-phase pipeline that converts natural language questions into database results. Understanding each phase is critical to building a robust system.\n",
    "\n",
    "#### **Phase 1: Schema Context Building** üèóÔ∏è\n",
    "- Generate a comprehensive description of our database structure\n",
    "- Tell the LLM what tables exist, what columns they have, and what sample values look like\n",
    "- This gives the LLM enough context to write accurate queries\n",
    "- Without this context, the LLM would hallucinate table and column names\n",
    "\n",
    "#### **Phase 2: Natural Language Input** üí¨\n",
    "- User asks a question in plain English\n",
    "- Example: \"How many critical tickets are still open?\"\n",
    "- The question can be as simple or complex as needed\n",
    "- No SQL knowledge required from the user\n",
    "\n",
    "#### **Phase 3: SQL Generation** ü§ñ\n",
    "- Send the schema context + user question to gpt-5-nano\n",
    "- The LLM analyzes the question and generates appropriate SQL\n",
    "- Example output: `SELECT COUNT(*) FROM tickets WHERE priority = 'Critical' AND status = 'Open';`\n",
    "- The LLM uses its understanding of both natural language and SQL syntax\n",
    "\n",
    "#### **Phase 4: Query Validation & Cleaning** üõ°Ô∏è\n",
    "- Extract just the SQL from the LLM response (remove explanatory text)\n",
    "- Validate it's a safe SELECT query (no DELETE, DROP, etc.)\n",
    "- Ensure no malicious code (SQL injection prevention)\n",
    "- This phase is critical for security and reliability\n",
    "\n",
    "#### **Phase 5: Execution** ‚öôÔ∏è\n",
    "- Execute the validated query against our SQLite database\n",
    "- Use parameterized queries and safe connection handling\n",
    "- Retrieve results as structured data\n",
    "- Handle any database errors gracefully\n",
    "\n",
    "#### **Phase 6: Results Presentation** üìä\n",
    "- Display results in a user-friendly format\n",
    "- Show both the generated query (for transparency) and the data returned\n",
    "- Format the output so non-technical users can understand it\n",
    "- Provide context about what the results mean\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Pipeline Approach?\n",
    "\n",
    "Breaking the system into distinct phases provides several benefits:\n",
    "\n",
    "1. **Modularity** ‚Üí Each phase can be tested and improved independently\n",
    "2. **Security** ‚Üí Validation phase prevents malicious queries\n",
    "3. **Debugging** ‚Üí Easy to identify which phase is failing\n",
    "4. **Transparency** ‚Üí Users can see the SQL being generated\n",
    "5. **Maintainability** ‚Üí Changes to one phase don't affect others\n",
    "\n",
    "Now let's implement each phase step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è PHASE 1: SCHEMA CONTEXT BUILDING\n",
    "\n",
    "### Theory: Teaching the LLM About Our Database\n",
    "\n",
    "Large Language Models don't have built-in knowledge of your specific database. They know SQL syntax and general database concepts, but they don't know what tables YOU have or what columns exist in YOUR database.\n",
    "\n",
    "We must explicitly tell the LLM:\n",
    "- **What tables we have** ‚Üí \"tickets\" and \"technicians\"\n",
    "- **What columns each table contains** ‚Üí column names and their purposes\n",
    "- **What type of data each column holds** ‚Üí text, integers, dates, etc.\n",
    "- **Example values** ‚Üí Show the LLM what actual data looks like\n",
    "\n",
    "This is called **schema context** or **table definitions**. The more detailed and accurate our description, the better the SQL generation will be.\n",
    "\n",
    "Think of it like giving directions to someone: you need to tell them what roads exist, not just assume they know the area.\n",
    "\n",
    "### Build Schema Context Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_context(table_name, df, num_samples=3):\n",
    "    \"\"\"\n",
    "    Generate schema context for a single table\n",
    "    \n",
    "    This creates a detailed description of the table structure that the LLM can understand.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the database table\n",
    "        df: Pandas DataFrame containing the table data\n",
    "        num_samples: How many sample rows to include\n",
    "        \n",
    "    Returns:\n",
    "        String containing formatted table description\n",
    "    \"\"\"\n",
    "    \n",
    "    context = f\"\\nTable: {table_name}\\n\"\n",
    "    context += \"Columns:\\n\"\n",
    "    \n",
    "    # Add column information with data types\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        context += f\"  - {col} ({dtype})\\n\"\n",
    "    \n",
    "    # Add sample values to show the LLM what data looks like\n",
    "    context += f\"\\nSample rows (first {num_samples}):\\n\"\n",
    "    sample_rows = df.head(num_samples).to_dict('records')\n",
    "    for i, row in enumerate(sample_rows, 1):\n",
    "        context += f\"  Row {i}: {row}\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "print(\"‚úÖ Schema context function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Schema Context for Both Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate context for tickets table\n",
    "tickets_context = generate_table_context('tickets', tickets_df, num_samples=3)\n",
    "\n",
    "# Generate context for technicians table\n",
    "technicians_context = generate_table_context('technicians', technicians_df, num_samples=3)\n",
    "\n",
    "# Combine into complete database schema context\n",
    "database_schema = tickets_context + \"\\n\" + technicians_context\n",
    "\n",
    "print(\"üìã DATABASE SCHEMA CONTEXT\")\n",
    "print(\"=\" * 80)\n",
    "print(database_schema)\n",
    "print(\"\\n‚úÖ Schema context generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Complete System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the system prompt that will be sent to the LLM\n",
    "# This prompt gives the LLM its role, context, and instructions\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert SQL query generator for an IT Service Desk database.\n",
    "\n",
    "Your task is to convert natural language questions into valid SQL queries.\n",
    "\n",
    "DATABASE SCHEMA:\n",
    "{database_schema}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Generate ONLY SELECT queries (no INSERT, UPDATE, DELETE, DROP)\n",
    "2. Use proper SQL syntax for SQLite\n",
    "3. Return ONLY the SQL query, nothing else\n",
    "4. End the query with a semicolon\n",
    "5. Use appropriate JOINs when data from multiple tables is needed\n",
    "6. Use COUNT, SUM, AVG, etc. for aggregation questions\n",
    "7. Use WHERE clauses to filter data based on conditions\n",
    "8. Use ORDER BY and LIMIT when appropriate\n",
    "\n",
    "IMPORTANT:\n",
    "- Return ONLY the SQL query\n",
    "- Do not include explanations or markdown formatting\n",
    "- Do not use code blocks (```) \n",
    "- The query should be executable as-is\n",
    "\n",
    "EXAMPLE:\n",
    "Question: \"How many open tickets are there?\"\n",
    "Response: SELECT COUNT(*) FROM tickets WHERE status = 'Open';\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ System prompt created\")\n",
    "print(\"\\nThis prompt gives the LLM everything it needs to generate accurate SQL queries for our IT service desk database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ PHASE 2 & 3: NATURAL LANGUAGE INPUT AND SQL GENERATION\n",
    "\n",
    "### Theory: Converting Questions to SQL\n",
    "\n",
    "Now that the LLM understands our database schema, it can generate SQL queries from natural language questions. The process works like this:\n",
    "\n",
    "1. We send the **system prompt** (which contains the schema) to establish context\n",
    "2. We send the **user's question** as a message\n",
    "3. The LLM analyzes the question and generates appropriate SQL\n",
    "4. The LLM returns the query as a response\n",
    "\n",
    "**Why gpt-5-nano?**\n",
    "\n",
    "We use gpt-5-nano for this task because it offers:\n",
    "- **Cost-effectiveness** ‚Üí Much cheaper than larger models like GPT-4\n",
    "- **Fast response times** ‚Üí Quick enough for interactive use\n",
    "- **Sufficient accuracy** ‚Üí Excellent at structured tasks like SQL generation\n",
    "- **Good instruction following** ‚Üí Reliably follows the format we specify\n",
    "\n",
    "For SQL generation, we don't need the most powerful model. A smaller, focused model works perfectly and saves money.\n",
    "\n",
    "### Build Query Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_query(question, system_prompt):\n",
    "    \"\"\"\n",
    "    Use OpenAI API to convert natural language to SQL\n",
    "    \n",
    "    Args:\n",
    "        question: Natural language question from user\n",
    "        system_prompt: System prompt with database schema\n",
    "        \n",
    "    Returns:\n",
    "        Generated SQL query as string\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüí¨ Question: {question}\")\n",
    "    print(\"ü§ñ Generating SQL query...\")\n",
    "    \n",
    "    try:\n",
    "        # Call OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            temperature=0,  # Use 0 for consistent, deterministic outputs\n",
    "            max_tokens=500  # SQL queries are typically short\n",
    "        )\n",
    "        \n",
    "        # Extract the generated SQL\n",
    "        sql_query = response.choices[0].message.content.strip()\n",
    "        \n",
    "        print(\"‚úÖ SQL query generated\")\n",
    "        return sql_query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating SQL: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Query generation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the SQL generation with a simple question\n",
    "test_question = \"How many critical tickets are currently open?\"\n",
    "\n",
    "generated_sql = generate_sql_query(test_question, system_prompt)\n",
    "\n",
    "if generated_sql:\n",
    "    print(\"\\nüìù Generated SQL:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(generated_sql)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ°Ô∏è PHASE 4: QUERY VALIDATION AND CLEANING\n",
    "\n",
    "### Theory: Security and Safety Considerations\n",
    "\n",
    "Large Language Models can generate code, but we should **never blindly execute it**. This is a critical security principle. Even though we've instructed the LLM to only generate SELECT queries, we must validate this programmatically.\n",
    "\n",
    "**Security Risks:**\n",
    "\n",
    "1. **SQL Injection** ‚Üí Malicious users might try to craft questions that generate harmful queries\n",
    "   - Example: A question designed to drop tables or access sensitive data\n",
    "   - We must validate that only safe operations are allowed\n",
    "\n",
    "2. **Incorrect Queries** ‚Üí The LLM might generate invalid or malformed SQL\n",
    "   - Syntax errors could crash the application\n",
    "   - We need to catch these before execution\n",
    "\n",
    "3. **Harmful Operations** ‚Üí We only want SELECT queries, not DELETE/UPDATE/DROP\n",
    "   - Even accidental data modification could be disastrous\n",
    "   - Strict validation prevents this\n",
    "\n",
    "**Our Solution: Two-Step Validation Process**\n",
    "\n",
    "1. **Extract the SQL query** ‚Üí Remove any explanatory text the LLM might have included\n",
    "2. **Validate it's safe** ‚Üí Ensure it's a SELECT query with no forbidden keywords\n",
    "\n",
    "This approach provides defense-in-depth: multiple layers of protection.\n",
    "\n",
    "### Build SQL Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql_query(text):\n",
    "    \"\"\"\n",
    "    Extracts SQL SELECT query from LLM response text\n",
    "    \n",
    "    Handles cases where LLM includes:\n",
    "    - Explanation before/after the query\n",
    "    - SQL wrapped in markdown code blocks\n",
    "    - Multiple queries (takes the first)\n",
    "    \n",
    "    Args:\n",
    "        text: Raw response from LLM\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned SQL query string or None if no valid query found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove markdown code blocks if present\n",
    "    text = re.sub(r'```sql\\n?', '', text)\n",
    "    text = re.sub(r'```\\n?', '', text)\n",
    "    \n",
    "    # Pattern to match SELECT queries\n",
    "    # Matches: SELECT ... ; (case insensitive, multiline)\n",
    "    pattern = r'SELECT\\s+.*?;'\n",
    "    \n",
    "    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        query = match.group(0).strip()\n",
    "        return query\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid SELECT query found in response\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ SQL extraction function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SQL Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sql_query(query):\n",
    "    \"\"\"\n",
    "    Validates that SQL query is safe to execute\n",
    "    \n",
    "    Security checks:\n",
    "    - Must be a SELECT statement\n",
    "    - Cannot contain DELETE, DROP, UPDATE, INSERT, ALTER\n",
    "    - Must use only allowed tables (tickets, technicians)\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query string to validate\n",
    "        \n",
    "    Returns:\n",
    "        Boolean: True if safe, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    if not query:\n",
    "        return False\n",
    "    \n",
    "    # Convert to uppercase for checking\n",
    "    query_upper = query.upper()\n",
    "    \n",
    "    # Must start with SELECT\n",
    "    if not query_upper.strip().startswith('SELECT'):\n",
    "        print(\"‚ùå Query must start with SELECT\")\n",
    "        return False\n",
    "    \n",
    "    # Forbidden operations\n",
    "    forbidden_keywords = ['DELETE', 'DROP', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE', 'EXEC', 'EXECUTE']\n",
    "    for keyword in forbidden_keywords:\n",
    "        if keyword in query_upper:\n",
    "            print(f\"‚ùå Forbidden keyword detected: {keyword}\")\n",
    "            return False\n",
    "    \n",
    "    # Allowed tables only (basic check)\n",
    "    allowed_tables = ['tickets', 'technicians']\n",
    "    \n",
    "    print(\"‚úÖ Query passed validation\")\n",
    "    return True\n",
    "\n",
    "print(\"‚úÖ SQL validation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Combined Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_generated_sql(raw_sql):\n",
    "    \"\"\"\n",
    "    Process raw LLM output: extract and validate SQL query\n",
    "    \n",
    "    Args:\n",
    "        raw_sql: Raw text from LLM\n",
    "        \n",
    "    Returns:\n",
    "        Validated SQL query or None if invalid\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüõ°Ô∏è Processing and validating SQL...\")\n",
    "    \n",
    "    # Step 1: Extract SQL\n",
    "    query = extract_sql_query(raw_sql)\n",
    "    \n",
    "    if not query:\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìù Extracted query: {query[:100]}{'...' if len(query) > 100 else ''}\")\n",
    "    \n",
    "    # Step 2: Validate\n",
    "    if validate_sql_query(query):\n",
    "        return query\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Combined processing function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with our previously generated SQL\n",
    "if generated_sql:\n",
    "    validated_query = process_generated_sql(generated_sql)\n",
    "    \n",
    "    if validated_query:\n",
    "        print(\"\\n‚úÖ VALIDATION SUCCESSFUL\")\n",
    "        print(\"Query is safe to execute:\")\n",
    "        print(validated_query)\n",
    "    else:\n",
    "        print(\"\\n‚ùå VALIDATION FAILED\")\n",
    "        print(\"Query was rejected by security checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è PHASE 5: QUERY EXECUTION\n",
    "\n",
    "### Theory: Running SQL Against the Database\n",
    "\n",
    "Now we have a validated, safe SQL query that we're confident is secure to execute. The next phase is to actually run it against our SQLite database and retrieve the results.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "1. **Safe Connection Handling** ‚Üí We use SQLAlchemy's connection context managers (`with` statement) to ensure connections are properly closed, even if errors occur\n",
    "\n",
    "2. **Results Format** ‚Üí SQLite returns results as tuples (one tuple per row), which we'll need to format for display\n",
    "\n",
    "3. **Error Handling** ‚Üí Database errors should be caught and reported gracefully, not crash the application\n",
    "\n",
    "4. **Read-Only Operations** ‚Üí Since we only allow SELECT queries, we're operating in a read-only mode, which is safer\n",
    "\n",
    "### Build Query Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_query(query, database_engine):\n",
    "    \"\"\"\n",
    "    Execute SQL query against the database\n",
    "    \n",
    "    Args:\n",
    "        query: Validated SQL query string\n",
    "        database_engine: SQLAlchemy engine connected to database\n",
    "        \n",
    "    Returns:\n",
    "        List of result rows (tuples) or None if error\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è Executing query...\")\n",
    "    \n",
    "    try:\n",
    "        # Use context manager for safe connection handling\n",
    "        with database_engine.connect() as conn:\n",
    "            # Execute the query\n",
    "            result = conn.execute(text(query))\n",
    "            \n",
    "            # Fetch all rows\n",
    "            rows = result.fetchall()\n",
    "            \n",
    "            print(f\"‚úÖ Query executed successfully\")\n",
    "            print(f\"üìä Returned {len(rows)} row(s)\")\n",
    "            \n",
    "            return rows\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Query execution function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Our First Complete Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the validated query we generated earlier\n",
    "if validated_query:\n",
    "    results = execute_sql_query(validated_query, temp_db)\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\nüìä QUERY RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Question: {test_question}\")\n",
    "        print(f\"SQL Query: {validated_query}\")\n",
    "        print(f\"\\nResults:\")\n",
    "        for row in results:\n",
    "            print(f\"  {row}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n‚úÖ First complete NLP-to-SQL pipeline execution successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ PHASE 6: COMPLETE PIPELINE FUNCTION\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "We've successfully built and tested all the individual pieces of our NLP-to-SQL pipeline:\n",
    "\n",
    "1. ‚úÖ Schema context generation\n",
    "2. ‚úÖ SQL query generation with LLM\n",
    "3. ‚úÖ Query validation and security checks\n",
    "4. ‚úÖ Query execution against database\n",
    "\n",
    "Now we'll combine these components into a single, easy-to-use function that handles the entire pipeline automatically. This function will:\n",
    "\n",
    "- Take a natural language question as input\n",
    "- Process it through all phases\n",
    "- Return both the generated SQL and the results\n",
    "- Handle errors gracefully at each step\n",
    "- Provide progress feedback to the user\n",
    "\n",
    "This makes it simple to query the database: just call one function with a question!\n",
    "\n",
    "### Build Complete Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_to_sql_pipeline(question):\n",
    "    \"\"\"\n",
    "    Complete NLP-to-SQL pipeline\n",
    "    \n",
    "    Takes a natural language question and returns database results.\n",
    "    Handles all phases: generation, validation, execution.\n",
    "    \n",
    "    Args:\n",
    "        question: Natural language question (string)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: (sql_query, results) or (None, None) if any phase fails\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ STARTING NLP-TO-SQL PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Phase 1: Generate SQL query\n",
    "    raw_sql = generate_sql_query(question, system_prompt)\n",
    "    if not raw_sql:\n",
    "        print(\"‚ùå Pipeline failed: Could not generate SQL\")\n",
    "        return None, None\n",
    "    \n",
    "    # Phase 2: Validate and clean SQL\n",
    "    validated_sql = process_generated_sql(raw_sql)\n",
    "    if not validated_sql:\n",
    "        print(\"‚ùå Pipeline failed: Query validation failed\")\n",
    "        return None, None\n",
    "    \n",
    "    # Phase 3: Execute query\n",
    "    results = execute_sql_query(validated_sql, temp_db)\n",
    "    if results is None:\n",
    "        print(\"‚ùå Pipeline failed: Query execution failed\")\n",
    "        return validated_sql, None\n",
    "    \n",
    "    print(\"\\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return validated_sql, results\n",
    "\n",
    "print(\"‚úÖ Complete pipeline function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Results Display Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(question, query, results):\n",
    "    \"\"\"\n",
    "    Display query results in a user-friendly format\n",
    "    \n",
    "    Args:\n",
    "        question: Original natural language question\n",
    "        query: Generated SQL query\n",
    "        results: Query results (list of tuples)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(f\"\\nüîç Generated SQL:\")\n",
    "    print(f\"   {query}\")\n",
    "    print(f\"\\nüìà Results ({len(results)} row(s)):\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, row in enumerate(results, 1):\n",
    "        print(f\"   {i}. {row}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"‚úÖ Display function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíº PRACTICAL EXAMPLES SECTION\n",
    "\n",
    "Now let's test our complete NLP-to-SQL pipeline with various real-world questions that IT managers might ask. These examples demonstrate different SQL operations: counting, filtering, aggregation, joins, and time-based queries.\n",
    "\n",
    "### Example 1: Simple Count Query\n",
    "\n",
    "**Scenario:** Manager wants to know current workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Simple count\n",
    "question = \"How many tickets are currently open?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Filtering with Conditions\n",
    "\n",
    "**Scenario:** Manager needs to know about urgent issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Filtering by priority\n",
    "question = \"Show me all critical tickets that are not yet resolved\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Technician Performance\n",
    "\n",
    "**Scenario:** Manager evaluating team productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: Technician workload\n",
    "question = \"How many tickets has each technician resolved?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Time-Based Analysis\n",
    "\n",
    "**Scenario:** Manager tracking recent activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: Recent tickets\n",
    "question = \"Show me tickets created in the last 7 days\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Category Analysis\n",
    "\n",
    "**Scenario:** Manager identifying common problem types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5: Most common categories\n",
    "question = \"What are the top 3 most common ticket categories?\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: JOIN Query\n",
    "\n",
    "**Scenario:** Manager wants to see technician specializations with tickets\n",
    "\n",
    "**Note:** This demonstrates the LLM's ability to generate JOIN queries when data from multiple tables is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6: JOIN query across tables\n",
    "question = \"Show me open tickets along with the specialization of the assigned technician\"\n",
    "query, results = nlp_to_sql_pipeline(question)\n",
    "\n",
    "if query and results:\n",
    "    display_results(question, query, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ TESTING & EDGE CASES SECTION\n",
    "\n",
    "### Theory: Understanding System Limitations\n",
    "\n",
    "Even with excellent prompts and validation, Large Language Models can make mistakes. It's important to understand the system's boundaries and limitations:\n",
    "\n",
    "**Why Testing Matters:**\n",
    "\n",
    "1. **LLMs are probabilistic** ‚Üí They don't always generate perfect queries\n",
    "2. **Ambiguous questions** ‚Üí Some questions need clarification from users\n",
    "3. **Edge cases** ‚Üí Unusual questions might produce unexpected SQL\n",
    "4. **Business logic complexity** ‚Üí Complex requirements may not translate well\n",
    "\n",
    "Testing helps us understand:\n",
    "- What types of questions work reliably\n",
    "- What types of questions fail or produce errors\n",
    "- Where we need to improve our prompts\n",
    "- What guidance users need when asking questions\n",
    "\n",
    "### Run Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions covering different scenarios\n",
    "test_questions = [\n",
    "    # Simple queries\n",
    "    \"How many tickets are open?\",\n",
    "    \"Count all critical tickets\",\n",
    "    \n",
    "    # Aggregations\n",
    "    \"What's the most common ticket category?\",\n",
    "    \"Average tickets per technician\",\n",
    "    \n",
    "    # Filtering\n",
    "    \"Show me hardware issues\",\n",
    "    \"Which tickets are assigned to Sarah Johnson?\",\n",
    "    \n",
    "    # Complex\n",
    "    \"Which technician has resolved the most critical tickets?\",\n",
    "    \"Show me tickets that took more than 5 days to resolve\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"Find tickets with no assigned technician\",\n",
    "    \"What tickets are from TechCorp?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ RUNNING TEST SUITE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_queries = 0\n",
    "failed_queries = 0\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n\\n--- TEST {i}/{len(test_questions)} ---\")\n",
    "    query, results = nlp_to_sql_pipeline(question)\n",
    "    \n",
    "    if query and results is not None:\n",
    "        successful_queries += 1\n",
    "        print(f\"‚úÖ SUCCESS: {question}\")\n",
    "        print(f\"   SQL: {query}\")\n",
    "        print(f\"   Results: {len(results)} rows\")\n",
    "    else:\n",
    "        failed_queries += 1\n",
    "        print(f\"‚ùå FAILED: {question}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä TEST SUITE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Successful: {successful_queries}/{len(test_questions)}\")\n",
    "print(f\"‚ùå Failed: {failed_queries}/{len(test_questions)}\")\n",
    "print(f\"üìà Success Rate: {(successful_queries/len(test_questions))*100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "Based on testing, here's what typically works well and what doesn't:\n",
    "\n",
    "**‚úÖ Questions That Work Well:**\n",
    "- Simple counts and aggregations (\"How many...\", \"Count all...\")\n",
    "- Filtering by specific values (\"Show me critical tickets\")\n",
    "- Time-based queries with clear timeframes (\"last 7 days\")\n",
    "- Straightforward joins (\"tickets with technician info\")\n",
    "- Sorting and limiting (\"top 5\", \"most common\")\n",
    "\n",
    "**‚ùå Questions That May Fail:**\n",
    "- Ambiguous terms (\"best technician\" - best by what metric?)\n",
    "- Vague timeframes (\"recently\" - how recent?)\n",
    "- Complex business logic (\"tickets that violated SLA\" - SLA rules not defined)\n",
    "- Questions requiring data not in database (\"customer satisfaction\" if not tracked)\n",
    "- Very long, compound questions with multiple parts\n",
    "\n",
    "**üîß How to Improve:**\n",
    "- **Better prompts** ‚Üí Add more examples and edge cases to system prompt\n",
    "- **User guidance** ‚Üí Teach users to ask specific, clear questions\n",
    "- **Iterative refinement** ‚Üí If a query fails, ask user to rephrase\n",
    "- **Expand schema context** ‚Üí Include business rules and definitions\n",
    "- **Add validation layers** ‚Üí Check that generated SQL makes sense for the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì BEST PRACTICES & SECURITY CONSIDERATIONS\n",
    "\n",
    "### For Production Implementation:\n",
    "\n",
    "**‚úÖ MUST DO:**\n",
    "\n",
    "1. **Always validate generated SQL before execution** ‚Üí Never trust LLM output blindly\n",
    "2. **Use read-only database connections** ‚Üí Prevent accidental data modification\n",
    "3. **Log all queries for auditing** ‚Üí Track who asked what and when\n",
    "4. **Set appropriate token limits** ‚Üí Control API costs (SQL queries are typically short)\n",
    "5. **Use temperature=0 for consistency** ‚Üí Deterministic outputs for the same question\n",
    "6. **Provide comprehensive schema context** ‚Üí Include sample data and relationships\n",
    "7. **Implement rate limiting** ‚Üí Prevent API abuse and cost overruns\n",
    "8. **Add query timeout limits** ‚Üí Prevent long-running expensive queries\n",
    "9. **Test extensively** ‚Üí Validate behavior across many question types\n",
    "10. **Monitor API costs** ‚Üí Track tokens per query and set budget alerts\n",
    "\n",
    "**‚ö†Ô∏è WARNINGS:**\n",
    "\n",
    "1. **Never expose this system directly to end users without human review** ‚Üí At least initially, have technical staff verify queries\n",
    "2. **Don't use for sensitive production databases** ‚Üí Start with non-critical data\n",
    "3. **Be aware of hallucinations** ‚Üí LLM might generate plausible but incorrect SQL\n",
    "4. **Don't assume 100% accuracy** ‚Üí Always have a fallback to manual querying\n",
    "5. **Watch for prompt injection** ‚Üí Malicious users might try to manipulate the system\n",
    "\n",
    "---\n",
    "\n",
    "### Security Checklist:\n",
    "\n",
    "**üõ°Ô∏è SQL Injection Prevention:**\n",
    "- ‚úÖ Validate all queries before execution\n",
    "- ‚úÖ Only allow SELECT statements\n",
    "- ‚úÖ Use parameterized queries when possible\n",
    "- ‚úÖ Implement query complexity limits\n",
    "- ‚úÖ Sanitize and escape special characters\n",
    "- ‚úÖ Block forbidden keywords (DELETE, DROP, UPDATE, etc.)\n",
    "\n",
    "**üîí Access Control:**\n",
    "- ‚úÖ Use read-only database credentials\n",
    "- ‚úÖ Implement user authentication and authorization\n",
    "- ‚úÖ Log all queries with user identifiers\n",
    "- ‚úÖ Set up query result row limits\n",
    "- ‚úÖ Restrict access to sensitive tables/columns\n",
    "\n",
    "**üí∞ Cost Management:**\n",
    "- ‚úÖ Set maximum token limits per query\n",
    "- ‚úÖ Implement daily/monthly API usage caps\n",
    "- ‚úÖ Cache common queries to reduce API calls\n",
    "- ‚úÖ Monitor and alert on unusual usage patterns\n",
    "- ‚úÖ Use cost-effective models (gpt-5-nano vs GPT-4)\n",
    "\n",
    "**üìä Operational Best Practices:**\n",
    "- ‚úÖ Log all questions, generated SQL, and results\n",
    "- ‚úÖ Monitor query success/failure rates\n",
    "- ‚úÖ Collect user feedback on result quality\n",
    "- ‚úÖ Regularly review logs for suspicious activity\n",
    "- ‚úÖ Update prompts based on failure patterns\n",
    "- ‚úÖ Version control your system prompts\n",
    "- ‚úÖ Test with diverse question types regularly\n",
    "\n",
    "---\n",
    "\n",
    "### Privacy Considerations:\n",
    "\n",
    "**Remember:** When using OpenAI's API, your questions and generated SQL are sent to OpenAI's servers. Consider:\n",
    "\n",
    "- Don't include sensitive customer data in questions\n",
    "- Review OpenAI's data usage policies\n",
    "- Consider enterprise agreements for sensitive use cases\n",
    "- Anonymize or pseudonymize data when possible\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps for Learning:\n",
    "\n",
    "**To further improve this system, explore:**\n",
    "\n",
    "1. **Query result formatting** ‚Üí Convert results to natural language summaries\n",
    "2. **Conversation memory** ‚Üí Allow follow-up questions (\"And how many are high priority?\")\n",
    "3. **Query explanation** ‚Üí Have LLM explain what the SQL does in plain English\n",
    "4. **Visualization integration** ‚Üí Automatically generate charts from query results\n",
    "5. **Multi-database support** ‚Üí Adapt to PostgreSQL, MySQL, SQL Server\n",
    "6. **Fine-tuning** ‚Üí Train custom models on your specific database patterns\n",
    "7. **RAG enhancement** ‚Üí Use vector databases to retrieve relevant schema context\n",
    "8. **Feedback loop** ‚Üí Let users rate query quality to improve over time\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a complete NLP-to-SQL pipeline that:\n",
    "\n",
    "‚úÖ Converts natural language to SQL using gpt-5-nano  \n",
    "‚úÖ Validates queries for security and correctness  \n",
    "‚úÖ Executes queries safely against a database  \n",
    "‚úÖ Returns results in an understandable format  \n",
    "‚úÖ Handles edge cases and errors gracefully  \n",
    "\n",
    "This system can democratize data access in IT organizations, enabling managers and stakeholders to get answers without SQL knowledge or technical bottlenecks.\n",
    "\n",
    "**Remember:** This is a learning prototype. Production deployment requires additional security, monitoring, and testing, but you now have the foundational knowledge to build sophisticated NLP-to-SQL systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
