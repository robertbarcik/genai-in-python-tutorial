{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Function Calling: Connecting LLMs to the Real World\n",
    "\n",
    "Welcome to Exercise #13! In this notebook, you'll learn how to extend Large Language Models beyond text generation by teaching them to interact with external functions and tools.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf What You'll Learn\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "\n",
    "1. **Understand function calling fundamentals** - What it is, why it matters, and when to use it\n",
    "2. **Define tool schemas** - Create structured descriptions that help LLMs understand available functions\n",
    "3. **Master the function calling loop** - Execute the complete request-response-execution-response cycle\n",
    "4. **Handle responses correctly** - Distinguish between text responses and function call requests\n",
    "5. **Build production-ready implementations** - Write reusable, error-handled function calling code\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcbc Business Value: Why Function Calling Matters\n",
    "\n",
    "Function calling bridges the gap between conversational AI and real-world actions. Instead of LLMs just generating text, they can now:\n",
    "\n",
    "- **Access live data** - Fetch current weather, stock prices, database records\n",
    "- **Perform actions** - Book appointments, send emails, update records\n",
    "- **Query systems** - Search knowledge bases, run database queries, call APIs\n",
    "- **Automate workflows** - Chain multiple operations together intelligently\n",
    "\n",
    "### \ud83c\udf1f Real-World Examples:\n",
    "\n",
    "- **Customer Service Chatbots** - Check order status, process refunds, schedule callbacks\n",
    "- **Personal Assistants** - Book meetings, set reminders, check calendars\n",
    "- **Data Analysis Tools** - Query databases, generate reports, visualize data\n",
    "- **Smart Home Control** - Adjust temperature, turn on lights, check security cameras\n",
    "- **E-commerce** - Search products, check inventory, process orders\n",
    "\n",
    "Without function calling, LLMs can only *suggest* what to do. With function calling, they can actually *do it*.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcda 1. Theory: What is Function Calling?\n",
    "\n",
    "## Core Concept\n",
    "\n",
    "**Function calling** is a capability that allows Large Language Models to intelligently request the execution of external functions. Instead of the LLM trying to generate an answer from its training data alone, it can recognize when it needs external information or actions, and request specific function calls with appropriate parameters.\n",
    "\n",
    "Think of it like this: Imagine you're a manager (the LLM) with a team of specialists (functions). When someone asks you a question, you can either answer from your own knowledge, or you can ask a specialist on your team to help. Function calling is how you ask your specialists for help.\n",
    "\n",
    "## Why It Matters\n",
    "\n",
    "LLMs are trained on static data with a knowledge cutoff. They don't have access to:\n",
    "- Real-time information (current weather, stock prices, etc.)\n",
    "- Private data (your database, user records, etc.)\n",
    "- System capabilities (sending emails, updating databases, etc.)\n",
    "\n",
    "Function calling solves this by letting LLMs:\n",
    "1. **Recognize** when external data or actions are needed\n",
    "2. **Request** specific functions with the right parameters\n",
    "3. **Incorporate** the results into their responses\n",
    "\n",
    "## When to Use Function Calling vs. Just Prompting\n",
    "\n",
    "**Use regular prompting when:**\n",
    "- The answer can be generated from the LLM's training data\n",
    "- You need creative or analytical responses\n",
    "- No real-time or private data is required\n",
    "\n",
    "**Use function calling when:**\n",
    "- You need real-time data (weather, prices, availability)\n",
    "- You need to query private systems (databases, APIs)\n",
    "- You need to perform actions (book, send, update, delete)\n",
    "- You want structured, reliable data access\n",
    "\n",
    "## The Request-Response Cycle\n",
    "\n",
    "Here's how function calling works at a high level:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 User Query  \u2502  \"What's the weather in Paris?\"\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   LLM Decides   \u2502  \"I need to call get_current_weather(location='Paris, France')\"\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Function Call   \u2502  Execute: get_current_weather(location='Paris, France')\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502    Execute      \u2502  Return: {\"temperature\": 18, \"condition\": \"Cloudy\"}\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Result Back    \u2502  Send function result back to LLM\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "       \u2502\n",
    "       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  LLM Response   \u2502  \"The weather in Paris is currently 18\u00b0C and cloudy.\"\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "## \ud83d\udca1 Key Point: Function Calling vs. Text Generation\n",
    "\n",
    "**Important distinction:**\n",
    "- **Without function calling:** The LLM would try to guess the weather based on general knowledge (\"Paris typically has...\")\n",
    "- **With function calling:** The LLM recognizes it needs current data, requests the function, and uses the actual result\n",
    "\n",
    "The LLM doesn't *execute* the function itself - it *requests* that you execute it. You're still in control!\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Key Takeaways:\n",
    "\n",
    "- Function calling lets LLMs request execution of external functions instead of just generating text\n",
    "- It bridges LLMs with real-time data, private systems, and actionable capabilities\n",
    "- The LLM decides WHEN to call a function and WHICH parameters to use\n",
    "- You (the developer) execute the actual function and return results to the LLM\n",
    "- This enables building chatbots that can actually DO things, not just talk about them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd27 2. Setup\n",
    "\n",
    "Before we dive into function calling, let's set up our environment. We'll need:\n",
    "1. OpenAI API access\n",
    "2. The OpenAI Python library\n",
    "3. A few supporting libraries\n",
    "\n",
    "## \ud83d\udce6 Install Dependencies\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI library\n",
    "!pip install -q openai\n",
    "\n",
    "# Suppress deprecation warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"\u2705 All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd11 API Key Configuration\n",
    "\n",
    "You have two methods to provide your OpenAI API key:\n",
    "\n",
    "**Method 1 (Recommended)**: Use Colab Secrets\n",
    "1. Click the \ud83d\udd11 icon in the left sidebar\n",
    "2. Click \"Add new secret\"\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your OpenAI API key\n",
    "5. Enable notebook access\n",
    "\n",
    "**Method 2 (Fallback)**: Manual input when prompted\n",
    "\n",
    "Run the cell below to configure authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"\u2705 API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"\ud83d\udca1 To use Colab secrets: Go to \ud83d\udd11 (left sidebar) \u2192 Add new secret \u2192 Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"\u274c ERROR: No API key provided!\")\n",
    "\n",
    "print(\"\u2705 Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Cost-efficient model for function calling\n",
    "print(f\"\ud83e\udd16 Selected Model: {OPENAI_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Initialize OpenAI Client\n",
    "\n",
    "Now let's import the necessary libraries and create our OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "# This client will be used to make all API calls\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"\u2705 OpenAI client initialized successfully!\")\n",
    "print(\"\\n\ud83c\udf93 Ready to learn about function calling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udccb 3. Understanding Tool Schemas\n",
    "\n",
    "## What is a Tool Schema?\n",
    "\n",
    "A **tool schema** is a JSON specification that describes a function to the LLM. Think of it as a menu that tells the LLM:\n",
    "- What functions are available\n",
    "- What each function does\n",
    "- What parameters each function accepts\n",
    "- What types those parameters should be\n",
    "- Which parameters are required\n",
    "\n",
    "## Why We Need Structured Schemas\n",
    "\n",
    "The LLM needs to understand:\n",
    "1. **When** to use each function (based on the description)\n",
    "2. **How** to use it (what parameters to provide)\n",
    "3. **What** values are acceptable (types, enums, etc.)\n",
    "\n",
    "Without a clear schema, the LLM wouldn't know which function to call or how to call it correctly.\n",
    "\n",
    "## \ud83d\udca1 Key Point: Schema vs. Implementation\n",
    "\n",
    "**Critical distinction:**\n",
    "- **Tool Schema (JSON):** A *description* of the function for the LLM to read\n",
    "- **Function Implementation (Python):** The *actual code* that executes when called\n",
    "\n",
    "The schema is like a restaurant menu - it describes the dish. The implementation is like the kitchen - it actually makes the dish.\n",
    "\n",
    "The LLM only sees the schema. You execute the implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf24\ufe0f Step 1: Define the Actual Functions (Implementation)\n",
    "\n",
    "Let's start by creating our actual Python functions. For this tutorial, we'll use mock weather functions that simulate API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION: These are the actual Python functions\n",
    "# The LLM will REQUEST these functions, and we will EXECUTE them\n",
    "\n",
    "def get_current_weather(location: str, format: str) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to simulate getting the current weather.\n",
    "    In a real application, this would call an actual weather API.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA.\n",
    "        format (str): The temperature format, either 'celsius' or 'fahrenheit'.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A mock response with random weather data.\n",
    "    \"\"\"\n",
    "    # Generate random weather data for demonstration\n",
    "    temperature = random.uniform(15.0, 35.0) if format == \"celsius\" else random.uniform(60.0, 95.0)\n",
    "    weather_conditions = random.choice([\"Sunny\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Windy\"])\n",
    "    \n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": round(temperature, 2),\n",
    "        \"unit\": format,\n",
    "        \"condition\": weather_conditions\n",
    "    }\n",
    "\n",
    "\n",
    "def get_n_day_weather_forecast(location: str, format: str, num_days: int) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to simulate getting an N-day weather forecast.\n",
    "    In a real application, this would call an actual weather API.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The city and state, e.g., San Francisco, CA.\n",
    "        format (str): The temperature format, either 'celsius' or 'fahrenheit'.\n",
    "        num_days (int): The number of days to forecast.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A mock response with random weather data for each day.\n",
    "    \"\"\"\n",
    "    forecast = []\n",
    "    for day in range(num_days):\n",
    "        temperature = random.uniform(15.0, 35.0) if format == \"celsius\" else random.uniform(60.0, 95.0)\n",
    "        weather_conditions = random.choice([\"Sunny\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Windy\"])\n",
    "        forecast.append({\n",
    "            \"day\": f\"Day {day + 1}\",\n",
    "            \"temperature\": round(temperature, 2),\n",
    "            \"unit\": format,\n",
    "            \"condition\": weather_conditions\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Weather functions defined!\")\n",
    "print(\"\\n\ud83d\udcdd Note: These are MOCK functions - they generate random data for learning purposes.\")\n",
    "print(\"   In production, these would call real weather APIs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly test our functions to see how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the current weather function\n",
    "test_result = get_current_weather(\"Paris, France\", \"celsius\")\n",
    "print(\"\ud83e\uddea Test: get_current_weather('Paris, France', 'celsius')\")\n",
    "print(json.dumps(test_result, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test the forecast function\n",
    "test_forecast = get_n_day_weather_forecast(\"Tokyo, Japan\", \"celsius\", 3)\n",
    "print(\"\ud83e\uddea Test: get_n_day_weather_forecast('Tokyo, Japan', 'celsius', 3)\")\n",
    "print(json.dumps(test_forecast, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdc Step 2: Define Tool Schemas (Descriptions for the LLM)\n",
    "\n",
    "Now that we have our actual functions, we need to describe them to the LLM using JSON schemas. The LLM will read these descriptions to understand when and how to call our functions.\n",
    "\n",
    "Let's create the tool schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SCHEMAS: These are descriptions that tell the LLM about our functions\n# The LLM reads these to decide when and how to call functions\n\ntools = [\n    {\n        # Type: Specifies this is a function tool\n        \"type\": \"function\",\n\n        # Name: Must match the actual Python function name EXACTLY\n        \"name\": \"get_current_weather\",\n\n        # Description: Tells the LLM WHEN to use this function\n        # Be clear and specific - this guides the LLM's decision-making\n        \"description\": \"Get the current weather in a given location. Use this when the user asks about current or present weather conditions.\",\n\n        # Parameters: JSON Schema describing the function inputs\n        \"parameters\": {\n            \"type\": \"object\",\n\n            # Properties: Each parameter and its details\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    # Description helps LLM know what format to use\n                    \"description\": \"The city and state, e.g., San Francisco, CA or Paris, France\"\n                },\n                \"format\": {\n                    \"type\": \"string\",\n                    # Enum restricts to specific valid values\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use. Use 'celsius' for metric and 'fahrenheit' for imperial.\"\n                }\n            },\n\n            # Required: List of mandatory parameters\n            # The LLM will try to gather these before calling the function\n            \"required\": [\"location\", \"format\"]\n        }\n    },\n    {\n        \"type\": \"function\",\n\n        # Second function: Weather forecast\n        \"name\": \"get_n_day_weather_forecast\",\n\n        # Clear description distinguishes this from get_current_weather\n        \"description\": \"Get an N-day weather forecast for a given location. Use this when the user asks about future weather or multi-day forecasts.\",\n\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g., San Francisco, CA or Paris, France\"\n                },\n                \"format\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"The temperature unit to use. Use 'celsius' for metric and 'fahrenheit' for imperial.\"\n                },\n                \"num_days\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The number of days to forecast, between 1 and 7 days\"\n                }\n            },\n            \"required\": [\"location\", \"format\", \"num_days\"]\n        }\n    }\n]\n\nprint(\"\u2705 Tool schemas defined!\")\nprint(f\"\\n\ud83d\udccb Number of tools available to LLM: {len(tools)}\")\nprint(\"\\n\ud83d\udd0d Tool names:\")\nfor tool in tools:\n    print(f\"  - {tool['name']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd0d Schema Deep Dive\n\nLet's break down what each part of the schema does:\n\n### Structure Breakdown\n\n```json\n{\n  \"type\": \"function\",  // Tells OpenAI this is a function tool\n  \"name\": \"get_current_weather\",  // Must match Python function name\n  \"description\": \"...\",  // Helps LLM decide WHEN to use this\n  \"parameters\": {  // JSON Schema format\n      \"type\": \"object\",\n      \"properties\": {  // Define each parameter\n        \"location\": {\n          \"type\": \"string\",  // Data type\n          \"description\": \"...\"  // Helps LLM know what to provide\n        },\n        \"format\": {\n          \"type\": \"string\",\n          \"enum\": [\"celsius\", \"fahrenheit\"]  // Restricts to valid values\n        }\n      },\n      \"required\": [\"location\", \"format\"]  // Mandatory parameters\n    }\n  }\n}\n```\n\n### Key Components Explained\n\n1. **`type: \"function\"`** - Tells the LLM this is a function it can call\n\n2. **`name`** - The function identifier. **Must match your Python function name exactly!**\n\n3. **`description`** - Critical for helping the LLM decide when to use this function. Be specific about:\n   - What the function does\n   - When it should be used\n   - How it differs from similar functions\n\n4. **`parameters`** - Uses JSON Schema format to define inputs:\n   - `type: \"object\"` - Parameters are provided as an object\n   - `properties` - Each parameter's definition\n   - Each property has a `type` and `description`\n\n5. **`required`** - Array of mandatory parameter names. The LLM will try to gather these before calling.\n\n6. **`enum`** - Restricts parameter values to a specific list. Helps prevent invalid inputs.\n\n### \u26a0\ufe0f Common Mistake: Mismatched Names\n\n**Wrong:**\n```python\n# Python function\ndef get_weather(location, format):\n    ...\n\n# Schema\n{\n    \"name\": \"getCurrentWeather\",  # \u274c Doesn't match!\n    ...\n}\n```\n\n**Correct:**\n```python\n# Python function\ndef get_weather(location, format):\n    ...\n\n# Schema\n{\n    \"name\": \"get_weather\",  # \u2705 Exact match!\n    ...\n}\n```\n\n---\n\n## \u2705 Key Takeaways: Tool Schemas\n\n- Tool schemas are JSON descriptions that tell the LLM about available functions\n- Schemas are NOT the functions themselves - they're instructions for the LLM\n- The `name` field must match your Python function name exactly\n- The `description` field is critical - it helps the LLM decide when to use the function\n- Parameter descriptions guide the LLM on what values to provide\n- Use `enum` to restrict parameters to specific valid values\n- The `required` array tells the LLM which parameters are mandatory"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd04 4. Understanding LLM Responses\n",
    "\n",
    "## Response Types\n",
    "\n",
    "When you make an API call with function calling enabled, the LLM can respond in two ways:\n",
    "\n",
    "1. **Regular text response** - The LLM provides a conversational answer (no function needed)\n",
    "2. **Function call request** - The LLM requests to execute one or more functions\n",
    "\n",
    "Understanding which type of response you received is crucial for handling function calling correctly.\n",
    "\n",
    "## Response Structure\n",
    "\n",
    "The response object contains:\n",
    "- **`content`** - Text content (if the LLM is providing a conversational response)\n",
    "- **`tool_calls`** - List of function calls (if the LLM wants to call functions)\n",
    "\n",
    "We need to check which one is present to know how to proceed.\n",
    "\n",
    "## \ud83d\udca1 Key Point: LLMs Can Ask for Clarification\n",
    "\n",
    "The LLM won't just blindly call functions. If it doesn't have enough information, it will ask the user for clarification. This is intelligent behavior!\n",
    "\n",
    "For example:\n",
    "- User: \"What's the weather?\"\n",
    "- LLM: \"I'd be happy to check the weather for you. Which city are you interested in?\"\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\uddea Example 1: LLM Asks for Clarification\n",
    "\n",
    "Let's see what happens when we ask a vague question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start a new conversation\n# In function calling, we maintain a conversation history with messages\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather like today?\"\n    }\n]\n\n# Make API call with function calling enabled\n# The 'tools' parameter tells the LLM what functions are available\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=messages,\n    tools=tools,\n    tool_choice=\"auto\"  # Let the LLM decide whether to call a function\n)\n\n# Extract the assistant's message\n# Get function calls from response\nfunction_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n\nprint(\"\ud83e\udd16 Assistant Response:\")\nprint(\"=\"*60)\n\n# Check if the response contains tool calls or regular content\nif function_calls:\n    print(\"\ud83d\udcde Response type: FUNCTION CALL\")\n    print(f\"Number of function calls: {len(function_calls)}\")\n    for tool_call in function_calls:\n        print(f\"  - Function: {tool_call.name}\")\nelse:\n    print(\"\ud83d\udcac Response type: TEXT CONTENT\")\n    print(f\"\\nContent: {response.output_text}\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd0d What Just Happened?\n",
    "\n",
    "The LLM received our question \"What's the weather like today?\" but noticed it's **missing required information**:\n",
    "- We didn't specify a location\n",
    "- We didn't specify celsius or fahrenheit\n",
    "\n",
    "Instead of making assumptions, the LLM asked for clarification. This is the **right behavior** - the system prompt told it not to guess!\n",
    "\n",
    "The response contains **`content`** (text), not **`tool_calls`** (function requests).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\uddea Example 2: LLM Calls a Function\n",
    "\n",
    "Now let's provide complete information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Continue the conversation by adding the user's clarification\n# We're building on the previous messages - this is ONE conversation\n# Note: For Responses API, we add function calls via tool messages  # Add the assistant's clarification question\nmessages.append({\n    \"role\": \"user\",\n    \"content\": \"I'm in Glasgow, Scotland and prefer Celsius.\"\n})\n\n# Make another API call with the updated conversation\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=messages,\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# Get function calls from response\nfunction_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n\nprint(\"\ud83e\udd16 Assistant Response:\")\nprint(\"=\"*60)\n\n# Check the response type\nif function_calls:\n    print(\"\ud83d\udcde Response type: FUNCTION CALL\")\n    print(f\"\\nThe LLM decided to call a function!\\n\")\n    print(f\"Number of function calls: {len(function_calls)}\")\n    \n    for i, tool_call in enumerate(function_calls, 1):\n        print(f\"\\nFunction Call #{i}:\")\n        print(f\"  - Function name: {tool_call.name}\")\n        print(f\"  - Arguments: {tool_call.arguments}\")\n        print(f\"  - Tool call ID: {tool_call.id}\")\nelse:\n    print(\"\ud83d\udcac Response type: TEXT CONTENT\")\n    print(f\"\\nContent: {response.output_text}\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Anatomy of a Tool Call Response\n",
    "\n",
    "Perfect! Now the LLM has all the information it needs, so it requested a function call. Let's break down the response structure:\n",
    "\n",
    "### Tool Call Components:\n",
    "\n",
    "1. **`tool_calls`** - A list containing function call requests (can be multiple)\n",
    "\n",
    "2. **`tool_calls[0].id`** - Unique identifier for this specific function call\n",
    "   - Used to track which function result corresponds to which request\n",
    "   - Critical for the response loop\n",
    "\n",
    "3. **`tool_calls[0].type`** - Should always be `\"function\"` for function calls\n",
    "\n",
    "4. **`tool_calls[0].function.name`** - The name of the function to call\n",
    "   - This tells us WHICH function to execute\n",
    "   - Will match one of the names in our tool schemas\n",
    "\n",
    "5. **`tool_calls[0].function.arguments`** - A JSON string containing the parameters\n",
    "   - \u26a0\ufe0f Important: This is a STRING, not a dict!\n",
    "   - We need to parse it with `json.loads()` before using it\n",
    "\n",
    "Let's extract these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract the tool call information\n# We're accessing the FIRST tool call (index 0)\ntool_call = function_calls[0]\n\nprint(\"\ud83d\udd0d Extracting Tool Call Information:\")\nprint(\"=\"*60)\n\n# 1. Tool call ID (for tracking)\ntool_call_id = tool_call.call_id\nprint(f\"\\n1\ufe0f\u20e3 Tool Call ID: {tool_call_id}\")\nprint(\"   (This unique ID tracks this specific function call)\")\n\n# 2. Function name (which function to call)\nfunction_name = tool_call.name\nprint(f\"\\n2\ufe0f\u20e3 Function Name: {function_name}\")\nprint(\"   (This tells us WHICH function to execute)\")\n\n# 3. Arguments (the parameters - comes as a JSON string)\nfunction_arguments_string = tool_call.arguments\nprint(f\"\\n3\ufe0f\u20e3 Arguments (as JSON string): {function_arguments_string}\")\nprint(f\"   Type: {type(function_arguments_string)}\")\n\n# 4. Parse the arguments from JSON string to dictionary\nfunction_arguments = json.loads(function_arguments_string)\nprint(f\"\\n4\ufe0f\u20e3 Arguments (parsed to dict): {function_arguments}\")\nprint(f\"   Type: {type(function_arguments)}\")\nprint(f\"\\n   Location: {function_arguments.get('location')}\")\nprint(f\"   Format: {function_arguments.get('format')}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\n\u2705 Now we know WHAT function to call and WITH WHAT parameters!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udca1 Key Point: Arguments Are JSON Strings\n",
    "\n",
    "**Important detail:**\n",
    "- `tool_call.function.arguments` is a **JSON string**, not a Python dict\n",
    "- You must parse it with `json.loads()` before using it\n",
    "- This is easy to forget and causes errors!\n",
    "\n",
    "```python\n",
    "# \u274c Wrong - arguments is a string\n",
    "location = tool_call.function.arguments['location']  # Error!\n",
    "\n",
    "# \u2705 Correct - parse first, then use\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "location = args['location']  # Works!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Key Takeaways: LLM Responses\n",
    "\n",
    "- LLM responses can contain either text content OR function calls (or both)\n",
    "- Always check if `tool_calls` exists before trying to access it\n",
    "- LLMs will ask for clarification if they don't have required information (smart behavior!)\n",
    "- Each tool call has: `id` (tracking), `function.name` (which function), `function.arguments` (parameters)\n",
    "- Function arguments come as JSON strings - you must parse them with `json.loads()`\n",
    "- The `tool_call_id` is critical - you'll need it when sending the function result back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# \ud83d\udd01 5. The Complete Function Calling Loop\n\n## Understanding the Full Cycle\n\nNow we're ready to see the complete function calling flow. This is where everything comes together!\n\n### The 5-Step Process:\n\n```\nSTEP 1: Initial API Call\n\u251c\u2500 Send user query to LLM with available tools\n\u2514\u2500 LLM responds with function call request\n\nSTEP 2: Extract Function Details\n\u251c\u2500 Get tool_call_id (for tracking)\n\u251c\u2500 Get function name (which function to call)\n\u2514\u2500 Parse arguments (what parameters to use)\n\nSTEP 3: Execute the Function\n\u251c\u2500 Match function name to actual Python function\n\u251c\u2500 Call the function with extracted arguments\n\u2514\u2500 Get the function result\n\nSTEP 4: Send Result Back to LLM\n\u251c\u2500 Create a \"user\" message with the function result\n\u251c\u2500 Format the result clearly for the LLM\n\u2514\u2500 Append to conversation history\n\nSTEP 5: Get Final Response\n\u251c\u2500 Make another API call with updated conversation\n\u251c\u2500 LLM uses function result to create natural language response\n\u2514\u2500 Return final answer to user\n```\n\n## \ud83d\udca1 Key Point: This is ONE Conversation\n\n**Critical concept:**\n- We're NOT starting a new conversation for each step\n- We're CONTINUING the same conversation by appending messages\n- The messages list grows: User \u2192 Assistant (tool call) \u2192 Tool (result) \u2192 Assistant (final answer)\n\nThink of it like a real conversation:\n1. User: \"What's the weather?\"\n2. Assistant: *checks weather* (function call)\n3. System: *weather data* (function result)\n4. Assistant: \"It's 22\u00b0C and sunny!\" (final response)\n\n\n\n## \ud83d\udca1 Key Point: Responses API Difference\n\n**Important for gpt-5-nano (Responses API):**\n- The Responses API does NOT support the `'tool'` role\n- Supported roles: `'assistant'`, `'system'`, `'developer'`, and `'user'`\n- Function results must be sent as **'user'** messages\n- Format: `{\"role\": \"user\", \"content\": f\"Function {name} returned: {result}\"}`\n\nThis is different from the Chat Completions API which uses a dedicated `'tool'` role.\n## \u26a0\ufe0f Common Mistake: Creating New Conversations\n\n**Wrong approach:**\n```python\n# \u274c Starting fresh conversations\nresponse1 = client.responses.create(input=[{\"role\": \"user\", \"content\": \"Weather?\"}])\n# ... execute function ...\nresponse2 = client.responses.create(input=[{\"role\": \"user\", \"content\": result}])  # Wrong!\n```\n\n**Correct approach:**\n```python\n# \u2705 Continuing the same conversation\nmessages = [{\"role\": \"user\", \"content\": \"Weather?\"}]\nresponse1 = client.responses.create(input=messages)\nmessages.append(response1.output)  # Add assistant's function call\nmessages.append(tool_result)  # Add function result\nresponse2 = client.responses.create(input=messages)  # Continue conversation\n```\n\n---\n\n## \ud83c\udfaf Let's Execute the Complete Loop!\n\nWe'll go through each step clearly with detailed explanations."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd37 STEP 1: Initial API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\ud83d\udd37 STEP 1: Making initial API call with function calling enabled\")\nprint(\"=\"*60)\n\n# Start fresh conversation with a clear, complete query\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate weather data.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What's the weather in Bratislava right now? Use Celsius.\"\n    }\n]\n\nprint(\"\ud83d\udce4 Sending request to LLM...\")\nprint(f\"   User query: '{messages[1]['content']}'\")\nprint(f\"   Available tools: {len(tools)}\")\n\n# Make the API call with tools enabled\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=messages,\n    tools=tools,\n    tool_choice=\"auto\"  # Let LLM decide\n)\n\n# Store the assistant's message (which will contain the function call)\n# Get function calls from response\nfunction_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n\nprint(\"\\n\ud83d\udce5 Response received!\")\nprint(f\"   Response type: {'FUNCTION CALL' if function_calls else 'TEXT'}\")\n\nif function_calls:\n    print(f\"   Function to call: {function_calls[0].name}\")\n\nprint(\"\\n\u2705 Step 1 complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd37 STEP 2: Extract Tool Call Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\ud83d\udd37 STEP 2: Extracting function call details\")\nprint(\"=\"*60)\n\n# Access the first (and in this case, only) tool call\ntool_call = function_calls[0]\n\n# Extract the three key pieces of information\ntool_call_id = tool_call.call_id\nfunction_name = tool_call.name\nfunction_arguments = json.loads(tool_call.arguments)\n\nprint(\"\ud83d\udccb Extracted information:\")\nprint(f\"   Tool Call ID: {tool_call_id}\")\nprint(f\"   Function Name: {function_name}\")\nprint(f\"   Function Arguments: {json.dumps(function_arguments, indent=6)}\")\n\nprint(\"\\n\u2705 Step 2 complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd37 STEP 3: Execute the Actual Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd37 STEP 3: Executing the actual Python function\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Match the function name to the actual Python function and execute it\n",
    "# We use if/elif to route to the correct function\n",
    "if function_name == \"get_current_weather\":\n",
    "    print(f\"\ud83c\udf24\ufe0f  Calling: get_current_weather()\")\n",
    "    print(f\"   Parameters: location='{function_arguments['location']}', format='{function_arguments['format']}'\")\n",
    "    \n",
    "    # Execute the function with the extracted arguments\n",
    "    # The ** operator unpacks the dictionary as keyword arguments\n",
    "    function_result = get_current_weather(**function_arguments)\n",
    "    \n",
    "elif function_name == \"get_n_day_weather_forecast\":\n",
    "    print(f\"\ud83d\udcc5 Calling: get_n_day_weather_forecast()\")\n",
    "    print(f\"   Parameters: {function_arguments}\")\n",
    "    function_result = get_n_day_weather_forecast(**function_arguments)\n",
    "else:\n",
    "    # This shouldn't happen if our schemas are correct, but good to handle\n",
    "    print(f\"\u274c Unknown function: {function_name}\")\n",
    "    function_result = {\"error\": \"Unknown function\"}\n",
    "\n",
    "print(\"\\n\ud83d\udcca Function result:\")\n",
    "print(json.dumps(function_result, indent=3))\n",
    "\n",
    "print(\"\\n\u2705 Step 3 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd37 STEP 4: Add Function Result to Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\ud83d\udd37 STEP 4: Adding function result to conversation history\")\nprint(\"=\"*60)\n\n# First, append the assistant's message (the function call request)\n# This is important for maintaining conversation context!\n# Note: With Responses API, we don't need to append the function call request\n# We only append the tool message with results\n\n# Now, create a \"tool\" message with the function result\n# This tells the LLM \"here's the result of the function you requested\"\ntool_message = {\n    \"role\": \"user\",  # Responses API requires 'user' role for function results\n    \"content\": f\"Function {function_name} returned: {json.dumps(function_result)}\"\n}\n\n# Append the tool message to the conversation\nmessages.append(tool_message)\nprint(\"\u2705 Appended function result to messages\")\n\nprint(\"\\n\ud83d\udcdd Current conversation structure:\")\nfor i, msg in enumerate(messages, 1):\n    role = msg.get('role', 'N/A')\n    if role == 'tool':\n        print(f\"   {i}. {role.upper()} - Function result from {msg['name']}\")\n    # (function calls are handled via tool messages)\n    else:\n        content_preview = str(msg.get('content', ''))[:50]\n        print(f\"   {i}. {role.upper()} - {content_preview}...\")\n\nprint(\"\\n\u2705 Step 4 complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd37 STEP 5: Get Final Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd37 STEP 5: Getting final natural language response\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make another API call with the updated conversation\n",
    "# Now the conversation includes the function result!\n",
    "# This time we DON'T pass 'tools' because we don't want another function call\n",
    "print(\"\ud83d\udce4 Sending conversation (with function result) back to LLM...\")\n",
    "\n",
    "final_response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages\n",
    "    # Note: No 'tools' parameter - we're done with function calling\n",
    ")\n",
    "\n",
    "# Extract the final response\n",
    "final_message = final_response.output_text\n",
    "\n",
    "print(\"\\n\ud83d\udce5 Final response received!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83e\udd16 ASSISTANT:\")\n",
    "print(final_message)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\u2705 Step 5 complete!\")\n",
    "print(\"\\n\ud83c\udf89 FUNCTION CALLING LOOP COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udfaf What Just Happened? Complete Flow Recap\n\nLet's trace through the entire conversation flow:\n\n### Message Flow:\n\n1. **User Message** (Step 1)\n   - \"What's the weather in Bratislava right now? Use Celsius.\"\n\n2. **Assistant Message - Function Call Request** (Step 1 response)\n   - \"I need to call `get_current_weather` with `location='Bratislava, Slovakia'` and `format='celsius'`\"\n\n3. **We Execute the Function** (Steps 2-3)\n   - Extract function name and parameters\n   - Call the actual Python function\n   - Get result: `{\"location\": \"Bratislava\", \"temperature\": 22.5, \"condition\": \"Sunny\"}`\n\n4. **User Message - Function Result** (Step 4)\n   - Send the function result back to the LLM as a user message\n   - Formatted as: \"Function {name} returned: {result}\"\n\n5. **Assistant Message - Final Response** (Step 5)\n   - LLM uses the function result to create natural language response\n   - \"The current weather in Bratislava is 22.5\u00b0C and sunny!\"\n\n### Why This Works:\n\n- **Conversation continuity:** We maintained one conversation thread throughout\n- **Proper message roles:** System \u2192 User \u2192 Assistant (tool call) \u2192 Tool (result) \u2192 Assistant (answer)\n- **Tool call tracking:** The `tool_call_id` linked the function result to the original request\n- **Context preservation:** Each API call had access to the full conversation history\n\n---\n\n## \u2705 Key Takeaways: Function Calling Loop\n\n- Function calling is a 5-step process: Request \u2192 Extract \u2192 Execute \u2192 Return \u2192 Respond\n- This is ONE continuous conversation, not separate interactions\n- Always append messages to maintain conversation history\n- The assistant message (with function call) AND the tool message (with result) both get added\n- The `tool_call_id` is critical for linking results to requests\n- The final API call doesn't include `tools` parameter (we don't want more function calls)\n- The LLM uses the function result to create a natural, conversational response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udd27 6. Helper Function for Reusability\n",
    "\n",
    "Now that we understand the complete flow, let's wrap it into a reusable function. This makes function calling much easier to use in real applications!\n",
    "\n",
    "## Why Create a Helper Function?\n",
    "\n",
    "Instead of manually coding the 5-step process every time, we can:\n",
    "- Encapsulate the logic once\n",
    "- Handle errors gracefully\n",
    "- Make it easier to use function calling throughout our application\n",
    "- Support multiple function calls in one response\n",
    "\n",
    "Let's create a robust helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def chat_with_function_execution(messages, tools, available_functions):\n    \"\"\"\n    Execute a chat completion with automatic function calling.\n    \n    This function handles the complete function calling loop:\n    1. Makes initial API call\n    2. If LLM requests function calls, executes them\n    3. Sends results back to LLM\n    4. Returns final response\n    \n    Args:\n        messages (list): Conversation history\n        tools (list): Available tool schemas\n        available_functions (dict): Mapping of function names to actual Python functions\n    \n    Returns:\n        tuple: (final_response_text, updated_messages)\n    \"\"\"\n    \n    print(\"\ud83e\udd16 Starting chat with function execution...\\n\")\n    \n    # STEP 1: Initial API call\n    print(\"\ud83d\udce4 Step 1: Making initial API call...\")\n    response = client.responses.create(\n        model=OPENAI_MODEL,\n        input=messages,\n        tools=tools,\n        tool_choice=\"auto\"\n    )\n    \n    # Get function calls from response\n    function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n    \n    # Check if LLM wants to call functions\n    if not function_calls:\n        # No function calls - just return the text response\n        print(\"\ud83d\udcac No function calls needed. Returning text response.\\n\")\n        # Note: For Responses API, we add function calls via tool messages\n        return response.output_text, messages\n    \n    # LLM wants to call one or more functions\n    print(f\"\ud83d\udcde LLM requested {len(function_calls)} function call(s)\\n\")\n    \n    # Append the assistant's function call request\n    # Note: For Responses API, we add function calls via tool messages\n    \n    # STEPS 2-4: Process each function call\n    for i, tool_call in enumerate(function_calls, 1):\n        # STEP 2: Extract function details\n        function_name = tool_call.name\n        function_arguments = json.loads(tool_call.arguments)\n        tool_call_id = tool_call.call_id\n        \n        print(f\"\ud83d\udd27 Function call {i}: {function_name}\")\n        print(f\"   Arguments: {function_arguments}\")\n        \n        # STEP 3: Execute the function\n        try:\n            # Check if function exists in our available functions\n            if function_name not in available_functions:\n                function_result = {\n                    \"error\": f\"Function '{function_name}' not found\"\n                }\n                print(f\"   \u274c Error: Function not found\")\n            else:\n                # Execute the function\n                function_to_call = available_functions[function_name]\n                function_result = function_to_call(**function_arguments)\n                print(f\"   \u2705 Function executed successfully\")\n        \n        except Exception as e:\n            # Handle execution errors gracefully\n            function_result = {\n                \"error\": f\"Function execution failed: {str(e)}\"\n            }\n            print(f\"   \u274c Error: {str(e)}\")\n        \n        # STEP 4: Add function result to conversation\n        messages.append({\n            \"role\": \"user\",\n            \"content\": f\"Function {function_name} returned: {json.dumps(function_result)}\"\n        })\n        print(f\"   \ud83d\udcdd Result added to conversation\\n\")\n    \n    # STEP 5: Get final response with function results\n    print(\"\ud83d\udce4 Step 5: Getting final response from LLM...\\n\")\n    final_response = client.responses.create(\n        model=OPENAI_MODEL,\n        input=messages\n    )\n    \n    final_message = final_response.output_text\n    \n    print(\"\u2705 Function calling complete!\\n\")\n    \n    return final_message, messages\n\n\nprint(\"\u2705 Helper function created!\")\nprint(\"\\n\ud83d\udcdd Usage:\")\nprint(\"   response, updated_messages = chat_with_function_execution(messages, tools, available_functions)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Test the Helper Function\n",
    "\n",
    "Let's test our helper function with a multi-day forecast request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping function names to actual Python functions\n",
    "# This makes it easy for our helper to route function calls\n",
    "available_functions = {\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"get_n_day_weather_forecast\": get_n_day_weather_forecast\n",
    "}\n",
    "\n",
    "# Start a new conversation\n",
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate weather data.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the 5-day forecast for San Francisco in Fahrenheit?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\ud83e\uddea Testing helper function with forecast request\")\n",
    "print(\"=\"*60)\n",
    "print(f\"User query: {test_messages[1]['content']}\\n\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Use our helper function!\n",
    "response, updated_messages = chat_with_function_execution(\n",
    "    messages=test_messages,\n",
    "    tools=tools,\n",
    "    available_functions=available_functions\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83e\udd16 FINAL RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83c\udf89 Success!\n",
    "\n",
    "Our helper function:\n",
    "- \u2705 Detected the LLM wanted to call `get_n_day_weather_forecast`\n",
    "- \u2705 Extracted the parameters (location, format, num_days)\n",
    "- \u2705 Executed the actual Python function\n",
    "- \u2705 Sent the result back to the LLM\n",
    "- \u2705 Got a natural language response\n",
    "\n",
    "All in one simple function call!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# \u2699\ufe0f 7. Understanding the `tool_choice` Parameter\n\n## What is `tool_choice`?\n\nThe `tool_choice` parameter gives you control over **whether and how** the LLM uses function calling. It tells the LLM:\n- Can you call functions?\n- Should you call functions?\n- Must you call functions?\n\n## Available Options\n\n### 1. `tool_choice=\"auto\"` (Default)\n- **Behavior:** LLM decides whether to call a function or respond with text\n- **When to use:** Most of the time! This gives the LLM freedom to choose the best approach\n- **Example:** User asks \"What's the weather?\" \u2192 LLM calls function. User asks \"Tell me a joke\" \u2192 LLM responds with text\n\n### 2. `tool_choice=\"none\"`\n- **Behavior:** LLM will NOT call any functions, only text responses\n- **When to use:** When you want to disable function calling temporarily\n- **Example:** After getting function results, you don't want another function call\n\n### 3. `tool_choice=\"required\"`\n- **Behavior:** LLM MUST call at least one function (cannot respond with text only)\n- **When to use:** When you need to force a function call, even if the LLM might normally just answer with text\n- **Example:** Data extraction tasks where you always need structured output\n\n### 4. `tool_choice={\"type\": \"function\", \"name\": \"function_name\"}`\n- **Behavior:** Forces the LLM to call a SPECIFIC function\n- **When to use:** When you know exactly which function should be called\n- **Example:** User clicked a \"Get Weather\" button - you want to force `get_current_weather`\n\n## \ud83d\udca1 Key Point: When to Force Function Calls\n\nForcing function calls is useful when:\n- Building structured data extraction tools\n- Creating button-driven interfaces (\"Check Weather\" button = force weather function)\n- Ensuring consistent behavior for specific commands\n- Testing functions during development\n\n---\n\n## \ud83e\uddea Example: `tool_choice=\"none\"`\n\nLet's see what happens when we disable function calling:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83e\uddea Testing with tool_choice='none'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Paris right now?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make API call with tool_choice=\"none\" - LLM CANNOT call functions\n",
    "response = client.responses.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    input=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"none\"  # Disable function calling\n",
    ")\n",
    "\n",
    "print(\"\ud83e\udd16 Assistant response (tool_choice='none'):\")\n",
    "print(response.output_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\ud83d\udcdd Notice: The LLM responded with text instead of calling the weather function!\")\n",
    "print(\"   Even though the function would provide accurate data, tool_choice='none' prevented it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Example: `tool_choice=\"required\"`\n",
    "\n",
    "Now let's force the LLM to call a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\ud83e\uddea Testing with tool_choice='required'\")\nprint(\"=\"*60)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"I'm in London and want to know if I need an umbrella. Use Celsius.\"\n    }\n]\n\n# Make API call with tool_choice=\"required\" - LLM MUST call a function\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=messages,\n    tools=tools,\n    tool_choice=\"required\"  # Force function calling\n)\n\n# Get function calls from response\nfunction_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n\nif function_calls:\n    print(\"\u2705 LLM was forced to call a function!\")\n    print(f\"\\nFunction called: {function_calls[0].name}\")\n    print(f\"Arguments: {function_calls[0].arguments}\")\nelse:\n    print(\"\u274c Unexpected: No function call despite tool_choice='required'\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\n\ud83d\udcdd Notice: Even though the user didn't explicitly ask for weather,\")\nprint(\"   tool_choice='required' forced the LLM to call a function to answer.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Example: Force a Specific Function\n",
    "\n",
    "Let's force the LLM to call a specific function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\ud83e\uddea Testing with tool_choice forcing specific function\")\nprint(\"=\"*60)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"I need weather info for Tokyo in Celsius\"\n    }\n]\n\n# Force the LLM to call get_current_weather specifically\nresponse = client.responses.create(\n    model=OPENAI_MODEL,\n    input=messages,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\",\n        \"name\": \"get_current_weather\"\n    }\n)\n\n# Get function calls from response\nfunction_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n\nif function_calls:\n    print(\"\u2705 LLM called the forced function!\")\n    print(f\"\\nFunction called: {function_calls[0].name}\")\n    print(f\"Arguments: {function_calls[0].arguments}\")\n    print(\"\\n\ud83d\udcdd Even if the user had asked for a forecast, this would call get_current_weather\")\n    print(\"   because we forced that specific function.\")\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u2705 Key Takeaways: tool_choice Parameter\n",
    "\n",
    "- `tool_choice=\"auto\"` - Default, LLM decides (use this most of the time)\n",
    "- `tool_choice=\"none\"` - Disable function calling (useful after getting function results)\n",
    "- `tool_choice=\"required\"` - LLM must call at least one function\n",
    "- Specific function forcing - Useful for button-driven interfaces and structured extraction\n",
    "- Generally, \"auto\" is the best choice - let the LLM be intelligent about when to call functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udee1\ufe0f 8. Error Handling & Edge Cases\n",
    "\n",
    "## Why Error Handling Matters\n",
    "\n",
    "In production applications, many things can go wrong:\n",
    "- The LLM might request a function that doesn't exist\n",
    "- Function arguments might be invalid or malformed\n",
    "- The function itself might throw an error\n",
    "- The LLM might generate invalid JSON\n",
    "- Network issues might cause API failures\n",
    "\n",
    "Robust error handling ensures your application doesn't crash and provides useful feedback.\n",
    "\n",
    "## \u26a0\ufe0f Common Mistakes to Avoid\n",
    "\n",
    "### 1. Not Checking if `tool_calls` Exists\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# \u274c This will crash if no tool calls\n",
    "tool_call = response.output.tool_calls[0]\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# \u2705 Always check first\n",
    "if hasattr(response.output, 'tool_calls') and response.output.tool_calls:\n",
    "    tool_call = response.output.tool_calls[0]\n",
    "```\n",
    "\n",
    "### 2. Not Handling JSON Parse Errors\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# \u274c Will crash if JSON is invalid\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# \u2705 Handle parse errors\n",
    "try:\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing arguments: {e}\")\n",
    "    args = {}\n",
    "```\n",
    "\n",
    "### 3. Not Validating Function Exists\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# \u274c Will crash if function doesn't exist\n",
    "result = available_functions[function_name](**args)\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# \u2705 Check function exists\n",
    "if function_name in available_functions:\n",
    "    result = available_functions[function_name](**args)\n",
    "else:\n",
    "    result = {\"error\": f\"Function {function_name} not found\"}\n",
    "```\n",
    "\n",
    "### 4. Forgetting to Convert Function Result to String\n",
    "\n",
    "**Wrong:**\n",
    "```python\n",
    "# \u274c Tool message content must be a string\n",
    "tool_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": function_result  # \u274c This is a dict!\n",
    "}\n",
    "```\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# \u2705 Convert to JSON string\n",
    "tool_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps(function_result)  # \u2705 String\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd27 Robust Function Execution Handler\n",
    "\n",
    "Let's create an improved function handler with comprehensive error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def execute_function_call_safely(tool_call, available_functions):\n    \"\"\"\n    Safely execute a function call with comprehensive error handling.\n    \n    Args:\n        tool_call: The tool call object from LLM response\n        available_functions (dict): Mapping of function names to Python functions\n    \n    Returns:\n        dict: Contains 'success' boolean and either 'result' or 'error'\n    \"\"\"\n    \n    try:\n        # Extract function name\n        function_name = tool_call.name\n        \n        # Check if function exists\n        if function_name not in available_functions:\n            return {\n                \"success\": False,\n                \"error\": f\"Function '{function_name}' not found in available functions\",\n                \"error_type\": \"function_not_found\"\n            }\n        \n        # Parse arguments with error handling\n        try:\n            function_arguments = json.loads(tool_call.arguments)\n        except json.JSONDecodeError as e:\n            return {\n                \"success\": False,\n                \"error\": f\"Invalid JSON in function arguments: {str(e)}\",\n                \"error_type\": \"json_parse_error\"\n            }\n        \n        # Execute the function with error handling\n        try:\n            function_to_call = available_functions[function_name]\n            result = function_to_call(**function_arguments)\n            \n            return {\n                \"success\": True,\n                \"result\": result\n            }\n        \n        except TypeError as e:\n            # Wrong arguments provided\n            return {\n                \"success\": False,\n                \"error\": f\"Invalid arguments for function '{function_name}': {str(e)}\",\n                \"error_type\": \"invalid_arguments\"\n            }\n        \n        except Exception as e:\n            # Function execution error\n            return {\n                \"success\": False,\n                \"error\": f\"Function '{function_name}' execution failed: {str(e)}\",\n                \"error_type\": \"execution_error\"\n            }\n    \n    except Exception as e:\n        # Catch-all for unexpected errors\n        return {\n            \"success\": False,\n            \"error\": f\"Unexpected error: {str(e)}\",\n            \"error_type\": \"unknown_error\"\n        }\n\n\nprint(\"\u2705 Robust function execution handler created!\")\nprint(\"\\n\ud83d\udee1\ufe0f This handler protects against:\")\nprint(\"   - Function not found errors\")\nprint(\"   - JSON parsing errors\")\nprint(\"   - Invalid argument errors\")\nprint(\"   - Function execution errors\")\nprint(\"   - Unexpected errors\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Testing Error Handling\n",
    "\n",
    "Let's test our robust handler with different error scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock tool call for testing\n",
    "class MockToolCall:\n",
    "    def __init__(self, function_name, arguments_json):\n",
    "        self.function = type('obj', (object,), {\n",
    "            'name': function_name,\n",
    "            'arguments': arguments_json\n",
    "        })\n",
    "\n",
    "print(\"\ud83e\uddea Testing error handling scenarios\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Valid function call\n",
    "print(\"\\nTest 1: Valid function call\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris, France\", \"format\": \"celsius\"}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "if result['success']:\n",
    "    print(f\"Weather data: {result['result']}\")\n",
    "\n",
    "# Test 2: Function doesn't exist\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 2: Non-existent function\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_stock_price\",  # This function doesn't exist\n",
    "    '{\"symbol\": \"AAPL\"}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "# Test 3: Invalid JSON\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 3: Invalid JSON in arguments\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris\", invalid json here}'\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "# Test 4: Missing required argument\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"\\nTest 4: Missing required argument\")\n",
    "mock_call = MockToolCall(\n",
    "    \"get_current_weather\",\n",
    "    '{\"location\": \"Paris, France\"}'  # Missing 'format' argument\n",
    ")\n",
    "result = execute_function_call_safely(mock_call, available_functions)\n",
    "print(f\"Result: {result['success']}\")\n",
    "print(f\"Error: {result['error']}\")\n",
    "print(f\"Error type: {result['error_type']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\u2705 All error scenarios handled gracefully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u2705 Key Takeaways: Error Handling\n",
    "\n",
    "- Always check if `tool_calls` exists before accessing it\n",
    "- Wrap JSON parsing in try/except blocks\n",
    "- Validate that requested functions exist before calling them\n",
    "- Handle function execution errors gracefully\n",
    "- Convert function results to JSON strings for tool messages\n",
    "- Return meaningful error messages that help debug issues\n",
    "- Consider error types to handle different failures appropriately\n",
    "- Never let unhandled exceptions crash your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83c\udfaf 9. Complete End-to-End Example\n",
    "\n",
    "Let's put everything together with a complete, production-ready example. This demonstrates:\n",
    "- Proper conversation flow\n",
    "- Error handling\n",
    "- Multiple function calls\n",
    "- Natural conversation\n",
    "\n",
    "## \ud83c\udf1f Complete Weather Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def weather_assistant(user_query):\n    \"\"\"\n    Complete weather assistant with function calling.\n    \n    Args:\n        user_query (str): The user's question\n    \n    Returns:\n        str: The assistant's response\n    \"\"\"\n    \n    # Initialize conversation\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful weather assistant. Use the provided functions to get accurate, real-time weather data. Be friendly and informative.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_query\n        }\n    ]\n    \n    # Available functions\n    available_functions = {\n        \"get_current_weather\": get_current_weather,\n        \"get_n_day_weather_forecast\": get_n_day_weather_forecast\n    }\n    \n    try:\n        # Step 1: Initial API call\n        response = client.responses.create(\n            model=OPENAI_MODEL,\n            input=messages,\n            tools=tools,\n            tool_choice=\"auto\"\n        )\n        \n        # Get function calls from response\n        function_calls = [item for item in response.output if hasattr(item, 'type') and item.type == 'function_call']\n        \n        # Check if function calling is needed\n        if not function_calls:\n            # No function call needed - return text response\n            return response.output_text\n        \n        # Step 2-4: Process function calls\n        # Note: For Responses API, we add function calls via tool messages\n        \n        for tool_call in function_calls:\n            # Execute function safely\n            execution_result = execute_function_call_safely(tool_call, available_functions)\n            \n            # Prepare result for LLM\n            if execution_result['success']:\n                result_content = json.dumps(execution_result['result'])\n            else:\n                result_content = json.dumps({\n                    \"error\": execution_result['error']\n                })\n            \n            # Add to conversation\n            messages.append({\n                \"role\": \"user\",\n                \"content\": f\"Function {tool_call.name} returned: {result_content}\"\n            })\n        \n        # Step 5: Get final response\n        final_response = client.responses.create(\n            model=OPENAI_MODEL,\n            input=messages\n        )\n        \n        return final_response.output_text\n    \n    except Exception as e:\n        return f\"Sorry, I encountered an error: {str(e)}\"\n\n\nprint(\"\u2705 Complete weather assistant created!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Test the Complete Assistant\n",
    "\n",
    "Let's test with different types of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf24\ufe0f  WEATHER ASSISTANT - DEMO\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Current weather\n",
    "print(\"\\n\ud83d\udc64 User: What's the current weather in London? Use Celsius.\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"What's the current weather in London? Use Celsius.\")\n",
    "print(f\"\ud83e\udd16 Assistant: {response}\")\n",
    "\n",
    "# Test 2: Forecast\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\ud83d\udc64 User: Give me a 3-day forecast for New York in Fahrenheit.\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"Give me a 3-day forecast for New York in Fahrenheit.\")\n",
    "print(f\"\ud83e\udd16 Assistant: {response}\")\n",
    "\n",
    "# Test 3: Vague query (should ask for clarification)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\ud83d\udc64 User: What's the weather like?\")\n",
    "print(\"-\"*60)\n",
    "response = weather_assistant(\"What's the weather like?\")\n",
    "print(f\"\ud83e\udd16 Assistant: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n\ud83c\udf89 Complete weather assistant working perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83d\udcda 10. Best Practices & Key Takeaways\n",
    "\n",
    "## \u2705 Best Practices for Function Calling\n",
    "\n",
    "### 1. **Write Clear, Descriptive Function Descriptions**\n",
    "   - The `description` field is critical for the LLM's decision-making\n",
    "   - Be specific about WHEN to use each function\n",
    "   - Distinguish similar functions clearly\n",
    "   - Example: \"Get the current weather\" vs \"Get an N-day weather forecast\"\n",
    "\n",
    "### 2. **Be Specific About Parameter Types and Constraints**\n",
    "   - Use `enum` to restrict values when possible\n",
    "   - Provide examples in descriptions (\"e.g., San Francisco, CA\")\n",
    "   - Specify format requirements clearly\n",
    "   - Mark required parameters appropriately\n",
    "\n",
    "### 3. **Always Validate Function Exists Before Calling**\n",
    "   - Check if function name is in your available_functions dict\n",
    "   - Return meaningful error messages if not found\n",
    "   - Don't let unknown functions crash your application\n",
    "\n",
    "### 4. **Handle Errors Gracefully**\n",
    "   - Wrap JSON parsing in try/except\n",
    "   - Catch function execution errors\n",
    "   - Return errors to the LLM in the tool message\n",
    "   - Let the LLM explain errors to users naturally\n",
    "\n",
    "### 5. **Keep Functions Focused and Simple**\n",
    "   - Each function should do ONE thing well\n",
    "   - Avoid complex multi-purpose functions\n",
    "   - Make functions easy to test independently\n",
    "   - Keep parameter lists manageable (3-5 parameters max)\n",
    "\n",
    "### 6. **Use tool_choice Strategically**\n",
    "   - Default to \"auto\" for natural behavior\n",
    "   - Use \"required\" for structured data extraction\n",
    "   - Use \"none\" when you don't want function calls\n",
    "   - Force specific functions only when necessary\n",
    "\n",
    "### 7. **Test With Various Queries**\n",
    "   - Clear queries with all information\n",
    "   - Vague queries missing details\n",
    "   - Edge cases and error scenarios\n",
    "   - Multiple function calls in one request\n",
    "\n",
    "### 8. **Consider Cost**\n",
    "   - Each function call = multiple API requests (initial + final)\n",
    "   - Monitor token usage carefully\n",
    "   - Cache results when appropriate\n",
    "   - Use cheaper models (like gpt-5-nano) when possible\n",
    "\n",
    "### 9. **Use Mock Functions for Learning/Testing**\n",
    "   - Develop with mock data before connecting real APIs\n",
    "   - Test function calling logic without external dependencies\n",
    "   - Add real API calls only after logic is solid\n",
    "\n",
    "### 10. **Maintain Conversation Context Properly**\n",
    "   - Always append messages, don't create new conversations\n",
    "   - Include both assistant message (tool call) and tool message (result)\n",
    "   - Preserve conversation history throughout the loop\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udca1 Pro Tips\n",
    "\n",
    "### 1. **Batch Function Definitions**\n",
    "Create a separate file for function definitions and schemas:\n",
    "```python\n",
    "# functions.py\n",
    "def get_weather(...):\n",
    "    ...\n",
    "\n",
    "WEATHER_TOOLS = [ ... ]\n",
    "```\n",
    "This keeps your main code clean and organized.\n",
    "\n",
    "### 2. **Log Function Calls**\n",
    "In production, log:\n",
    "- Which functions are being called\n",
    "- What parameters are being used\n",
    "- Execution time and results\n",
    "- Any errors that occur\n",
    "\n",
    "This helps debug issues and understand usage patterns.\n",
    "\n",
    "### 3. **Version Your Function Schemas**\n",
    "If you change function signatures, version them:\n",
    "```python\n",
    "\"get_weather_v2\"\n",
    "```\n",
    "This prevents breaking changes for existing implementations.\n",
    "\n",
    "### 4. **Add Parameter Validation**\n",
    "Even though the LLM should provide valid parameters, validate them in your functions:\n",
    "```python\n",
    "def get_weather(location, format):\n",
    "    if format not in [\"celsius\", \"fahrenheit\"]:\n",
    "        raise ValueError(\"Invalid format\")\n",
    "    ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f What NOT to Do\n",
    "\n",
    "### 1. **Don't Start New Conversations**\n",
    "\u274c Creating a fresh message list for each API call\n",
    "\u2705 Append to the same messages list throughout the loop\n",
    "\n",
    "### 2. **Don't Forget to Parse Arguments**\n",
    "\u274c Using `tool_call.function.arguments` directly (it's a string!)\n",
    "\u2705 Parse with `json.loads()` first\n",
    "\n",
    "### 3. **Don't Return Non-String Content in Tool Messages**\n",
    "\u274c `\"content\": function_result` (dict)\n",
    "\u2705 `\"content\": json.dumps(function_result)` (string)\n",
    "\n",
    "### 4. **Don't Make Function Descriptions Too Vague**\n",
    "\u274c \"Get weather\" (too vague, when should this be used?)\n",
    "\u2705 \"Get the current weather in a given location. Use this for present conditions.\"\n",
    "\n",
    "### 5. **Don't Ignore Errors**\n",
    "\u274c Letting exceptions crash your application\n",
    "\u2705 Catch errors and handle them gracefully\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Summary: What You've Learned\n",
    "\n",
    "You now know how to:\n",
    "- \u2705 Understand what function calling is and why it's powerful\n",
    "- \u2705 Define tool schemas that describe functions to the LLM\n",
    "- \u2705 Distinguish between schemas (descriptions) and implementations (actual code)\n",
    "- \u2705 Parse and understand LLM responses (text vs tool calls)\n",
    "- \u2705 Execute the complete 5-step function calling loop\n",
    "- \u2705 Maintain conversation context properly\n",
    "- \u2705 Use the tool_choice parameter effectively\n",
    "- \u2705 Handle errors gracefully in production code\n",
    "- \u2705 Build reusable helper functions\n",
    "- \u2705 Apply best practices for robust function calling\n",
    "\n",
    "**Congratulations!** You're now ready to build AI applications that can interact with the real world! \ud83c\udf89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83c\udfcb\ufe0f 11. Practice Exercises\n",
    "\n",
    "Now it's your turn to practice! Try these exercises to solidify your understanding:\n",
    "\n",
    "## \ud83d\udcdd Exercise 1: Calculator Functions\n",
    "\n",
    "**Goal:** Create a calculator assistant that can perform basic math operations.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create four functions: `add()`, `subtract()`, `multiply()`, `divide()`\n",
    "2. Each function should take two numbers as parameters\n",
    "3. Create tool schemas for each function\n",
    "4. Build a calculator assistant that uses these functions\n",
    "5. Test with queries like:\n",
    "   - \"What's 15 + 27?\"\n",
    "   - \"Divide 144 by 12\"\n",
    "   - \"What's 8 times 9?\"\n",
    "\n",
    "**Bonus:** Add a `power()` function for exponentiation.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcdd Exercise 2: Database Query Functions\n",
    "\n",
    "**Goal:** Create mock database query functions for a product catalog.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create mock functions:\n",
    "   - `search_products(query, category, max_price)`\n",
    "   - `get_product_details(product_id)`\n",
    "   - `check_inventory(product_id)`\n",
    "2. Make functions return mock data (use random values)\n",
    "3. Create tool schemas with clear descriptions\n",
    "4. Build an assistant that helps users find products\n",
    "5. Test with queries like:\n",
    "   - \"Show me laptops under $1000\"\n",
    "   - \"What are the details for product ID 12345?\"\n",
    "   - \"Is product 67890 in stock?\"\n",
    "\n",
    "**Bonus:** Add a `compare_products()` function that takes multiple product IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udca1 Tips for Exercises:\n",
    "\n",
    "- Start with the function implementations first\n",
    "- Then create the tool schemas\n",
    "- Test each function independently before integrating\n",
    "- Use the helper functions we created in this notebook\n",
    "- Add error handling as you go\n",
    "- Test with both clear and vague queries\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Challenge: Multi-Step Workflow\n",
    "\n",
    "For an advanced challenge, try creating a booking system that requires multiple function calls:\n",
    "\n",
    "1. `check_availability(date, service_type)` - Check if time slots are available\n",
    "2. `book_appointment(date, time, service_type, customer_name)` - Make a booking\n",
    "3. `cancel_booking(booking_id)` - Cancel a booking\n",
    "\n",
    "The assistant should:\n",
    "- First check availability before booking\n",
    "- Confirm details with the user\n",
    "- Handle the booking\n",
    "- Provide a booking confirmation\n",
    "\n",
    "This requires chaining multiple function calls together intelligently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# \ud83c\udf93 12. Summary & Next Steps\n",
    "\n",
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You've completed the Function Calling tutorial! You now have the foundational knowledge to build AI applications that can:\n",
    "- Access real-time data\n",
    "- Query databases and APIs\n",
    "- Perform actions on behalf of users\n",
    "- Integrate with existing systems\n",
    "\n",
    "## \ud83d\udcda What We Covered\n",
    "\n",
    "1. **Theory:** Understanding what function calling is and why it matters\n",
    "2. **Tool Schemas:** Defining function descriptions for the LLM\n",
    "3. **Response Handling:** Parsing and understanding LLM responses\n",
    "4. **Function Loop:** The complete 5-step execution cycle\n",
    "5. **Helper Functions:** Building reusable, production-ready code\n",
    "6. **tool_choice:** Controlling function calling behavior\n",
    "7. **Error Handling:** Building robust, fault-tolerant applications\n",
    "8. **Best Practices:** Professional patterns and anti-patterns\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "### Immediate Practice:\n",
    "1. Complete the practice exercises above\n",
    "2. Modify the weather assistant with your own functions\n",
    "3. Build a simple chatbot for your use case\n",
    "\n",
    "### Advanced Topics to Explore:\n",
    "1. **Parallel Function Calling:** Handling multiple function calls simultaneously\n",
    "2. **Function Chaining:** Using output from one function as input to another\n",
    "3. **Real API Integration:** Connect to actual APIs (weather, databases, etc.)\n",
    "4. **Streaming Responses:** Handle function calling with streaming\n",
    "5. **State Management:** Maintain conversation state across sessions\n",
    "6. **Security:** Validate and sanitize function parameters\n",
    "7. **Rate Limiting:** Handle API quotas and rate limits\n",
    "8. **Caching:** Cache function results for efficiency\n",
    "\n",
    "### Related Topics:\n",
    "- **Prompt Engineering:** Craft better system prompts for function calling\n",
    "- **RAG (Retrieval Augmented Generation):** Combine function calling with knowledge bases\n",
    "- **Agents:** Build autonomous agents that plan and execute multi-step tasks\n",
    "- **Embeddings:** Use function calling with semantic search\n",
    "\n",
    "## \ud83d\udcd6 Resources for Further Learning\n",
    "\n",
    "- **OpenAI Function Calling Guide:** https://platform.openai.com/docs/guides/function-calling\n",
    "- **OpenAI Cookbook:** https://cookbook.openai.com\n",
    "- **API Reference:** https://platform.openai.com/docs/api-reference\n",
    "\n",
    "## \ud83d\udca1 Remember:\n",
    "\n",
    "- Function calling bridges conversational AI with real-world actions\n",
    "- The LLM requests functions, YOU execute them (you're in control)\n",
    "- Maintain conversation context by appending to the messages list\n",
    "- Always handle errors gracefully\n",
    "- Start with mock functions, add real integrations later\n",
    "- Test thoroughly with various query types\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude4f Thank You!\n",
    "\n",
    "Thank you for completing this tutorial. Function calling is a powerful capability that opens up countless possibilities for AI applications. \n",
    "\n",
    "Keep building, keep experimenting, and most importantly - have fun creating amazing AI-powered applications! \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding!** \ud83c\udf89"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}